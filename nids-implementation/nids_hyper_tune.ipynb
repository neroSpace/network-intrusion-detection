{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from pygod.detector import DOMINANT, OCGNN, CONAD, GAE, AnomalyDAE\n",
    "from pygod.metric import eval_average_precision, eval_roc_auc, eval_f1, eval_precision_at_k, eval_recall_at_k\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Graph Skenario  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = pickle.load(open('model_graph/train_graph.pkl', 'rb'))\n",
    "label_train = pickle.load(open('model_graph/label_train.pkl', 'rb'))\n",
    "train_node_features = pickle.load(open('model_graph/train_node_features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph = pickle.load(open('model_graph/test_graph.pkl', 'rb'))\n",
    "label_test = pickle.load(open('model_graph/label_test.pkl', 'rb'))\n",
    "test_node_features = pickle.load(open('model_graph/test_node_features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15451\n",
      "9525\n"
     ]
    }
   ],
   "source": [
    "## number of nodes\n",
    "print(train_graph.number_of_nodes())\n",
    "print(test_graph.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(train_graph))))\n",
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(test_graph))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor(0): tensor(10070), tensor(1): tensor(5381)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert label_train to numpy array\n",
    "label_train_np = np.array(label_train)\n",
    "label_train_torch = torch.from_numpy(label_train_np)\n",
    "unique, counts = torch.unique(label_train_torch, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Graph Skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_2 = pickle.load(open('model_graph/train_graph_port.pkl', 'rb'))\n",
    "label_train_2 = pickle.load(open('model_graph/label_train_port.pkl', 'rb'))\n",
    "train_node_features_2 = pickle.load(open('model_graph/train_node_features_ip_port.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph_2 = pickle.load(open('model_graph/test_graph_port.pkl', 'rb'))\n",
    "label_test_2 = pickle.load(open('model_graph/label_test_port.pkl', 'rb'))\n",
    "test_node_features_2 = pickle.load(open('model_graph/test_node_features_port.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11213\n",
      "6999\n"
     ]
    }
   ],
   "source": [
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(train_graph_2))))\n",
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(test_graph_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "## number of nodes\n",
    "print(train_graph_2.number_of_nodes())\n",
    "print(test_graph_2.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "print(len(label_train_2))\n",
    "print(len(label_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Graph Skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_3 = pickle.load(open('model_graph/train_graph_ip_port.pkl', 'rb'))\n",
    "label_train_3 = pickle.load(open('model_graph/label_train_ip_port.pkl', 'rb'))\n",
    "train_node_features_3 = pickle.load(open('model_graph/train_node_features_ip_port.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph_3 = pickle.load(open('model_graph/test_graph_ip_port.pkl', 'rb'))\n",
    "label_test_3 = pickle.load(open('model_graph/label_test_ip_port.pkl', 'rb'))\n",
    "test_node_features_3 = pickle.load(open('model_graph/test_node_features_ip_port.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13629\n",
      "7832\n"
     ]
    }
   ],
   "source": [
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(train_graph_3))))\n",
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(test_graph_3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "## number of nodes\n",
    "print(train_graph_3.number_of_nodes())\n",
    "print(test_graph_3.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "print(len(train_node_features_3))\n",
    "print(len(test_node_features_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "print(len(label_train_3))\n",
    "print(len(label_test_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with default parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOMINANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dominant(label_test, dominant_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "    print(pyG_test.x.shape)\n",
    "    dominant_ip_pred_res, dominant_ip_score_res = dominant_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True)\n",
    "    \n",
    "    precision_pygod = eval_precision_at_k(label_test, dominant_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, dominant_ip_score_res)\n",
    "    f1_score = 2 * (precision_pygod * recall_pygod) / (precision_pygod + recall_pygod)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    return precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_dominant_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    print(pyG_train.x.shape)\n",
    "    dominant_model = DOMINANT(epoch=5, verbose=3, gpu=-1)\n",
    "    dominant_compile = dominant_model.fit(pyG_train)\n",
    "    return dominant_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15457, 1])\n",
      "Epoch 0000: Loss 394326.3438 |  | Time 4.12\n",
      "Epoch 0001: Loss 158415.8594 |  | Time 3.46\n",
      "Epoch 0002: Loss 57213.7812 |  | Time 3.83\n",
      "Epoch 0003: Loss 24591.7090 |  | Time 3.46\n",
      "Epoch 0004: Loss 20910.1738 |  | Time 3.70\n",
      "torch.Size([6955, 1])\n",
      "Test: Loss 3.8285 | AUC 0.8984 | Recall 0.7909 | Precision 0.7909 | AP 0.8871 | F1 0.7909 | Time 0.43\n",
      "F1 score:  tensor(0.7909)\n",
      "Precision:  tensor(0.7909)\n",
      "Recall:  tensor(0.7909)\n"
     ]
    }
   ],
   "source": [
    "dominant_model = make_dominant_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_dominant(label_test, dominant_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15480, 1])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 15473 is out of bounds for dimension 0 with size 15473",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dominant_model \u001b[39m=\u001b[39m make_dominant_model(train_graph_2, train_node_features_2, label_train_2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_dominant(label_test_2, dominant_model, test_graph_2, test_node_features_2)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 30\u001b[0m in \u001b[0;36mmake_dominant_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(pyG_train\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m dominant_model \u001b[39m=\u001b[39m DOMINANT(epoch\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m dominant_compile \u001b[39m=\u001b[39m dominant_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dominant_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\dominant.py:164\u001b[0m, in \u001b[0;36mDOMINANT.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    159\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m x_, s_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x, edge_index)\n\u001b[0;32m    162\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    163\u001b[0m                              x_[:batch_size],\n\u001b[1;32m--> 164\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    165\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    166\u001b[0m                              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n\u001b[0;32m    168\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n\u001b[0;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m loss, score\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15473 is out of bounds for dimension 0 with size 15473"
     ]
    }
   ],
   "source": [
    "dominant_model = make_dominant_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_dominant(label_test_2, dominant_model, test_graph_2, test_node_features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15480, 1])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 15473 is out of bounds for dimension 0 with size 15473",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dominant_model \u001b[39m=\u001b[39m make_dominant_model(train_graph_3, train_node_features_3, label_train_3)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_dominant(label_test_3, dominant_model, test_graph_3, test_node_features_3)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 32\u001b[0m in \u001b[0;36mmake_dominant_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(pyG_train\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m dominant_model \u001b[39m=\u001b[39m DOMINANT(epoch\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m dominant_compile \u001b[39m=\u001b[39m dominant_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dominant_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\dominant.py:164\u001b[0m, in \u001b[0;36mDOMINANT.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    159\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m x_, s_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x, edge_index)\n\u001b[0;32m    162\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    163\u001b[0m                              x_[:batch_size],\n\u001b[1;32m--> 164\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    165\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    166\u001b[0m                              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n\u001b[0;32m    168\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n\u001b[0;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m loss, score\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15473 is out of bounds for dimension 0 with size 15473"
     ]
    }
   ],
   "source": [
    "dominant_model = make_dominant_model(train_graph_3, train_node_features_3, label_train_3)\n",
    "precision_score, recall_score, f1_score = predict_dominant(label_test_3, dominant_model, test_graph_3, test_node_features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ocgnn(label_test, ocgnn_compile, pyG_test, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(pyG_test)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    ocgnn_ip_pred_res, ocgnn_ip_score_res = ocgnn_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "\n",
    "    precision_pygod = eval_precision_at_k(label_test, ocgnn_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, ocgnn_ip_score_res)\n",
    "    f1_score = 2 * (precision_pygod * recall_pygod) / (precision_pygod + recall_pygod)\n",
    "    f1_pygod = eval_f1(label_test, ocgnn_ip_pred_res)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    print(\"F1 pygod: \", f1_pygod)\n",
    "\n",
    "    return precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_ocgnn_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    ocgnn_model = OCGNN(epoch=100, verbose=3)\n",
    "    ocgnn_compile = ocgnn_model.fit(pyG_train)\n",
    "    return ocgnn_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 38821.9766 |  | Time 0.10\n",
      "Epoch 0001: Loss 23365.4883 |  | Time 0.17\n",
      "Epoch 0002: Loss 16890.0449 |  | Time 0.09\n",
      "Epoch 0003: Loss 12041.4941 |  | Time 0.09\n",
      "Epoch 0004: Loss 8644.4150 |  | Time 0.10\n",
      "Epoch 0005: Loss 6417.8452 |  | Time 0.10\n",
      "Epoch 0006: Loss 5052.6890 |  | Time 0.10\n",
      "Epoch 0007: Loss 4352.5947 |  | Time 0.08\n",
      "Epoch 0008: Loss 4159.5566 |  | Time 0.09\n",
      "Epoch 0009: Loss 4206.7051 |  | Time 0.11\n",
      "Epoch 0010: Loss 4318.9199 |  | Time 0.12\n",
      "Epoch 0011: Loss 4421.4116 |  | Time 0.11\n",
      "Epoch 0012: Loss 4475.7905 |  | Time 0.10\n",
      "Epoch 0013: Loss 4466.1040 |  | Time 0.09\n",
      "Epoch 0014: Loss 4390.8037 |  | Time 0.10\n",
      "Epoch 0015: Loss 4259.7285 |  | Time 0.10\n",
      "Epoch 0016: Loss 4090.1499 |  | Time 0.10\n",
      "Epoch 0017: Loss 3900.6592 |  | Time 0.11\n",
      "Epoch 0018: Loss 3706.2373 |  | Time 0.09\n",
      "Epoch 0019: Loss 3517.1367 |  | Time 0.10\n",
      "Epoch 0020: Loss 3340.6465 |  | Time 0.10\n",
      "Epoch 0021: Loss 3182.5569 |  | Time 0.12\n",
      "Epoch 0022: Loss 3051.9395 |  | Time 0.09\n",
      "Epoch 0023: Loss 2963.8274 |  | Time 0.09\n",
      "Epoch 0024: Loss 2904.4702 |  | Time 0.10\n",
      "Epoch 0025: Loss 2866.4211 |  | Time 0.10\n",
      "Epoch 0026: Loss 2842.9028 |  | Time 0.10\n",
      "Epoch 0027: Loss 2828.9448 |  | Time 0.12\n",
      "Epoch 0028: Loss 2817.8750 |  | Time 0.12\n",
      "Epoch 0029: Loss 2806.2195 |  | Time 0.12\n",
      "Epoch 0030: Loss 2792.2715 |  | Time 0.13\n",
      "Epoch 0031: Loss 2775.2314 |  | Time 0.15\n",
      "Epoch 0032: Loss 2755.0352 |  | Time 0.10\n",
      "Epoch 0033: Loss 2732.1858 |  | Time 0.12\n",
      "Epoch 0034: Loss 2707.4336 |  | Time 0.15\n",
      "Epoch 0035: Loss 2681.4377 |  | Time 0.12\n",
      "Epoch 0036: Loss 2654.9695 |  | Time 0.12\n",
      "Epoch 0037: Loss 2628.8145 |  | Time 0.13\n",
      "Epoch 0038: Loss 2603.7944 |  | Time 0.14\n",
      "Epoch 0039: Loss 2580.5132 |  | Time 0.12\n",
      "Epoch 0040: Loss 2559.4187 |  | Time 0.11\n",
      "Epoch 0041: Loss 2540.9087 |  | Time 0.14\n",
      "Epoch 0042: Loss 2524.8948 |  | Time 0.12\n",
      "Epoch 0043: Loss 2511.3250 |  | Time 0.13\n",
      "Epoch 0044: Loss 2500.2466 |  | Time 0.12\n",
      "Epoch 0045: Loss 2491.2615 |  | Time 0.11\n",
      "Epoch 0046: Loss 2483.8691 |  | Time 0.11\n",
      "Epoch 0047: Loss 2477.5730 |  | Time 0.11\n",
      "Epoch 0048: Loss 2471.8430 |  | Time 0.10\n",
      "Epoch 0049: Loss 2466.2266 |  | Time 0.13\n",
      "Epoch 0050: Loss 2460.3970 |  | Time 0.13\n",
      "Epoch 0051: Loss 2454.1619 |  | Time 0.11\n",
      "Epoch 0052: Loss 2447.4463 |  | Time 0.18\n",
      "Epoch 0053: Loss 2440.2561 |  | Time 0.13\n",
      "Epoch 0054: Loss 2432.6716 |  | Time 0.10\n",
      "Epoch 0055: Loss 2424.8315 |  | Time 0.11\n",
      "Epoch 0056: Loss 2416.9124 |  | Time 0.10\n",
      "Epoch 0057: Loss 2409.1084 |  | Time 0.10\n",
      "Epoch 0058: Loss 2401.5913 |  | Time 0.11\n",
      "Epoch 0059: Loss 2394.4644 |  | Time 0.11\n",
      "Epoch 0060: Loss 2387.7827 |  | Time 0.11\n",
      "Epoch 0061: Loss 2381.5420 |  | Time 0.10\n",
      "Epoch 0062: Loss 2375.6890 |  | Time 0.09\n",
      "Epoch 0063: Loss 2370.1394 |  | Time 0.10\n",
      "Epoch 0064: Loss 2364.7820 |  | Time 0.11\n",
      "Epoch 0065: Loss 2359.5134 |  | Time 0.11\n",
      "Epoch 0066: Loss 2354.2471 |  | Time 0.11\n",
      "Epoch 0067: Loss 2348.9268 |  | Time 0.11\n",
      "Epoch 0068: Loss 2343.5349 |  | Time 0.11\n",
      "Epoch 0069: Loss 2338.0801 |  | Time 0.10\n",
      "Epoch 0070: Loss 2332.5823 |  | Time 0.11\n",
      "Epoch 0071: Loss 2327.0647 |  | Time 0.11\n",
      "Epoch 0072: Loss 2321.5544 |  | Time 0.10\n",
      "Epoch 0073: Loss 2316.0789 |  | Time 0.12\n",
      "Epoch 0074: Loss 2310.6772 |  | Time 0.12\n",
      "Epoch 0075: Loss 2305.3613 |  | Time 0.10\n",
      "Epoch 0076: Loss 2300.1284 |  | Time 0.11\n",
      "Epoch 0077: Loss 2294.9673 |  | Time 0.13\n",
      "Epoch 0078: Loss 2289.8809 |  | Time 0.11\n",
      "Epoch 0079: Loss 2284.8665 |  | Time 0.12\n",
      "Epoch 0080: Loss 2279.8726 |  | Time 0.10\n",
      "Epoch 0081: Loss 2274.8879 |  | Time 0.10\n",
      "Epoch 0082: Loss 2269.8916 |  | Time 0.11\n",
      "Epoch 0083: Loss 2264.8804 |  | Time 0.11\n",
      "Epoch 0084: Loss 2259.8604 |  | Time 0.12\n",
      "Epoch 0085: Loss 2254.8320 |  | Time 0.10\n",
      "Epoch 0086: Loss 2249.8027 |  | Time 0.12\n",
      "Epoch 0087: Loss 2244.7764 |  | Time 0.10\n",
      "Epoch 0088: Loss 2239.7542 |  | Time 0.10\n",
      "Epoch 0089: Loss 2234.7341 |  | Time 0.12\n",
      "Epoch 0090: Loss 2229.7285 |  | Time 0.10\n",
      "Epoch 0091: Loss 2224.7454 |  | Time 0.11\n",
      "Epoch 0092: Loss 2219.7893 |  | Time 0.11\n",
      "Epoch 0093: Loss 2214.8555 |  | Time 0.10\n",
      "Epoch 0094: Loss 2209.9429 |  | Time 0.11\n",
      "Epoch 0095: Loss 2205.0491 |  | Time 0.11\n",
      "Epoch 0096: Loss 2200.1753 |  | Time 0.10\n",
      "Epoch 0097: Loss 2195.3176 |  | Time 0.10\n",
      "Epoch 0098: Loss 2190.4744 |  | Time 0.10\n",
      "Epoch 0099: Loss 2185.6377 |  | Time 0.10\n",
      "Test: Loss 0.3125 | AUC 0.1491 | Recall 0.2129 | Precision 0.2129 | AP 0.3497 | F1 0.2129 | Time 0.02\n",
      "F1 score:  tensor(0.2129)\n",
      "Precision:  tensor(0.2129)\n",
      "Recall:  tensor(0.2129)\n",
      "F1 pygod:  0.0481444332998997\n"
     ]
    }
   ],
   "source": [
    "ocgnn_model = make_ocgnn_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_ocgnn(label_test, ocgnn_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 272329.4688 |  | Time 0.22\n",
      "Epoch 0001: Loss 196284.0312 |  | Time 0.07\n",
      "Epoch 0002: Loss 146980.3125 |  | Time 0.05\n",
      "Epoch 0003: Loss 109201.5938 |  | Time 0.06\n",
      "Epoch 0004: Loss 80649.5156 |  | Time 0.06\n",
      "Epoch 0005: Loss 59695.8398 |  | Time 0.08\n",
      "Epoch 0006: Loss 44741.0273 |  | Time 0.08\n",
      "Epoch 0007: Loss 34311.1680 |  | Time 0.06\n",
      "Epoch 0008: Loss 27367.6602 |  | Time 0.06\n",
      "Epoch 0009: Loss 23044.4102 |  | Time 0.05\n",
      "Epoch 0010: Loss 20421.2363 |  | Time 0.06\n",
      "Epoch 0011: Loss 18928.2773 |  | Time 0.07\n",
      "Epoch 0012: Loss 18156.6289 |  | Time 0.06\n",
      "Epoch 0013: Loss 17805.0645 |  | Time 0.06\n",
      "Epoch 0014: Loss 17627.0273 |  | Time 0.06\n",
      "Epoch 0015: Loss 17443.8516 |  | Time 0.06\n",
      "Epoch 0016: Loss 17160.8320 |  | Time 0.12\n",
      "Epoch 0017: Loss 16752.4473 |  | Time 0.06\n",
      "Epoch 0018: Loss 16233.4678 |  | Time 0.11\n",
      "Epoch 0019: Loss 15633.1641 |  | Time 0.05\n",
      "Epoch 0020: Loss 14983.0029 |  | Time 0.06\n",
      "Epoch 0021: Loss 14312.9004 |  | Time 0.07\n",
      "Epoch 0022: Loss 13647.0371 |  | Time 0.06\n",
      "Epoch 0023: Loss 12999.0518 |  | Time 0.05\n",
      "Epoch 0024: Loss 12371.8984 |  | Time 0.06\n",
      "Epoch 0025: Loss 11763.5000 |  | Time 0.06\n",
      "Epoch 0026: Loss 11173.7969 |  | Time 0.05\n",
      "Epoch 0027: Loss 10609.7314 |  | Time 0.06\n",
      "Epoch 0028: Loss 10091.1357 |  | Time 0.04\n",
      "Epoch 0029: Loss 9617.1455 |  | Time 0.05\n",
      "Epoch 0030: Loss 9195.5068 |  | Time 0.06\n",
      "Epoch 0031: Loss 8834.6885 |  | Time 0.08\n",
      "Epoch 0032: Loss 8539.4512 |  | Time 0.06\n",
      "Epoch 0033: Loss 8308.7090 |  | Time 0.05\n",
      "Epoch 0034: Loss 8138.8828 |  | Time 0.06\n",
      "Epoch 0035: Loss 8016.5371 |  | Time 0.05\n",
      "Epoch 0036: Loss 7926.1543 |  | Time 0.06\n",
      "Epoch 0037: Loss 7856.2178 |  | Time 0.06\n",
      "Epoch 0038: Loss 7797.1499 |  | Time 0.05\n",
      "Epoch 0039: Loss 7738.2969 |  | Time 0.08\n",
      "Epoch 0040: Loss 7678.7861 |  | Time 0.06\n",
      "Epoch 0041: Loss 7617.4438 |  | Time 0.06\n",
      "Epoch 0042: Loss 7553.8496 |  | Time 0.06\n",
      "Epoch 0043: Loss 7488.3574 |  | Time 0.06\n",
      "Epoch 0044: Loss 7422.1631 |  | Time 0.06\n",
      "Epoch 0045: Loss 7356.6455 |  | Time 0.06\n",
      "Epoch 0046: Loss 7292.9590 |  | Time 0.06\n",
      "Epoch 0047: Loss 7231.5879 |  | Time 0.06\n",
      "Epoch 0048: Loss 7173.1943 |  | Time 0.05\n",
      "Epoch 0049: Loss 7118.4854 |  | Time 0.06\n",
      "Epoch 0050: Loss 7068.0234 |  | Time 0.06\n",
      "Epoch 0051: Loss 7022.1094 |  | Time 0.06\n",
      "Epoch 0052: Loss 6981.4653 |  | Time 0.05\n",
      "Epoch 0053: Loss 6944.6699 |  | Time 0.05\n",
      "Epoch 0054: Loss 6911.1357 |  | Time 0.06\n",
      "Epoch 0055: Loss 6880.5112 |  | Time 0.06\n",
      "Epoch 0056: Loss 6852.5664 |  | Time 0.06\n",
      "Epoch 0057: Loss 6826.8789 |  | Time 0.07\n",
      "Epoch 0058: Loss 6803.6250 |  | Time 0.05\n",
      "Epoch 0059: Loss 6784.2520 |  | Time 0.06\n",
      "Epoch 0060: Loss 6766.9775 |  | Time 0.06\n",
      "Epoch 0061: Loss 6751.4424 |  | Time 0.05\n",
      "Epoch 0062: Loss 6737.3281 |  | Time 0.06\n",
      "Epoch 0063: Loss 6724.3574 |  | Time 0.06\n",
      "Epoch 0064: Loss 6712.2231 |  | Time 0.11\n",
      "Epoch 0065: Loss 6700.6064 |  | Time 0.07\n",
      "Epoch 0066: Loss 6689.1826 |  | Time 0.06\n",
      "Epoch 0067: Loss 6677.6953 |  | Time 0.06\n",
      "Epoch 0068: Loss 6665.8789 |  | Time 0.06\n",
      "Epoch 0069: Loss 6653.5835 |  | Time 0.06\n",
      "Epoch 0070: Loss 6640.7236 |  | Time 0.06\n",
      "Epoch 0071: Loss 6627.2715 |  | Time 0.05\n",
      "Epoch 0072: Loss 6613.2617 |  | Time 0.06\n",
      "Epoch 0073: Loss 6598.7773 |  | Time 0.06\n",
      "Epoch 0074: Loss 6584.0020 |  | Time 0.06\n",
      "Epoch 0075: Loss 6569.1621 |  | Time 0.05\n",
      "Epoch 0076: Loss 6554.4805 |  | Time 0.05\n",
      "Epoch 0077: Loss 6540.1465 |  | Time 0.05\n",
      "Epoch 0078: Loss 6526.3164 |  | Time 0.05\n",
      "Epoch 0079: Loss 6513.0894 |  | Time 0.07\n",
      "Epoch 0080: Loss 6500.5298 |  | Time 0.05\n",
      "Epoch 0081: Loss 6488.6504 |  | Time 0.07\n",
      "Epoch 0082: Loss 6477.4058 |  | Time 0.06\n",
      "Epoch 0083: Loss 6466.7026 |  | Time 0.06\n",
      "Epoch 0084: Loss 6456.4058 |  | Time 0.11\n",
      "Epoch 0085: Loss 6446.3818 |  | Time 0.06\n",
      "Epoch 0086: Loss 6436.4917 |  | Time 0.06\n",
      "Epoch 0087: Loss 6426.6260 |  | Time 0.06\n",
      "Epoch 0088: Loss 6416.6958 |  | Time 0.06\n",
      "Epoch 0089: Loss 6406.6455 |  | Time 0.07\n",
      "Epoch 0090: Loss 6396.4604 |  | Time 0.06\n",
      "Epoch 0091: Loss 6386.1416 |  | Time 0.06\n",
      "Epoch 0092: Loss 6375.7065 |  | Time 0.06\n",
      "Epoch 0093: Loss 6365.2017 |  | Time 0.06\n",
      "Epoch 0094: Loss 6354.6680 |  | Time 0.07\n",
      "Epoch 0095: Loss 6344.1230 |  | Time 0.07\n",
      "Epoch 0096: Loss 6333.6021 |  | Time 0.07\n",
      "Epoch 0097: Loss 6323.1318 |  | Time 0.05\n",
      "Epoch 0098: Loss 6312.7256 |  | Time 0.07\n",
      "Epoch 0099: Loss 6302.3848 |  | Time 0.06\n",
      "Test: Loss 0.7333 | AUC 0.2339 | Recall 0.3328 | Precision 0.3328 | AP 0.3667 | F1 0.3533 | Time 0.03\n",
      "F1 score:  tensor(0.3328)\n",
      "Precision:  tensor(0.3328)\n",
      "Recall:  tensor(0.3328)\n",
      "F1 pygod:  0.03161320316132031\n"
     ]
    }
   ],
   "source": [
    "ocgnn_model = make_ocgnn_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_ocgnn(label_test_2, ocgnn_model, test_graph_2, test_node_features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 145493.6875 |  | Time 0.06\n",
      "Epoch 0001: Loss 98338.7734 |  | Time 0.06\n",
      "Epoch 0002: Loss 68696.9219 |  | Time 0.06\n",
      "Epoch 0003: Loss 47724.0703 |  | Time 0.06\n",
      "Epoch 0004: Loss 33233.4102 |  | Time 0.07\n",
      "Epoch 0005: Loss 23495.0137 |  | Time 0.08\n",
      "Epoch 0006: Loss 17337.1211 |  | Time 0.06\n",
      "Epoch 0007: Loss 13717.0420 |  | Time 0.06\n",
      "Epoch 0008: Loss 11834.3447 |  | Time 0.06\n",
      "Epoch 0009: Loss 10928.4941 |  | Time 0.05\n",
      "Epoch 0010: Loss 10522.3604 |  | Time 0.05\n",
      "Epoch 0011: Loss 10321.0176 |  | Time 0.06\n",
      "Epoch 0012: Loss 10160.2988 |  | Time 0.08\n",
      "Epoch 0013: Loss 9954.0576 |  | Time 0.09\n",
      "Epoch 0014: Loss 9664.7900 |  | Time 0.07\n",
      "Epoch 0015: Loss 9292.3750 |  | Time 0.08\n",
      "Epoch 0016: Loss 8863.8682 |  | Time 0.07\n",
      "Epoch 0017: Loss 8417.5449 |  | Time 0.08\n",
      "Epoch 0018: Loss 7987.0801 |  | Time 0.06\n",
      "Epoch 0019: Loss 7592.3994 |  | Time 0.06\n",
      "Epoch 0020: Loss 7238.5469 |  | Time 0.06\n",
      "Epoch 0021: Loss 6919.6494 |  | Time 0.06\n",
      "Epoch 0022: Loss 6623.9551 |  | Time 0.07\n",
      "Epoch 0023: Loss 6338.3887 |  | Time 0.07\n",
      "Epoch 0024: Loss 6052.5190 |  | Time 0.06\n",
      "Epoch 0025: Loss 5767.4385 |  | Time 0.06\n",
      "Epoch 0026: Loss 5479.4697 |  | Time 0.06\n",
      "Epoch 0027: Loss 5190.7314 |  | Time 0.07\n",
      "Epoch 0028: Loss 4913.2207 |  | Time 0.06\n",
      "Epoch 0029: Loss 4659.1348 |  | Time 0.06\n",
      "Epoch 0030: Loss 4436.7070 |  | Time 0.06\n",
      "Epoch 0031: Loss 4248.3857 |  | Time 0.06\n",
      "Epoch 0032: Loss 4092.2310 |  | Time 0.08\n",
      "Epoch 0033: Loss 3966.8052 |  | Time 0.08\n",
      "Epoch 0034: Loss 3870.2222 |  | Time 0.15\n",
      "Epoch 0035: Loss 3793.8862 |  | Time 0.07\n",
      "Epoch 0036: Loss 3731.4438 |  | Time 0.07\n",
      "Epoch 0037: Loss 3678.0305 |  | Time 0.06\n",
      "Epoch 0038: Loss 3629.7041 |  | Time 0.06\n",
      "Epoch 0039: Loss 3584.0908 |  | Time 0.09\n",
      "Epoch 0040: Loss 3540.3677 |  | Time 0.08\n",
      "Epoch 0041: Loss 3498.5830 |  | Time 0.10\n",
      "Epoch 0042: Loss 3458.3430 |  | Time 0.09\n",
      "Epoch 0043: Loss 3419.7607 |  | Time 0.06\n",
      "Epoch 0044: Loss 3383.3711 |  | Time 0.06\n",
      "Epoch 0045: Loss 3349.8989 |  | Time 0.06\n",
      "Epoch 0046: Loss 3319.7468 |  | Time 0.07\n",
      "Epoch 0047: Loss 3293.1943 |  | Time 0.06\n",
      "Epoch 0048: Loss 3270.1152 |  | Time 0.06\n",
      "Epoch 0049: Loss 3250.0483 |  | Time 0.06\n",
      "Epoch 0050: Loss 3232.2336 |  | Time 0.07\n",
      "Epoch 0051: Loss 3216.0352 |  | Time 0.05\n",
      "Epoch 0052: Loss 3200.7407 |  | Time 0.06\n",
      "Epoch 0053: Loss 3185.7769 |  | Time 0.06\n",
      "Epoch 0054: Loss 3170.8003 |  | Time 0.07\n",
      "Epoch 0055: Loss 3155.8032 |  | Time 0.07\n",
      "Epoch 0056: Loss 3140.7874 |  | Time 0.06\n",
      "Epoch 0057: Loss 3125.9727 |  | Time 0.06\n",
      "Epoch 0058: Loss 3111.3909 |  | Time 0.06\n",
      "Epoch 0059: Loss 3097.2891 |  | Time 0.06\n",
      "Epoch 0060: Loss 3083.8440 |  | Time 0.07\n",
      "Epoch 0061: Loss 3070.8140 |  | Time 0.07\n",
      "Epoch 0062: Loss 3058.2056 |  | Time 0.09\n",
      "Epoch 0063: Loss 3046.0894 |  | Time 0.10\n",
      "Epoch 0064: Loss 3034.4912 |  | Time 0.10\n",
      "Epoch 0065: Loss 3023.5425 |  | Time 0.09\n",
      "Epoch 0066: Loss 3013.3042 |  | Time 0.08\n",
      "Epoch 0067: Loss 3003.8657 |  | Time 0.09\n",
      "Epoch 0068: Loss 2995.2141 |  | Time 0.08\n",
      "Epoch 0069: Loss 2987.2329 |  | Time 0.09\n",
      "Epoch 0070: Loss 2979.7878 |  | Time 0.09\n",
      "Epoch 0071: Loss 2972.7556 |  | Time 0.08\n",
      "Epoch 0072: Loss 2965.9829 |  | Time 0.07\n",
      "Epoch 0073: Loss 2959.3281 |  | Time 0.07\n",
      "Epoch 0074: Loss 2952.7285 |  | Time 0.08\n",
      "Epoch 0075: Loss 2946.1704 |  | Time 0.09\n",
      "Epoch 0076: Loss 2939.6648 |  | Time 0.09\n",
      "Epoch 0077: Loss 2933.2300 |  | Time 0.08\n",
      "Epoch 0078: Loss 2926.8608 |  | Time 0.08\n",
      "Epoch 0079: Loss 2920.5342 |  | Time 0.08\n",
      "Epoch 0080: Loss 2914.2134 |  | Time 0.08\n",
      "Epoch 0081: Loss 2907.8618 |  | Time 0.09\n",
      "Epoch 0082: Loss 2901.4507 |  | Time 0.07\n",
      "Epoch 0083: Loss 2894.9634 |  | Time 0.07\n",
      "Epoch 0084: Loss 2888.3962 |  | Time 0.08\n",
      "Epoch 0085: Loss 2881.7637 |  | Time 0.07\n",
      "Epoch 0086: Loss 2875.0974 |  | Time 0.08\n",
      "Epoch 0087: Loss 2868.4468 |  | Time 0.08\n",
      "Epoch 0088: Loss 2861.8613 |  | Time 0.07\n",
      "Epoch 0089: Loss 2855.3787 |  | Time 0.09\n",
      "Epoch 0090: Loss 2849.0181 |  | Time 0.08\n",
      "Epoch 0091: Loss 2842.7825 |  | Time 0.07\n",
      "Epoch 0092: Loss 2836.6741 |  | Time 0.08\n",
      "Epoch 0093: Loss 2830.6528 |  | Time 0.07\n",
      "Epoch 0094: Loss 2824.6982 |  | Time 0.10\n",
      "Epoch 0095: Loss 2818.8027 |  | Time 0.08\n",
      "Epoch 0096: Loss 2812.9392 |  | Time 0.07\n",
      "Epoch 0097: Loss 2807.0872 |  | Time 0.07\n",
      "Epoch 0098: Loss 2801.2344 |  | Time 0.06\n",
      "Epoch 0099: Loss 2795.3962 |  | Time 0.08\n",
      "Test: Loss 0.4004 | AUC 0.2126 | Recall 0.3196 | Precision 0.3196 | AP 0.3593 | F1 0.3408 | Time 0.02\n",
      "F1 score:  tensor(0.3196)\n",
      "Precision:  tensor(0.3196)\n",
      "Recall:  tensor(0.3196)\n",
      "F1 pygod:  0.028250216200634188\n"
     ]
    }
   ],
   "source": [
    "ocgnn_model = make_ocgnn_model(train_graph_3, train_node_features_3, label_train_3)\n",
    "precision_score, recall_score, f1_score = predict_ocgnn(label_test_3, ocgnn_model, test_graph_3, test_node_features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gae(label_test, gae_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    gae_ip_pred_res, gae_ip_score_res = gae_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "    precision = eval_precision_at_k(label_test, gae_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, gae_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "    \n",
    "def make_gae_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "    label_train = label_train\n",
    "\n",
    "    gae_model = GAE(epoch=100, verbose=3)\n",
    "    gae_compile = gae_model.fit(pyG_train, label_train)\n",
    "    return gae_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 141640.9844 | AUC 0.7838 | Recall 0.6622 | Precision 0.6622 | AP 0.6506 | F1 0.7159 | Time 0.15\n",
      "Epoch 0001: Loss 86944.0312 | AUC 0.7773 | Recall 0.6523 | Precision 0.6523 | AP 0.6435 | F1 0.6720 | Time 0.08\n",
      "Epoch 0002: Loss 50552.5000 | AUC 0.7648 | Recall 0.6562 | Precision 0.6562 | AP 0.6256 | F1 0.6684 | Time 0.11\n",
      "Epoch 0003: Loss 27243.8516 | AUC 0.7514 | Recall 0.6709 | Precision 0.6709 | AP 0.5953 | F1 0.6704 | Time 0.09\n",
      "Epoch 0004: Loss 17069.3672 | AUC 0.6877 | Recall 0.5117 | Precision 0.5117 | AP 0.5125 | F1 0.5106 | Time 0.11\n",
      "Epoch 0005: Loss 19053.2266 | AUC 0.6940 | Recall 0.6158 | Precision 0.6158 | AP 0.5277 | F1 0.6157 | Time 0.11\n",
      "Epoch 0006: Loss 25842.7031 | AUC 0.6901 | Recall 0.6182 | Precision 0.6182 | AP 0.5349 | F1 0.6182 | Time 0.09\n",
      "Epoch 0007: Loss 28520.9570 | AUC 0.6882 | Recall 0.6130 | Precision 0.6130 | AP 0.5356 | F1 0.6149 | Time 0.09\n",
      "Epoch 0008: Loss 26046.2812 | AUC 0.6903 | Recall 0.6179 | Precision 0.6179 | AP 0.5350 | F1 0.6180 | Time 0.08\n",
      "Epoch 0009: Loss 21327.8223 | AUC 0.6919 | Recall 0.6173 | Precision 0.6173 | AP 0.5304 | F1 0.6076 | Time 0.09\n",
      "Epoch 0010: Loss 17625.9570 | AUC 0.6767 | Recall 0.5146 | Precision 0.5146 | AP 0.5100 | F1 0.5430 | Time 0.10\n",
      "Epoch 0011: Loss 16525.3555 | AUC 0.6089 | Recall 0.4781 | Precision 0.4781 | AP 0.4596 | F1 0.4781 | Time 0.10\n",
      "Epoch 0012: Loss 17626.8945 | AUC 0.6976 | Recall 0.5248 | Precision 0.5248 | AP 0.5324 | F1 0.5237 | Time 0.09\n",
      "Epoch 0013: Loss 19538.9141 | AUC 0.7331 | Recall 0.6477 | Precision 0.6477 | AP 0.5698 | F1 0.6341 | Time 0.09\n",
      "Epoch 0014: Loss 20885.3906 | AUC 0.7392 | Recall 0.6570 | Precision 0.6570 | AP 0.5761 | F1 0.6527 | Time 0.08\n",
      "Epoch 0015: Loss 20992.2031 | AUC 0.7397 | Recall 0.6572 | Precision 0.6572 | AP 0.5765 | F1 0.6527 | Time 0.09\n",
      "Epoch 0016: Loss 19978.8809 | AUC 0.7357 | Recall 0.6529 | Precision 0.6529 | AP 0.5724 | F1 0.6523 | Time 0.10\n",
      "Epoch 0017: Loss 18448.5410 | AUC 0.7152 | Recall 0.6349 | Precision 0.6349 | AP 0.5563 | F1 0.6335 | Time 0.11\n",
      "Epoch 0018: Loss 17125.3516 | AUC 0.6894 | Recall 0.5119 | Precision 0.5119 | AP 0.5157 | F1 0.5109 | Time 0.10\n",
      "Epoch 0019: Loss 16527.5938 | AUC 0.6775 | Recall 0.4789 | Precision 0.4789 | AP 0.4809 | F1 0.4789 | Time 0.11\n",
      "Epoch 0020: Loss 16785.2578 | AUC 0.6744 | Recall 0.5037 | Precision 0.5037 | AP 0.4924 | F1 0.5030 | Time 0.11\n",
      "Epoch 0021: Loss 17558.8379 | AUC 0.6777 | Recall 0.5074 | Precision 0.5074 | AP 0.5093 | F1 0.5074 | Time 0.09\n",
      "Epoch 0022: Loss 18265.2422 | AUC 0.6912 | Recall 0.6140 | Precision 0.6140 | AP 0.5253 | F1 0.6067 | Time 0.09\n",
      "Epoch 0023: Loss 18451.7148 | AUC 0.6930 | Recall 0.6147 | Precision 0.6147 | AP 0.5266 | F1 0.6043 | Time 0.10\n",
      "Epoch 0024: Loss 18052.0918 | AUC 0.6861 | Recall 0.6136 | Precision 0.6136 | AP 0.5221 | F1 0.6092 | Time 0.08\n",
      "Epoch 0025: Loss 17354.8008 | AUC 0.6771 | Recall 0.5076 | Precision 0.5076 | AP 0.5060 | F1 0.5076 | Time 0.09\n",
      "Epoch 0026: Loss 16755.4883 | AUC 0.6745 | Recall 0.5026 | Precision 0.5026 | AP 0.4913 | F1 0.5019 | Time 0.09\n",
      "Epoch 0027: Loss 16516.5703 | AUC 0.6805 | Recall 0.4815 | Precision 0.4815 | AP 0.4789 | F1 0.4815 | Time 0.09\n",
      "Epoch 0028: Loss 16656.2051 | AUC 0.6819 | Recall 0.4972 | Precision 0.4972 | AP 0.4978 | F1 0.4892 | Time 0.09\n",
      "Epoch 0029: Loss 16994.1973 | AUC 0.6867 | Recall 0.5100 | Precision 0.5100 | AP 0.5102 | F1 0.5100 | Time 0.08\n",
      "Epoch 0030: Loss 17288.2754 | AUC 0.6913 | Recall 0.5204 | Precision 0.5204 | AP 0.5206 | F1 0.5204 | Time 0.08\n",
      "Epoch 0031: Loss 17369.7910 | AUC 0.6916 | Recall 0.5222 | Precision 0.5222 | AP 0.5223 | F1 0.5222 | Time 0.09\n",
      "Epoch 0032: Loss 17212.4961 | AUC 0.6902 | Recall 0.5195 | Precision 0.5195 | AP 0.5184 | F1 0.5173 | Time 0.12\n",
      "Epoch 0033: Loss 16918.2676 | AUC 0.6850 | Recall 0.5096 | Precision 0.5096 | AP 0.5069 | F1 0.5082 | Time 0.10\n",
      "Epoch 0034: Loss 16644.9668 | AUC 0.6819 | Recall 0.4961 | Precision 0.4961 | AP 0.4971 | F1 0.4909 | Time 0.13\n",
      "Epoch 0035: Loss 16518.3066 | AUC 0.4374 | Recall 0.3953 | Precision 0.3953 | AP 0.3742 | F1 0.4125 | Time 0.09\n",
      "Epoch 0036: Loss 16568.9629 | AUC 0.6774 | Recall 0.4965 | Precision 0.4965 | AP 0.4864 | F1 0.4919 | Time 0.09\n",
      "Epoch 0037: Loss 16725.2109 | AUC 0.6755 | Recall 0.5022 | Precision 0.5022 | AP 0.4910 | F1 0.5001 | Time 0.10\n",
      "Epoch 0038: Loss 16865.1875 | AUC 0.6747 | Recall 0.5039 | Precision 0.5039 | AP 0.4950 | F1 0.5039 | Time 0.09\n",
      "Epoch 0039: Loss 16896.0156 | AUC 0.6750 | Recall 0.5044 | Precision 0.5044 | AP 0.4960 | F1 0.5036 | Time 0.12\n",
      "Epoch 0040: Loss 16806.5938 | AUC 0.6753 | Recall 0.5037 | Precision 0.5037 | AP 0.4936 | F1 0.5029 | Time 0.09\n",
      "Epoch 0041: Loss 16661.1816 | AUC 0.6771 | Recall 0.5004 | Precision 0.5004 | AP 0.4910 | F1 0.4931 | Time 0.10\n",
      "Epoch 0042: Loss 16546.7871 | AUC 0.6775 | Recall 0.4887 | Precision 0.4887 | AP 0.4823 | F1 0.4861 | Time 0.10\n",
      "Epoch 0043: Loss 16516.4688 | AUC 0.6801 | Recall 0.4778 | Precision 0.4778 | AP 0.4719 | F1 0.4771 | Time 0.09\n",
      "Epoch 0044: Loss 16564.4707 | AUC 0.6833 | Recall 0.4876 | Precision 0.4876 | AP 0.4873 | F1 0.4875 | Time 0.08\n",
      "Epoch 0045: Loss 16641.0625 | AUC 0.6815 | Recall 0.4961 | Precision 0.4961 | AP 0.4968 | F1 0.4911 | Time 0.09\n",
      "Epoch 0046: Loss 16689.7930 | AUC 0.6826 | Recall 0.4968 | Precision 0.4968 | AP 0.5001 | F1 0.4963 | Time 0.09\n",
      "Epoch 0047: Loss 16681.2363 | AUC 0.6824 | Recall 0.4970 | Precision 0.4970 | AP 0.4997 | F1 0.4970 | Time 0.09\n",
      "Epoch 0048: Loss 16625.1680 | AUC 0.6822 | Recall 0.4961 | Precision 0.4961 | AP 0.4953 | F1 0.4922 | Time 0.08\n",
      "Epoch 0049: Loss 16558.3828 | AUC 0.6834 | Recall 0.4876 | Precision 0.4876 | AP 0.4865 | F1 0.4876 | Time 0.10\n",
      "Epoch 0050: Loss 16518.8223 | AUC 0.5232 | Recall 0.4778 | Precision 0.4778 | AP 0.4083 | F1 0.4769 | Time 0.09\n",
      "Epoch 0051: Loss 16522.3965 | AUC 0.6799 | Recall 0.4839 | Precision 0.4839 | AP 0.4799 | F1 0.4826 | Time 0.09\n",
      "Epoch 0052: Loss 16556.0215 | AUC 0.6773 | Recall 0.4961 | Precision 0.4961 | AP 0.4844 | F1 0.4942 | Time 0.09\n",
      "Epoch 0053: Loss 16589.7832 | AUC 0.6776 | Recall 0.4987 | Precision 0.4987 | AP 0.4888 | F1 0.4971 | Time 0.10\n",
      "Epoch 0054: Loss 16598.6934 | AUC 0.6775 | Recall 0.4985 | Precision 0.4985 | AP 0.4893 | F1 0.4985 | Time 0.10\n",
      "Epoch 0055: Loss 16578.4043 | AUC 0.6778 | Recall 0.4967 | Precision 0.4967 | AP 0.4878 | F1 0.4896 | Time 0.10\n",
      "Epoch 0056: Loss 16544.7969 | AUC 0.6778 | Recall 0.4867 | Precision 0.4867 | AP 0.4820 | F1 0.4858 | Time 0.09\n",
      "Epoch 0057: Loss 16519.9785 | AUC 0.6803 | Recall 0.4835 | Precision 0.4835 | AP 0.4797 | F1 0.4828 | Time 0.09\n",
      "Epoch 0058: Loss 16516.5293 | AUC 0.6801 | Recall 0.4778 | Precision 0.4778 | AP 0.4719 | F1 0.4771 | Time 0.09\n",
      "Epoch 0059: Loss 16530.8965 | AUC 0.6845 | Recall 0.4826 | Precision 0.4826 | AP 0.4836 | F1 0.4826 | Time 0.09\n",
      "Epoch 0060: Loss 16548.6465 | AUC 0.6837 | Recall 0.4850 | Precision 0.4850 | AP 0.4855 | F1 0.4837 | Time 0.09\n",
      "Epoch 0061: Loss 16555.8613 | AUC 0.6832 | Recall 0.4874 | Precision 0.4874 | AP 0.4863 | F1 0.4874 | Time 0.09\n",
      "Epoch 0062: Loss 16548.1680 | AUC 0.6837 | Recall 0.4850 | Precision 0.4850 | AP 0.4854 | F1 0.4839 | Time 0.08\n",
      "Epoch 0063: Loss 16532.0098 | AUC 0.6846 | Recall 0.4826 | Precision 0.4826 | AP 0.4839 | F1 0.4824 | Time 0.09\n",
      "Epoch 0064: Loss 16518.5801 | AUC 0.5791 | Recall 0.4778 | Precision 0.4778 | AP 0.4290 | F1 0.4769 | Time 0.10\n",
      "Epoch 0065: Loss 16515.5137 | AUC 0.6831 | Recall 0.4817 | Precision 0.4817 | AP 0.4792 | F1 0.4805 | Time 0.09\n",
      "Epoch 0066: Loss 16522.1621 | AUC 0.6802 | Recall 0.4839 | Precision 0.4839 | AP 0.4800 | F1 0.4826 | Time 0.10\n",
      "Epoch 0067: Loss 16531.2852 | AUC 0.6793 | Recall 0.4852 | Precision 0.4852 | AP 0.4808 | F1 0.4852 | Time 0.10\n",
      "Epoch 0068: Loss 16535.1660 | AUC 0.6790 | Recall 0.4863 | Precision 0.4863 | AP 0.4813 | F1 0.4863 | Time 0.07\n",
      "Epoch 0069: Loss 16531.1992 | AUC 0.6793 | Recall 0.4852 | Precision 0.4852 | AP 0.4808 | F1 0.4852 | Time 0.10\n",
      "Epoch 0070: Loss 16522.9297 | AUC 0.6801 | Recall 0.4839 | Precision 0.4839 | AP 0.4800 | F1 0.4826 | Time 0.10\n",
      "Epoch 0071: Loss 16516.4512 | AUC 0.6822 | Recall 0.4817 | Precision 0.4817 | AP 0.4797 | F1 0.4812 | Time 0.09\n",
      "Epoch 0072: Loss 16515.5488 | AUC 0.6817 | Recall 0.4781 | Precision 0.4781 | AP 0.4751 | F1 0.4767 | Time 0.08\n",
      "Epoch 0073: Loss 16519.3535 | AUC 0.6705 | Recall 0.4778 | Precision 0.4778 | AP 0.4749 | F1 0.4766 | Time 0.10\n",
      "Epoch 0074: Loss 16523.7871 | AUC 0.6839 | Recall 0.4781 | Precision 0.4781 | AP 0.4812 | F1 0.4776 | Time 0.09\n",
      "Epoch 0075: Loss 16525.0742 | AUC 0.6840 | Recall 0.4807 | Precision 0.4807 | AP 0.4817 | F1 0.4802 | Time 0.09\n",
      "Epoch 0076: Loss 16522.4512 | AUC 0.6841 | Recall 0.4781 | Precision 0.4781 | AP 0.4808 | F1 0.4781 | Time 0.09\n",
      "Epoch 0077: Loss 16518.1895 | AUC 0.5853 | Recall 0.4778 | Precision 0.4778 | AP 0.4387 | F1 0.4769 | Time 0.09\n",
      "Epoch 0078: Loss 16515.4355 | AUC 0.6822 | Recall 0.4781 | Precision 0.4781 | AP 0.4762 | F1 0.4767 | Time 0.09\n",
      "Epoch 0079: Loss 16515.7012 | AUC 0.6833 | Recall 0.4817 | Precision 0.4817 | AP 0.4795 | F1 0.4802 | Time 0.09\n",
      "Epoch 0080: Loss 16517.9922 | AUC 0.6820 | Recall 0.4817 | Precision 0.4817 | AP 0.4802 | F1 0.4814 | Time 0.09\n",
      "Epoch 0081: Loss 16519.9609 | AUC 0.6808 | Recall 0.4833 | Precision 0.4833 | AP 0.4801 | F1 0.4834 | Time 0.08\n",
      "Epoch 0082: Loss 16519.9277 | AUC 0.6809 | Recall 0.4830 | Precision 0.4830 | AP 0.4801 | F1 0.4815 | Time 0.09\n",
      "Epoch 0083: Loss 16518.0781 | AUC 0.6820 | Recall 0.4817 | Precision 0.4817 | AP 0.4802 | F1 0.4814 | Time 0.10\n",
      "Epoch 0084: Loss 16515.9922 | AUC 0.6833 | Recall 0.4817 | Precision 0.4817 | AP 0.4801 | F1 0.4801 | Time 0.09\n",
      "Epoch 0085: Loss 16515.1758 | AUC 0.6828 | Recall 0.4781 | Precision 0.4781 | AP 0.4776 | F1 0.4777 | Time 0.10\n",
      "Epoch 0086: Loss 16515.8906 | AUC 0.6806 | Recall 0.4778 | Precision 0.4778 | AP 0.4725 | F1 0.4769 | Time 0.10\n",
      "Epoch 0087: Loss 16517.1621 | AUC 0.5091 | Recall 0.4778 | Precision 0.4778 | AP 0.3996 | F1 0.4772 | Time 0.09\n",
      "Epoch 0088: Loss 16517.7656 | AUC 0.6620 | Recall 0.4778 | Precision 0.4778 | AP 0.4683 | F1 0.4769 | Time 0.08\n",
      "Epoch 0089: Loss 16517.2266 | AUC 0.5774 | Recall 0.4778 | Precision 0.4778 | AP 0.4254 | F1 0.4772 | Time 0.09\n",
      "Epoch 0090: Loss 16516.0820 | AUC 0.6805 | Recall 0.4778 | Precision 0.4778 | AP 0.4723 | F1 0.4771 | Time 0.09\n",
      "Epoch 0091: Loss 16515.2656 | AUC 0.6826 | Recall 0.4781 | Precision 0.4781 | AP 0.4770 | F1 0.4766 | Time 0.09\n",
      "Epoch 0092: Loss 16515.2910 | AUC 0.6833 | Recall 0.4805 | Precision 0.4805 | AP 0.4789 | F1 0.4798 | Time 0.09\n",
      "Epoch 0093: Loss 16515.9199 | AUC 0.6835 | Recall 0.4817 | Precision 0.4817 | AP 0.4802 | F1 0.4801 | Time 0.09\n",
      "Epoch 0094: Loss 16516.4512 | AUC 0.6834 | Recall 0.4817 | Precision 0.4817 | AP 0.4805 | F1 0.4800 | Time 0.09\n",
      "Epoch 0095: Loss 16516.4062 | AUC 0.6835 | Recall 0.4817 | Precision 0.4817 | AP 0.4805 | F1 0.4801 | Time 0.08\n",
      "Epoch 0096: Loss 16515.8672 | AUC 0.6835 | Recall 0.4813 | Precision 0.4813 | AP 0.4802 | F1 0.4800 | Time 0.13\n",
      "Epoch 0097: Loss 16515.3203 | AUC 0.6834 | Recall 0.4805 | Precision 0.4805 | AP 0.4790 | F1 0.4798 | Time 0.09\n",
      "Epoch 0098: Loss 16515.1738 | AUC 0.6827 | Recall 0.4781 | Precision 0.4781 | AP 0.4774 | F1 0.4764 | Time 0.09\n",
      "Epoch 0099: Loss 16515.4375 | AUC 0.6824 | Recall 0.4778 | Precision 0.4778 | AP 0.4763 | F1 0.4768 | Time 0.08\n",
      "Test: Loss 3.2199 | AUC 0.7310 | Recall 0.7166 | Precision 0.7166 | AP 0.6616 | F1 0.7455 | Time 0.02\n",
      "F1 score:  tensor(0.7166)\n",
      "Precision:  tensor(0.7166)\n",
      "Recall:  tensor(0.7166)\n"
     ]
    }
   ],
   "source": [
    "gae_model = make_gae_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_gae(label_test_2, gae_model, test_graph_2, test_node_features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 102309.2344 | AUC 0.7763 | Recall 0.6492 | Precision 0.6492 | AP 0.6447 | F1 0.6690 | Time 0.08\n",
      "Epoch 0001: Loss 67907.7891 | AUC 0.7665 | Recall 0.6516 | Precision 0.6516 | AP 0.6314 | F1 0.6686 | Time 0.10\n",
      "Epoch 0002: Loss 45399.6367 | AUC 0.7588 | Recall 0.6574 | Precision 0.6574 | AP 0.6162 | F1 0.6651 | Time 0.09\n",
      "Epoch 0003: Loss 30760.7305 | AUC 0.7462 | Recall 0.6618 | Precision 0.6618 | AP 0.5940 | F1 0.6617 | Time 0.09\n",
      "Epoch 0004: Loss 21408.2969 | AUC 0.7249 | Recall 0.6318 | Precision 0.6318 | AP 0.5648 | F1 0.6305 | Time 0.09\n",
      "Epoch 0005: Loss 19268.4219 | AUC 0.6746 | Recall 0.5032 | Precision 0.5032 | AP 0.4991 | F1 0.5025 | Time 0.10\n",
      "Epoch 0006: Loss 23888.2012 | AUC 0.6934 | Recall 0.6150 | Precision 0.6150 | AP 0.5351 | F1 0.6085 | Time 0.10\n",
      "Epoch 0007: Loss 29230.3398 | AUC 0.6928 | Recall 0.6176 | Precision 0.6176 | AP 0.5395 | F1 0.6176 | Time 0.10\n",
      "Epoch 0008: Loss 29762.4082 | AUC 0.6924 | Recall 0.6174 | Precision 0.6174 | AP 0.5395 | F1 0.6174 | Time 0.09\n",
      "Epoch 0009: Loss 26305.8574 | AUC 0.6946 | Recall 0.6168 | Precision 0.6168 | AP 0.5391 | F1 0.6051 | Time 0.09\n",
      "Epoch 0010: Loss 22066.1016 | AUC 0.6940 | Recall 0.6133 | Precision 0.6133 | AP 0.5321 | F1 0.6122 | Time 0.09\n",
      "Epoch 0011: Loss 19416.7871 | AUC 0.6751 | Recall 0.5055 | Precision 0.5055 | AP 0.5036 | F1 0.5052 | Time 0.09\n",
      "Epoch 0012: Loss 19285.7812 | AUC 0.6833 | Recall 0.4929 | Precision 0.4929 | AP 0.5064 | F1 0.4929 | Time 0.10\n",
      "Epoch 0013: Loss 20564.3887 | AUC 0.7021 | Recall 0.5280 | Precision 0.5280 | AP 0.5440 | F1 0.5584 | Time 0.09\n",
      "Epoch 0014: Loss 21873.8711 | AUC 0.7311 | Recall 0.6366 | Precision 0.6366 | AP 0.5691 | F1 0.6299 | Time 0.08\n",
      "Epoch 0015: Loss 22252.3945 | AUC 0.7329 | Recall 0.6390 | Precision 0.6390 | AP 0.5716 | F1 0.6298 | Time 0.08\n",
      "Epoch 0016: Loss 21568.4141 | AUC 0.7266 | Recall 0.6326 | Precision 0.6326 | AP 0.5658 | F1 0.6302 | Time 0.09\n",
      "Epoch 0017: Loss 20260.4102 | AUC 0.6972 | Recall 0.5228 | Precision 0.5228 | AP 0.5368 | F1 0.5219 | Time 0.09\n",
      "Epoch 0018: Loss 19258.4102 | AUC 0.6828 | Recall 0.4929 | Precision 0.4929 | AP 0.5054 | F1 0.4910 | Time 0.08\n",
      "Epoch 0019: Loss 19124.5703 | AUC 0.6762 | Recall 0.4975 | Precision 0.4975 | AP 0.4958 | F1 0.4975 | Time 0.09\n",
      "Epoch 0020: Loss 19768.0000 | AUC 0.6784 | Recall 0.5084 | Precision 0.5084 | AP 0.5120 | F1 0.5067 | Time 0.08\n",
      "Epoch 0021: Loss 20319.8047 | AUC 0.6868 | Recall 0.6094 | Precision 0.6094 | AP 0.5263 | F1 0.6092 | Time 0.10\n",
      "Epoch 0022: Loss 20274.2324 | AUC 0.6851 | Recall 0.6094 | Precision 0.6094 | AP 0.5245 | F1 0.6094 | Time 0.10\n",
      "Epoch 0023: Loss 19719.1582 | AUC 0.6782 | Recall 0.5088 | Precision 0.5088 | AP 0.5112 | F1 0.5076 | Time 0.10\n",
      "Epoch 0024: Loss 19188.6426 | AUC 0.6760 | Recall 0.5008 | Precision 0.5008 | AP 0.4979 | F1 0.4891 | Time 0.09\n",
      "Epoch 0025: Loss 19079.8008 | AUC 0.6009 | Recall 0.4736 | Precision 0.4736 | AP 0.4652 | F1 0.4721 | Time 0.09\n",
      "Epoch 0026: Loss 19362.4492 | AUC 0.6819 | Recall 0.5006 | Precision 0.5006 | AP 0.5077 | F1 0.4900 | Time 0.07\n",
      "Epoch 0027: Loss 19674.8652 | AUC 0.6865 | Recall 0.5090 | Precision 0.5090 | AP 0.5192 | F1 0.5070 | Time 0.08\n",
      "Epoch 0028: Loss 19682.8633 | AUC 0.6868 | Recall 0.5093 | Precision 0.5093 | AP 0.5195 | F1 0.5070 | Time 0.09\n",
      "Epoch 0029: Loss 19426.8184 | AUC 0.6825 | Recall 0.5047 | Precision 0.5047 | AP 0.5095 | F1 0.5046 | Time 0.08\n",
      "Epoch 0030: Loss 19142.9746 | AUC 0.6824 | Recall 0.4829 | Precision 0.4829 | AP 0.4972 | F1 0.4829 | Time 0.10\n",
      "Epoch 0031: Loss 19067.5977 | AUC 0.6773 | Recall 0.4818 | Precision 0.4818 | AP 0.4874 | F1 0.4809 | Time 0.08\n",
      "Epoch 0032: Loss 19207.9004 | AUC 0.6750 | Recall 0.5019 | Precision 0.5019 | AP 0.4976 | F1 0.5011 | Time 0.09\n",
      "Epoch 0033: Loss 19358.6152 | AUC 0.6752 | Recall 0.5051 | Precision 0.5051 | AP 0.5023 | F1 0.5051 | Time 0.09\n",
      "Epoch 0034: Loss 19343.4023 | AUC 0.6753 | Recall 0.5049 | Precision 0.5049 | AP 0.5019 | F1 0.5028 | Time 0.08\n",
      "Epoch 0035: Loss 19194.7461 | AUC 0.6763 | Recall 0.5008 | Precision 0.5008 | AP 0.4982 | F1 0.4888 | Time 0.09\n",
      "Epoch 0036: Loss 19073.1855 | AUC 0.6763 | Recall 0.4820 | Precision 0.4820 | AP 0.4880 | F1 0.4807 | Time 0.09\n",
      "Epoch 0037: Loss 19084.9395 | AUC 0.6829 | Recall 0.4751 | Precision 0.4751 | AP 0.4903 | F1 0.4734 | Time 0.09\n",
      "Epoch 0038: Loss 19182.0059 | AUC 0.6831 | Recall 0.4920 | Precision 0.4920 | AP 0.5013 | F1 0.4847 | Time 0.10\n",
      "Epoch 0039: Loss 19234.0000 | AUC 0.6824 | Recall 0.4931 | Precision 0.4931 | AP 0.5049 | F1 0.4931 | Time 0.08\n",
      "Epoch 0040: Loss 19186.9648 | AUC 0.6828 | Recall 0.4920 | Precision 0.4920 | AP 0.5020 | F1 0.4837 | Time 0.10\n",
      "Epoch 0041: Loss 19099.7754 | AUC 0.6821 | Recall 0.4792 | Precision 0.4792 | AP 0.4914 | F1 0.4780 | Time 0.09\n",
      "Epoch 0042: Loss 19061.0176 | AUC 0.6786 | Recall 0.4759 | Precision 0.4759 | AP 0.4857 | F1 0.4752 | Time 0.09\n",
      "Epoch 0043: Loss 19095.4004 | AUC 0.6761 | Recall 0.4947 | Precision 0.4947 | AP 0.4926 | F1 0.4862 | Time 0.09\n",
      "Epoch 0044: Loss 19147.6934 | AUC 0.6771 | Recall 0.4982 | Precision 0.4982 | AP 0.4977 | F1 0.4928 | Time 0.10\n",
      "Epoch 0045: Loss 19153.5098 | AUC 0.6776 | Recall 0.4990 | Precision 0.4990 | AP 0.4980 | F1 0.4923 | Time 0.10\n",
      "Epoch 0046: Loss 19109.9023 | AUC 0.6764 | Recall 0.4953 | Precision 0.4953 | AP 0.4946 | F1 0.4935 | Time 0.10\n",
      "Epoch 0047: Loss 19067.3027 | AUC 0.6778 | Recall 0.4797 | Precision 0.4797 | AP 0.4874 | F1 0.4788 | Time 0.11\n",
      "Epoch 0048: Loss 19066.2539 | AUC 0.4002 | Recall 0.3473 | Precision 0.3473 | AP 0.3681 | F1 0.3851 | Time 0.10\n",
      "Epoch 0049: Loss 19096.3516 | AUC 0.6818 | Recall 0.4768 | Precision 0.4768 | AP 0.4908 | F1 0.4768 | Time 0.09\n",
      "Epoch 0050: Loss 19116.3379 | AUC 0.6816 | Recall 0.4814 | Precision 0.4814 | AP 0.4933 | F1 0.4804 | Time 0.09\n",
      "Epoch 0051: Loss 19103.5156 | AUC 0.6817 | Recall 0.4796 | Precision 0.4796 | AP 0.4916 | F1 0.4793 | Time 0.09\n",
      "Epoch 0052: Loss 19074.5918 | AUC 0.6826 | Recall 0.4735 | Precision 0.4735 | AP 0.4883 | F1 0.4726 | Time 0.08\n",
      "Epoch 0053: Loss 19060.9590 | AUC 0.6800 | Recall 0.4760 | Precision 0.4760 | AP 0.4862 | F1 0.4753 | Time 0.08\n",
      "Epoch 0054: Loss 19072.4297 | AUC 0.6779 | Recall 0.4818 | Precision 0.4818 | AP 0.4886 | F1 0.4811 | Time 0.09\n",
      "Epoch 0055: Loss 19089.7852 | AUC 0.6764 | Recall 0.4945 | Precision 0.4945 | AP 0.4920 | F1 0.4880 | Time 0.09\n",
      "Epoch 0056: Loss 19090.6797 | AUC 0.6765 | Recall 0.4947 | Precision 0.4947 | AP 0.4922 | F1 0.4881 | Time 0.11\n",
      "Epoch 0057: Loss 19075.2305 | AUC 0.6780 | Recall 0.4816 | Precision 0.4816 | AP 0.4890 | F1 0.4813 | Time 0.09\n",
      "Epoch 0058: Loss 19062.0430 | AUC 0.6786 | Recall 0.4783 | Precision 0.4783 | AP 0.4866 | F1 0.4780 | Time 0.08\n",
      "Epoch 0059: Loss 19064.0371 | AUC 0.6785 | Recall 0.4731 | Precision 0.4731 | AP 0.4806 | F1 0.4725 | Time 0.10\n",
      "Epoch 0060: Loss 19074.8613 | AUC 0.6824 | Recall 0.4735 | Precision 0.4735 | AP 0.4884 | F1 0.4723 | Time 0.11\n",
      "Epoch 0061: Loss 19079.4238 | AUC 0.6825 | Recall 0.4757 | Precision 0.4757 | AP 0.4894 | F1 0.4737 | Time 0.10\n",
      "Epoch 0062: Loss 19072.5332 | AUC 0.6825 | Recall 0.4735 | Precision 0.4735 | AP 0.4878 | F1 0.4727 | Time 0.10\n",
      "Epoch 0063: Loss 19063.0898 | AUC 0.6799 | Recall 0.4735 | Precision 0.4735 | AP 0.4833 | F1 0.4723 | Time 0.08\n",
      "Epoch 0064: Loss 19061.5195 | AUC 0.6798 | Recall 0.4759 | Precision 0.4759 | AP 0.4869 | F1 0.4750 | Time 0.09\n",
      "Epoch 0065: Loss 19067.4316 | AUC 0.6791 | Recall 0.4796 | Precision 0.4796 | AP 0.4882 | F1 0.4794 | Time 0.09\n",
      "Epoch 0066: Loss 19071.7871 | AUC 0.6784 | Recall 0.4797 | Precision 0.4797 | AP 0.4888 | F1 0.4798 | Time 0.09\n",
      "Epoch 0067: Loss 19069.0059 | AUC 0.6794 | Recall 0.4799 | Precision 0.4799 | AP 0.4887 | F1 0.4799 | Time 0.11\n",
      "Epoch 0068: Loss 19063.0312 | AUC 0.6792 | Recall 0.4788 | Precision 0.4788 | AP 0.4872 | F1 0.4775 | Time 0.10\n",
      "Epoch 0069: Loss 19060.9922 | AUC 0.6813 | Recall 0.4759 | Precision 0.4759 | AP 0.4865 | F1 0.4735 | Time 0.11\n",
      "Epoch 0070: Loss 19064.0957 | AUC 0.6790 | Recall 0.4731 | Precision 0.4731 | AP 0.4808 | F1 0.4731 | Time 0.09\n",
      "Epoch 0071: Loss 19067.2051 | AUC 0.6803 | Recall 0.4733 | Precision 0.4733 | AP 0.4825 | F1 0.4733 | Time 0.08\n",
      "Epoch 0072: Loss 19066.1230 | AUC 0.6755 | Recall 0.4731 | Precision 0.4731 | AP 0.4807 | F1 0.4705 | Time 0.12\n",
      "Epoch 0073: Loss 19062.5156 | AUC 0.6802 | Recall 0.4736 | Precision 0.4736 | AP 0.4841 | F1 0.4724 | Time 0.09\n",
      "Epoch 0074: Loss 19060.8945 | AUC 0.6812 | Recall 0.4759 | Precision 0.4759 | AP 0.4871 | F1 0.4759 | Time 0.11\n",
      "Epoch 0075: Loss 19062.5801 | AUC 0.6802 | Recall 0.4773 | Precision 0.4773 | AP 0.4876 | F1 0.4765 | Time 0.10\n",
      "Epoch 0076: Loss 19064.5918 | AUC 0.6794 | Recall 0.4788 | Precision 0.4788 | AP 0.4879 | F1 0.4774 | Time 0.10\n",
      "Epoch 0077: Loss 19064.0859 | AUC 0.6797 | Recall 0.4788 | Precision 0.4788 | AP 0.4876 | F1 0.4774 | Time 0.10\n",
      "Epoch 0078: Loss 19061.8965 | AUC 0.6802 | Recall 0.4760 | Precision 0.4760 | AP 0.4873 | F1 0.4751 | Time 0.09\n",
      "Epoch 0079: Loss 19060.8730 | AUC 0.6814 | Recall 0.4759 | Precision 0.4759 | AP 0.4870 | F1 0.4735 | Time 0.10\n",
      "Epoch 0080: Loss 19061.8945 | AUC 0.6805 | Recall 0.4736 | Precision 0.4736 | AP 0.4846 | F1 0.4722 | Time 0.09\n",
      "Epoch 0081: Loss 19063.1074 | AUC 0.6806 | Recall 0.4731 | Precision 0.4731 | AP 0.4836 | F1 0.4725 | Time 0.10\n",
      "Epoch 0082: Loss 19062.7559 | AUC 0.6809 | Recall 0.4733 | Precision 0.4733 | AP 0.4842 | F1 0.4726 | Time 0.09\n",
      "Epoch 0083: Loss 19061.4160 | AUC 0.6810 | Recall 0.4738 | Precision 0.4738 | AP 0.4854 | F1 0.4721 | Time 0.09\n",
      "Epoch 0084: Loss 19060.8633 | AUC 0.6815 | Recall 0.4757 | Precision 0.4757 | AP 0.4872 | F1 0.4733 | Time 0.09\n",
      "Epoch 0085: Loss 19061.5488 | AUC 0.6813 | Recall 0.4759 | Precision 0.4759 | AP 0.4878 | F1 0.4753 | Time 0.09\n",
      "Epoch 0086: Loss 19062.2363 | AUC 0.6807 | Recall 0.4766 | Precision 0.4766 | AP 0.4878 | F1 0.4766 | Time 0.11\n",
      "Epoch 0087: Loss 19061.9121 | AUC 0.6812 | Recall 0.4760 | Precision 0.4760 | AP 0.4879 | F1 0.4753 | Time 0.09\n",
      "Epoch 0088: Loss 19061.0918 | AUC 0.6816 | Recall 0.4759 | Precision 0.4759 | AP 0.4878 | F1 0.4757 | Time 0.10\n",
      "Epoch 0089: Loss 19060.8691 | AUC 0.6816 | Recall 0.4740 | Precision 0.4740 | AP 0.4868 | F1 0.4739 | Time 0.10\n",
      "Epoch 0090: Loss 19061.3496 | AUC 0.6812 | Recall 0.4738 | Precision 0.4738 | AP 0.4858 | F1 0.4723 | Time 0.08\n",
      "Epoch 0091: Loss 19061.6934 | AUC 0.6808 | Recall 0.4733 | Precision 0.4733 | AP 0.4849 | F1 0.4724 | Time 0.10\n",
      "Epoch 0092: Loss 19061.3906 | AUC 0.6812 | Recall 0.4738 | Precision 0.4738 | AP 0.4858 | F1 0.4723 | Time 0.09\n",
      "Epoch 0093: Loss 19060.9102 | AUC 0.6814 | Recall 0.4738 | Precision 0.4738 | AP 0.4866 | F1 0.4737 | Time 0.11\n",
      "Epoch 0094: Loss 19060.8828 | AUC 0.6816 | Recall 0.4757 | Precision 0.4757 | AP 0.4875 | F1 0.4737 | Time 0.11\n",
      "Epoch 0095: Loss 19061.2168 | AUC 0.6817 | Recall 0.4759 | Precision 0.4759 | AP 0.4879 | F1 0.4757 | Time 0.09\n",
      "Epoch 0096: Loss 19061.3359 | AUC 0.6817 | Recall 0.4759 | Precision 0.4759 | AP 0.4880 | F1 0.4757 | Time 0.09\n",
      "Epoch 0097: Loss 19061.0762 | AUC 0.6817 | Recall 0.4759 | Precision 0.4759 | AP 0.4879 | F1 0.4757 | Time 0.10\n",
      "Epoch 0098: Loss 19060.8242 | AUC 0.6818 | Recall 0.4755 | Precision 0.4755 | AP 0.4875 | F1 0.4751 | Time 0.09\n",
      "Epoch 0099: Loss 19060.8965 | AUC 0.6816 | Recall 0.4738 | Precision 0.4738 | AP 0.4867 | F1 0.4737 | Time 0.09\n",
      "Test: Loss 2.2105 | AUC 0.7338 | Recall 0.7187 | Precision 0.7187 | AP 0.6785 | F1 0.7545 | Time 0.03\n",
      "F1 score:  tensor(0.7187)\n",
      "Precision:  tensor(0.7187)\n",
      "Recall:  tensor(0.7187)\n"
     ]
    }
   ],
   "source": [
    "gae_model = make_gae_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_gae(label_test_2, gae_model, test_graph_2, test_node_features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 75579.4141 | AUC 0.7804 | Recall 0.6536 | Precision 0.6536 | AP 0.6499 | F1 0.6940 | Time 0.08\n",
      "Epoch 0001: Loss 36112.0898 | AUC 0.7673 | Recall 0.6503 | Precision 0.6503 | AP 0.6330 | F1 0.6907 | Time 0.08\n",
      "Epoch 0002: Loss 14729.1211 | AUC 0.7607 | Recall 0.6523 | Precision 0.6523 | AP 0.6214 | F1 0.6919 | Time 0.09\n",
      "Epoch 0003: Loss 8175.8115 | AUC 0.7263 | Recall 0.6770 | Precision 0.6770 | AP 0.5479 | F1 0.6799 | Time 0.07\n",
      "Epoch 0004: Loss 13862.7412 | AUC 0.7565 | Recall 0.6455 | Precision 0.6455 | AP 0.6182 | F1 0.6886 | Time 0.08\n",
      "Epoch 0005: Loss 19502.8555 | AUC 0.7600 | Recall 0.6449 | Precision 0.6449 | AP 0.6242 | F1 0.6884 | Time 0.09\n",
      "Epoch 0006: Loss 18901.1074 | AUC 0.7600 | Recall 0.6449 | Precision 0.6449 | AP 0.6241 | F1 0.6884 | Time 0.08\n",
      "Epoch 0007: Loss 14229.8262 | AUC 0.7564 | Recall 0.6447 | Precision 0.6447 | AP 0.6184 | F1 0.6883 | Time 0.09\n",
      "Epoch 0008: Loss 9920.7012 | AUC 0.7438 | Recall 0.6521 | Precision 0.6521 | AP 0.6070 | F1 0.6937 | Time 0.08\n",
      "Epoch 0009: Loss 8170.6470 | AUC 0.7269 | Recall 0.6774 | Precision 0.6774 | AP 0.5484 | F1 0.6802 | Time 0.08\n",
      "Epoch 0010: Loss 9005.8154 | AUC 0.7284 | Recall 0.6599 | Precision 0.6599 | AP 0.5753 | F1 0.6685 | Time 0.09\n",
      "Epoch 0011: Loss 10772.4873 | AUC 0.7521 | Recall 0.6542 | Precision 0.6542 | AP 0.6131 | F1 0.6937 | Time 0.09\n",
      "Epoch 0012: Loss 11995.3145 | AUC 0.7574 | Recall 0.6536 | Precision 0.6536 | AP 0.6165 | F1 0.6930 | Time 0.07\n",
      "Epoch 0013: Loss 12059.3467 | AUC 0.7574 | Recall 0.6538 | Precision 0.6538 | AP 0.6165 | F1 0.6931 | Time 0.08\n",
      "Epoch 0014: Loss 11113.2979 | AUC 0.7556 | Recall 0.6538 | Precision 0.6538 | AP 0.6152 | F1 0.6933 | Time 0.08\n",
      "Epoch 0015: Loss 9740.2637 | AUC 0.7342 | Recall 0.6538 | Precision 0.6538 | AP 0.5922 | F1 0.6649 | Time 0.09\n",
      "Epoch 0016: Loss 8598.6270 | AUC 0.7255 | Recall 0.6649 | Precision 0.6649 | AP 0.5655 | F1 0.6720 | Time 0.07\n",
      "Epoch 0017: Loss 8155.6567 | AUC 0.7300 | Recall 0.6838 | Precision 0.6838 | AP 0.5338 | F1 0.6844 | Time 0.08\n",
      "Epoch 0018: Loss 8480.6885 | AUC 0.7236 | Recall 0.6635 | Precision 0.6635 | AP 0.5641 | F1 0.6710 | Time 0.08\n",
      "Epoch 0019: Loss 9203.8691 | AUC 0.7308 | Recall 0.6547 | Precision 0.6547 | AP 0.5849 | F1 0.6652 | Time 0.09\n",
      "Epoch 0020: Loss 9733.7578 | AUC 0.7408 | Recall 0.6512 | Precision 0.6512 | AP 0.6046 | F1 0.6629 | Time 0.07\n",
      "Epoch 0021: Loss 9711.5742 | AUC 0.7393 | Recall 0.6518 | Precision 0.6518 | AP 0.6022 | F1 0.6631 | Time 0.08\n",
      "Epoch 0022: Loss 9216.5654 | AUC 0.7310 | Recall 0.6547 | Precision 0.6547 | AP 0.5853 | F1 0.6652 | Time 0.08\n",
      "Epoch 0023: Loss 8604.5371 | AUC 0.7258 | Recall 0.6612 | Precision 0.6612 | AP 0.5691 | F1 0.6696 | Time 0.08\n",
      "Epoch 0024: Loss 8215.6816 | AUC 0.7251 | Recall 0.6738 | Precision 0.6738 | AP 0.5549 | F1 0.6779 | Time 0.08\n",
      "Epoch 0025: Loss 8182.9331 | AUC 0.7300 | Recall 0.6811 | Precision 0.6811 | AP 0.5502 | F1 0.6826 | Time 0.09\n",
      "Epoch 0026: Loss 8416.1045 | AUC 0.7251 | Recall 0.6683 | Precision 0.6683 | AP 0.5619 | F1 0.6744 | Time 0.08\n",
      "Epoch 0027: Loss 8709.7119 | AUC 0.7268 | Recall 0.6640 | Precision 0.6640 | AP 0.5688 | F1 0.6713 | Time 0.08\n",
      "Epoch 0028: Loss 8877.1475 | AUC 0.7281 | Recall 0.6618 | Precision 0.6618 | AP 0.5731 | F1 0.6699 | Time 0.08\n",
      "Epoch 0029: Loss 8836.9902 | AUC 0.7278 | Recall 0.6620 | Precision 0.6620 | AP 0.5723 | F1 0.6700 | Time 0.08\n",
      "Epoch 0030: Loss 8628.3164 | AUC 0.7255 | Recall 0.6640 | Precision 0.6640 | AP 0.5663 | F1 0.6713 | Time 0.08\n",
      "Epoch 0031: Loss 8369.8105 | AUC 0.7258 | Recall 0.6707 | Precision 0.6707 | AP 0.5628 | F1 0.6760 | Time 0.08\n",
      "Epoch 0032: Loss 8190.7056 | AUC 0.7293 | Recall 0.6809 | Precision 0.6809 | AP 0.5502 | F1 0.6825 | Time 0.08\n",
      "Epoch 0033: Loss 8163.4160 | AUC 0.7282 | Recall 0.6794 | Precision 0.6794 | AP 0.5484 | F1 0.6815 | Time 0.08\n",
      "Epoch 0034: Loss 8267.8516 | AUC 0.7250 | Recall 0.6711 | Precision 0.6711 | AP 0.5607 | F1 0.6763 | Time 0.08\n",
      "Epoch 0035: Loss 8409.3975 | AUC 0.7226 | Recall 0.6648 | Precision 0.6648 | AP 0.5610 | F1 0.6720 | Time 0.08\n",
      "Epoch 0036: Loss 8483.7822 | AUC 0.7235 | Recall 0.6636 | Precision 0.6636 | AP 0.5638 | F1 0.6710 | Time 0.08\n",
      "Epoch 0037: Loss 8445.4307 | AUC 0.7232 | Recall 0.6642 | Precision 0.6642 | AP 0.5622 | F1 0.6716 | Time 0.08\n",
      "Epoch 0038: Loss 8328.5176 | AUC 0.7238 | Recall 0.6675 | Precision 0.6675 | AP 0.5616 | F1 0.6738 | Time 0.07\n",
      "Epoch 0039: Loss 8210.8525 | AUC 0.7254 | Recall 0.6748 | Precision 0.6748 | AP 0.5534 | F1 0.6785 | Time 0.08\n",
      "Epoch 0040: Loss 8156.1626 | AUC 0.7301 | Recall 0.6811 | Precision 0.6811 | AP 0.5478 | F1 0.6826 | Time 0.08\n",
      "Epoch 0041: Loss 8177.8564 | AUC 0.7307 | Recall 0.6818 | Precision 0.6818 | AP 0.5502 | F1 0.6831 | Time 0.08\n",
      "Epoch 0042: Loss 8241.8525 | AUC 0.7271 | Recall 0.6770 | Precision 0.6770 | AP 0.5553 | F1 0.6800 | Time 0.08\n",
      "Epoch 0043: Loss 8296.6494 | AUC 0.7269 | Recall 0.6746 | Precision 0.6746 | AP 0.5618 | F1 0.6784 | Time 0.09\n",
      "Epoch 0044: Loss 8306.4912 | AUC 0.7267 | Recall 0.6740 | Precision 0.6740 | AP 0.5623 | F1 0.6780 | Time 0.09\n",
      "Epoch 0045: Loss 8268.7578 | AUC 0.7269 | Recall 0.6755 | Precision 0.6755 | AP 0.5589 | F1 0.6790 | Time 0.08\n",
      "Epoch 0046: Loss 8209.7666 | AUC 0.7281 | Recall 0.6787 | Precision 0.6787 | AP 0.5524 | F1 0.6810 | Time 0.09\n",
      "Epoch 0047: Loss 8164.9976 | AUC 0.7319 | Recall 0.6838 | Precision 0.6838 | AP 0.5499 | F1 0.6845 | Time 0.08\n",
      "Epoch 0048: Loss 8156.4580 | AUC 0.7302 | Recall 0.6811 | Precision 0.6811 | AP 0.5483 | F1 0.6826 | Time 0.08\n",
      "Epoch 0049: Loss 8180.4937 | AUC 0.7269 | Recall 0.6775 | Precision 0.6775 | AP 0.5504 | F1 0.6803 | Time 0.08\n",
      "Epoch 0050: Loss 8213.0742 | AUC 0.7255 | Recall 0.6748 | Precision 0.6748 | AP 0.5543 | F1 0.6785 | Time 0.08\n",
      "Epoch 0051: Loss 8228.3018 | AUC 0.7257 | Recall 0.6744 | Precision 0.6744 | AP 0.5572 | F1 0.6780 | Time 0.09\n",
      "Epoch 0052: Loss 8216.3105 | AUC 0.7255 | Recall 0.6742 | Precision 0.6742 | AP 0.5555 | F1 0.6782 | Time 0.07\n",
      "Epoch 0053: Loss 8187.6211 | AUC 0.7263 | Recall 0.6775 | Precision 0.6775 | AP 0.5504 | F1 0.6801 | Time 0.08\n",
      "Epoch 0054: Loss 8162.4487 | AUC 0.7283 | Recall 0.6798 | Precision 0.6798 | AP 0.5485 | F1 0.6818 | Time 0.07\n",
      "Epoch 0055: Loss 8155.1128 | AUC 0.7310 | Recall 0.6838 | Precision 0.6838 | AP 0.5409 | F1 0.6844 | Time 0.09\n",
      "Epoch 0056: Loss 8165.4629 | AUC 0.7319 | Recall 0.6838 | Precision 0.6838 | AP 0.5500 | F1 0.6845 | Time 0.08\n",
      "Epoch 0057: Loss 8181.6973 | AUC 0.7295 | Recall 0.6809 | Precision 0.6809 | AP 0.5496 | F1 0.6825 | Time 0.08\n",
      "Epoch 0058: Loss 8190.4541 | AUC 0.7289 | Recall 0.6805 | Precision 0.6805 | AP 0.5499 | F1 0.6823 | Time 0.08\n",
      "Epoch 0059: Loss 8185.8774 | AUC 0.7294 | Recall 0.6809 | Precision 0.6809 | AP 0.5502 | F1 0.6825 | Time 0.08\n",
      "Epoch 0060: Loss 8172.1978 | AUC 0.7315 | Recall 0.6833 | Precision 0.6833 | AP 0.5507 | F1 0.6841 | Time 0.09\n",
      "Epoch 0061: Loss 8159.2627 | AUC 0.7317 | Recall 0.6844 | Precision 0.6844 | AP 0.5445 | F1 0.6848 | Time 0.08\n",
      "Epoch 0062: Loss 8154.9482 | AUC 0.7314 | Recall 0.6838 | Precision 0.6838 | AP 0.5447 | F1 0.6845 | Time 0.08\n",
      "Epoch 0063: Loss 8159.9463 | AUC 0.7291 | Recall 0.6805 | Precision 0.6805 | AP 0.5488 | F1 0.6822 | Time 0.07\n",
      "Epoch 0064: Loss 8168.2749 | AUC 0.7279 | Recall 0.6792 | Precision 0.6792 | AP 0.5486 | F1 0.6813 | Time 0.08\n",
      "Epoch 0065: Loss 8172.5684 | AUC 0.7276 | Recall 0.6790 | Precision 0.6790 | AP 0.5491 | F1 0.6810 | Time 0.08\n",
      "Epoch 0066: Loss 8169.6953 | AUC 0.7277 | Recall 0.6792 | Precision 0.6792 | AP 0.5486 | F1 0.6813 | Time 0.08\n",
      "Epoch 0067: Loss 8162.4033 | AUC 0.7285 | Recall 0.6799 | Precision 0.6799 | AP 0.5486 | F1 0.6819 | Time 0.07\n",
      "Epoch 0068: Loss 8156.2891 | AUC 0.7311 | Recall 0.6824 | Precision 0.6824 | AP 0.5489 | F1 0.6835 | Time 0.08\n",
      "Epoch 0069: Loss 8155.1299 | AUC 0.7311 | Recall 0.6837 | Precision 0.6837 | AP 0.5406 | F1 0.6844 | Time 0.14\n",
      "Epoch 0070: Loss 8158.4102 | AUC 0.7318 | Recall 0.6844 | Precision 0.6844 | AP 0.5439 | F1 0.6848 | Time 0.10\n",
      "Epoch 0071: Loss 8162.4688 | AUC 0.7315 | Recall 0.6837 | Precision 0.6837 | AP 0.5472 | F1 0.6844 | Time 0.07\n",
      "Epoch 0072: Loss 8163.7627 | AUC 0.7316 | Recall 0.6837 | Precision 0.6837 | AP 0.5482 | F1 0.6844 | Time 0.08\n",
      "Epoch 0073: Loss 8161.4644 | AUC 0.7316 | Recall 0.6838 | Precision 0.6838 | AP 0.5470 | F1 0.6845 | Time 0.08\n",
      "Epoch 0074: Loss 8157.6157 | AUC 0.7315 | Recall 0.6842 | Precision 0.6842 | AP 0.5419 | F1 0.6847 | Time 0.08\n",
      "Epoch 0075: Loss 8155.1191 | AUC 0.7312 | Recall 0.6838 | Precision 0.6838 | AP 0.5409 | F1 0.6845 | Time 0.07\n",
      "Epoch 0076: Loss 8155.3916 | AUC 0.7318 | Recall 0.6838 | Precision 0.6838 | AP 0.5474 | F1 0.6845 | Time 0.09\n",
      "Epoch 0077: Loss 8157.4985 | AUC 0.7303 | Recall 0.6811 | Precision 0.6811 | AP 0.5492 | F1 0.6826 | Time 0.08\n",
      "Epoch 0078: Loss 8159.2495 | AUC 0.7295 | Recall 0.6805 | Precision 0.6805 | AP 0.5489 | F1 0.6822 | Time 0.08\n",
      "Epoch 0079: Loss 8159.1230 | AUC 0.7295 | Recall 0.6805 | Precision 0.6805 | AP 0.5490 | F1 0.6822 | Time 0.09\n",
      "Epoch 0080: Loss 8157.3711 | AUC 0.7306 | Recall 0.6812 | Precision 0.6812 | AP 0.5492 | F1 0.6828 | Time 0.08\n",
      "Epoch 0081: Loss 8155.5146 | AUC 0.7318 | Recall 0.6838 | Precision 0.6838 | AP 0.5478 | F1 0.6845 | Time 0.08\n",
      "Epoch 0082: Loss 8154.9053 | AUC 0.7314 | Recall 0.6837 | Precision 0.6837 | AP 0.5442 | F1 0.6844 | Time 0.08\n",
      "Epoch 0083: Loss 8155.6694 | AUC 0.7303 | Recall 0.6840 | Precision 0.6840 | AP 0.5339 | F1 0.6846 | Time 0.08\n",
      "Epoch 0084: Loss 8156.8198 | AUC 0.7304 | Recall 0.6844 | Precision 0.6844 | AP 0.5339 | F1 0.6848 | Time 0.08\n",
      "Epoch 0085: Loss 8157.2441 | AUC 0.7303 | Recall 0.6842 | Precision 0.6842 | AP 0.5339 | F1 0.6847 | Time 0.09\n",
      "Epoch 0086: Loss 8156.6274 | AUC 0.7304 | Recall 0.6844 | Precision 0.6844 | AP 0.5339 | F1 0.6848 | Time 0.07\n",
      "Epoch 0087: Loss 8155.5649 | AUC 0.7303 | Recall 0.6840 | Precision 0.6840 | AP 0.5340 | F1 0.6846 | Time 0.07\n",
      "Epoch 0088: Loss 8154.9238 | AUC 0.7313 | Recall 0.6837 | Precision 0.6837 | AP 0.5437 | F1 0.6844 | Time 0.09\n",
      "Epoch 0089: Loss 8155.0864 | AUC 0.7317 | Recall 0.6838 | Precision 0.6838 | AP 0.5468 | F1 0.6845 | Time 0.07\n",
      "Epoch 0090: Loss 8155.7021 | AUC 0.7319 | Recall 0.6838 | Precision 0.6838 | AP 0.5480 | F1 0.6845 | Time 0.09\n",
      "Epoch 0091: Loss 8156.1104 | AUC 0.7318 | Recall 0.6835 | Precision 0.6835 | AP 0.5494 | F1 0.6842 | Time 0.09\n",
      "Epoch 0092: Loss 8155.9443 | AUC 0.7318 | Recall 0.6835 | Precision 0.6835 | AP 0.5485 | F1 0.6842 | Time 0.08\n",
      "Epoch 0093: Loss 8155.3970 | AUC 0.7318 | Recall 0.6838 | Precision 0.6838 | AP 0.5474 | F1 0.6845 | Time 0.09\n",
      "Epoch 0094: Loss 8154.9551 | AUC 0.7315 | Recall 0.6838 | Precision 0.6838 | AP 0.5451 | F1 0.6845 | Time 0.07\n",
      "Epoch 0095: Loss 8154.9360 | AUC 0.7313 | Recall 0.6837 | Precision 0.6837 | AP 0.5437 | F1 0.6844 | Time 0.08\n",
      "Epoch 0096: Loss 8155.2397 | AUC 0.7303 | Recall 0.6840 | Precision 0.6840 | AP 0.5340 | F1 0.6846 | Time 0.08\n",
      "Epoch 0097: Loss 8155.5103 | AUC 0.7303 | Recall 0.6840 | Precision 0.6840 | AP 0.5340 | F1 0.6846 | Time 0.08\n",
      "Epoch 0098: Loss 8155.4868 | AUC 0.7303 | Recall 0.6840 | Precision 0.6840 | AP 0.5340 | F1 0.6846 | Time 0.08\n",
      "Epoch 0099: Loss 8155.2129 | AUC 0.7303 | Recall 0.6840 | Precision 0.6840 | AP 0.5340 | F1 0.6846 | Time 0.08\n",
      "Test: Loss 2.3153 | AUC 0.7719 | Recall 0.6861 | Precision 0.6861 | AP 0.7278 | F1 0.7585 | Time 0.02\n",
      "F1 score:  tensor(0.6861)\n",
      "Precision:  tensor(0.6861)\n",
      "Recall:  tensor(0.6861)\n"
     ]
    }
   ],
   "source": [
    "gae_model = make_gae_model(train_graph_3, train_node_features_3, label_train_3)\n",
    "precision_score, recall_score, f1_score = predict_gae(label_test_3, gae_model, test_graph_3, test_node_features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnomalyDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_anomalydae(label_test, anomalydae_compile, test_graph, test_node_features):\n",
    "\n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "    \n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    anomalydae_ip_pred_res, anomalydae_ip_score_res = anomalydae_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear', )\n",
    "    \n",
    "    precision_pygod = eval_precision_at_k(label_test, anomalydae_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, anomalydae_ip_score_res)\n",
    "    f1_score = 2*(precision_pygod*recall_pygod)/(precision_pygod+recall_pygod)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    return  precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_anomalydae_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "\n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "    \n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    \n",
    "    anomalydae_model = AnomalyDAE(epoch=5, verbose=3, gpu=-1)\n",
    "    anomalydae_compile = anomalydae_model.fit(pyG_train)\n",
    "\n",
    "    return anomalydae_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\762223058.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 1250.0575 |  | Time 8.44\n",
      "Epoch 0001: Loss 155065.2656 |  | Time 8.61\n",
      "Epoch 0002: Loss 61847.4414 |  | Time 7.53\n",
      "Epoch 0003: Loss 8255.8564 |  | Time 7.63\n",
      "Epoch 0004: Loss 101.1241 |  | Time 7.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\762223058.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.1138 | AUC 0.8649 | Recall 0.7858 | Precision 0.7858 | AP 0.8266 | F1 0.7858 | Time 2.31\n",
      "F1 score:  tensor(0.7858)\n",
      "Precision:  tensor(0.7858)\n",
      "Recall:  tensor(0.7858)\n"
     ]
    }
   ],
   "source": [
    "anomalydae_model = make_anomalydae_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_anomalydae(label_test, anomalydae_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15473 is out of bounds for dimension 0 with size 15473",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 54\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m anomalydae_model \u001b[39m=\u001b[39m make_anomalydae_model(train_graph_2, train_node_features_2, label_train_2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_anomalydae(label_train_2, anomalydae_model, train_graph_2, train_node_features_2)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 54\u001b[0m in \u001b[0;36mmake_anomalydae_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m pyG_train\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m train_node_features\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m anomalydae_model \u001b[39m=\u001b[39m AnomalyDAE(epoch\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m anomalydae_compile \u001b[39m=\u001b[39m anomalydae_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m anomalydae_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\anomalydae.py:187\u001b[0m, in \u001b[0;36mAnomalyDAE.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    182\u001b[0m pos_weight_a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meta \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meta)\n\u001b[0;32m    183\u001b[0m pos_weight_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta)\n\u001b[0;32m    185\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    186\u001b[0m                              x_[:batch_size],\n\u001b[1;32m--> 187\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    188\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    189\u001b[0m                              weight,\n\u001b[0;32m    190\u001b[0m                              pos_weight_a,\n\u001b[0;32m    191\u001b[0m                              pos_weight_s)\n\u001b[0;32m    193\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n\u001b[0;32m    195\u001b[0m \u001b[39mreturn\u001b[39;00m loss, score\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15473 is out of bounds for dimension 0 with size 15473"
     ]
    }
   ],
   "source": [
    "anomalydae_model = make_anomalydae_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_anomalydae(label_train_2, anomalydae_model, train_graph_2, train_node_features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15473 is out of bounds for dimension 0 with size 15473",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 56\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m anomalydae_model \u001b[39m=\u001b[39m make_anomalydae_model(train_graph_3, train_node_features_3, label_train_3)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_anomalydae(label_test_3, anomalydae_model, test_graph_3, test_node_features_3)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 56\u001b[0m in \u001b[0;36mmake_anomalydae_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m pyG_train\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m train_node_features\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m anomalydae_model \u001b[39m=\u001b[39m AnomalyDAE(epoch\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m anomalydae_compile \u001b[39m=\u001b[39m anomalydae_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m anomalydae_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\anomalydae.py:187\u001b[0m, in \u001b[0;36mAnomalyDAE.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    182\u001b[0m pos_weight_a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meta \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meta)\n\u001b[0;32m    183\u001b[0m pos_weight_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta)\n\u001b[0;32m    185\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    186\u001b[0m                              x_[:batch_size],\n\u001b[1;32m--> 187\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    188\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    189\u001b[0m                              weight,\n\u001b[0;32m    190\u001b[0m                              pos_weight_a,\n\u001b[0;32m    191\u001b[0m                              pos_weight_s)\n\u001b[0;32m    193\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n\u001b[0;32m    195\u001b[0m \u001b[39mreturn\u001b[39;00m loss, score\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15473 is out of bounds for dimension 0 with size 15473"
     ]
    }
   ],
   "source": [
    "anomalydae_model = make_anomalydae_model(train_graph_3, train_node_features_3, label_train_3)\n",
    "precision_score, recall_score, f1_score = predict_anomalydae(label_test_3, anomalydae_model, test_graph_3, test_node_features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_conad(label_test, conda_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "    \n",
    "    conad_ip_pred_res, conad_ip_score_res = conda_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear')\n",
    "    precision = eval_precision_at_k(label_test, conad_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, conad_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def make_conad_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    conad_model = CONAD(epoch=5, gpu=-1, verbose=3)\n",
    "    conad_compile = conad_model.fit(pyG_train)\n",
    "\n",
    "    return conad_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 339767.2500 |  | Time 2.49\n",
      "Epoch 0001: Loss 175259.7656 |  | Time 2.94\n",
      "Epoch 0002: Loss 86086.2656 |  | Time 2.32\n",
      "Epoch 0003: Loss 41946.2266 |  | Time 2.12\n",
      "Epoch 0004: Loss 22432.9570 |  | Time 2.41\n",
      "Epoch 0005: Loss 15288.9619 |  | Time 1.92\n",
      "Epoch 0006: Loss 13692.7227 |  | Time 1.96\n",
      "Epoch 0007: Loss 13934.9629 |  | Time 1.95\n",
      "Epoch 0008: Loss 14214.0293 |  | Time 1.85\n",
      "Epoch 0009: Loss 13812.2324 |  | Time 1.90\n",
      "Epoch 0010: Loss 12621.3506 |  | Time 1.83\n",
      "Epoch 0011: Loss 10911.8545 |  | Time 2.02\n",
      "Epoch 0012: Loss 9074.7607 |  | Time 2.06\n",
      "Epoch 0013: Loss 7414.6177 |  | Time 1.92\n",
      "Epoch 0014: Loss 6072.0015 |  | Time 1.96\n",
      "Epoch 0015: Loss 5046.2222 |  | Time 1.82\n",
      "Epoch 0016: Loss 4261.4849 |  | Time 1.83\n",
      "Epoch 0017: Loss 3630.8293 |  | Time 1.89\n",
      "Epoch 0018: Loss 3094.5962 |  | Time 2.13\n",
      "Epoch 0019: Loss 2629.7449 |  | Time 1.96\n",
      "Epoch 0020: Loss 2238.3459 |  | Time 2.01\n",
      "Epoch 0021: Loss 1928.6136 |  | Time 1.88\n",
      "Epoch 0022: Loss 1700.3059 |  | Time 1.83\n",
      "Epoch 0023: Loss 1540.2194 |  | Time 1.91\n",
      "Epoch 0024: Loss 1426.4956 |  | Time 1.84\n",
      "Epoch 0025: Loss 1336.2688 |  | Time 1.88\n",
      "Epoch 0026: Loss 1251.6387 |  | Time 2.05\n",
      "Epoch 0027: Loss 1162.3311 |  | Time 1.86\n",
      "Epoch 0028: Loss 1065.8495 |  | Time 1.92\n",
      "Epoch 0029: Loss 965.7292 |  | Time 1.89\n",
      "Epoch 0030: Loss 868.3903 |  | Time 1.84\n",
      "Epoch 0031: Loss 779.6990 |  | Time 1.87\n",
      "Epoch 0032: Loss 702.3648 |  | Time 1.95\n",
      "Epoch 0033: Loss 635.0031 |  | Time 1.90\n",
      "Epoch 0034: Loss 573.2000 |  | Time 1.87\n",
      "Epoch 0035: Loss 512.1627 |  | Time 1.82\n",
      "Epoch 0036: Loss 449.6285 |  | Time 1.91\n",
      "Epoch 0037: Loss 387.3607 |  | Time 1.89\n",
      "Epoch 0038: Loss 330.2190 |  | Time 1.90\n",
      "Epoch 0039: Loss 283.3289 |  | Time 1.86\n",
      "Epoch 0040: Loss 249.1082 |  | Time 1.86\n",
      "Epoch 0041: Loss 226.0324 |  | Time 1.89\n",
      "Epoch 0042: Loss 209.7498 |  | Time 1.91\n",
      "Epoch 0043: Loss 195.7309 |  | Time 1.87\n",
      "Epoch 0044: Loss 181.5601 |  | Time 1.94\n",
      "Epoch 0045: Loss 167.5022 |  | Time 2.51\n",
      "Epoch 0046: Loss 155.2543 |  | Time 2.04\n",
      "Epoch 0047: Loss 146.1142 |  | Time 1.97\n",
      "Epoch 0048: Loss 139.8298 |  | Time 2.08\n",
      "Epoch 0049: Loss 134.8110 |  | Time 2.06\n",
      "Epoch 0050: Loss 129.1703 |  | Time 2.01\n",
      "Epoch 0051: Loss 121.7555 |  | Time 2.07\n",
      "Epoch 0052: Loss 112.4803 |  | Time 1.83\n",
      "Epoch 0053: Loss 102.0001 |  | Time 1.88\n",
      "Epoch 0054: Loss 91.1752 |  | Time 2.10\n",
      "Epoch 0055: Loss 80.7381 |  | Time 1.91\n",
      "Epoch 0056: Loss 71.2342 |  | Time 1.87\n",
      "Epoch 0057: Loss 63.0599 |  | Time 1.91\n",
      "Epoch 0058: Loss 56.4173 |  | Time 1.92\n",
      "Epoch 0059: Loss 51.2048 |  | Time 1.86\n",
      "Epoch 0060: Loss 47.0428 |  | Time 1.85\n",
      "Epoch 0061: Loss 43.4086 |  | Time 1.83\n",
      "Epoch 0062: Loss 39.9274 |  | Time 1.89\n",
      "Epoch 0063: Loss 36.5381 |  | Time 1.82\n",
      "Epoch 0064: Loss 33.4832 |  | Time 1.96\n",
      "Epoch 0065: Loss 31.1120 |  | Time 1.94\n",
      "Epoch 0066: Loss 29.6350 |  | Time 1.87\n",
      "Epoch 0067: Loss 28.9740 |  | Time 1.99\n",
      "Epoch 0068: Loss 28.8062 |  | Time 1.94\n",
      "Epoch 0069: Loss 28.6996 |  | Time 1.87\n",
      "Epoch 0070: Loss 28.3222 |  | Time 2.02\n",
      "Epoch 0071: Loss 27.5447 |  | Time 2.19\n",
      "Epoch 0072: Loss 26.4439 |  | Time 2.12\n",
      "Epoch 0073: Loss 25.1992 |  | Time 2.15\n",
      "Epoch 0074: Loss 23.9890 |  | Time 2.02\n",
      "Epoch 0075: Loss 22.9300 |  | Time 1.86\n",
      "Epoch 0076: Loss 22.0649 |  | Time 1.87\n",
      "Epoch 0077: Loss 21.3825 |  | Time 1.91\n",
      "Epoch 0078: Loss 20.8329 |  | Time 1.86\n",
      "Epoch 0079: Loss 20.3536 |  | Time 2.06\n",
      "Epoch 0080: Loss 19.8986 |  | Time 1.83\n",
      "Epoch 0081: Loss 19.4628 |  | Time 2.05\n",
      "Epoch 0082: Loss 19.0694 |  | Time 2.07\n",
      "Epoch 0083: Loss 18.7483 |  | Time 1.99\n",
      "Epoch 0084: Loss 18.5068 |  | Time 2.01\n",
      "Epoch 0085: Loss 18.3138 |  | Time 1.92\n",
      "Epoch 0086: Loss 18.1456 |  | Time 1.92\n",
      "Epoch 0087: Loss 17.9849 |  | Time 2.02\n",
      "Epoch 0088: Loss 17.8287 |  | Time 1.96\n",
      "Epoch 0089: Loss 17.6822 |  | Time 1.90\n",
      "Epoch 0090: Loss 17.5422 |  | Time 1.98\n",
      "Epoch 0091: Loss 17.3977 |  | Time 1.94\n",
      "Epoch 0092: Loss 17.2348 |  | Time 1.95\n",
      "Epoch 0093: Loss 17.0457 |  | Time 1.98\n",
      "Epoch 0094: Loss 16.8484 |  | Time 1.94\n",
      "Epoch 0095: Loss 16.6721 |  | Time 1.93\n",
      "Epoch 0096: Loss 16.5366 |  | Time 1.97\n",
      "Epoch 0097: Loss 16.4442 |  | Time 1.97\n",
      "Epoch 0098: Loss 16.3868 |  | Time 2.01\n",
      "Epoch 0099: Loss 16.3479 |  | Time 2.03\n",
      "Test: Loss 0.0021 | AUC 0.7261 | Recall 0.6004 | Precision 0.6004 | AP 0.6407 | F1 0.6004 | Time 0.60\n",
      "F1 score:  tensor(0.6004)\n",
      "Precision:  tensor(0.6004)\n",
      "Recall:  tensor(0.6004)\n"
     ]
    }
   ],
   "source": [
    "conad_model = make_conad_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_conad(label_test, conad_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [15473] at index 0 does not match the shape of the indexed tensor [15480, 1] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 62\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m conad_model \u001b[39m=\u001b[39m make_conad_model(train_graph_2, train_node_features_2, label_train_2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_conad(label_train_2, conad_model, train_graph_2, train_node_features_2)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 62\u001b[0m in \u001b[0;36mmake_conad_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m pyG_train\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m train_node_features\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m conad_model \u001b[39m=\u001b[39m CONAD(epoch\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m conad_compile \u001b[39m=\u001b[39m conad_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m conad_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\conad.py:197\u001b[0m, in \u001b[0;36mCONAD.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    194\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    196\u001b[0m     x_aug, edge_index_aug, label_aug \u001b[39m=\u001b[39m \\\n\u001b[1;32m--> 197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_augmentation(data)\n\u001b[0;32m    198\u001b[0m     x_aug \u001b[39m=\u001b[39m x_aug\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    199\u001b[0m     edge_index_aug \u001b[39m=\u001b[39m edge_index_aug\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\conad.py:276\u001b[0m, in \u001b[0;36mCONAD._data_augmentation\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    274\u001b[0m dv_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlogical_and(rate \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m prob, prob \u001b[39m<\u001b[39m rate \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m)\n\u001b[0;32m    275\u001b[0m feat_c \u001b[39m=\u001b[39m feat_aug[torch\u001b[39m.\u001b[39mrandperm(batch_size)[:surround]]\n\u001b[1;32m--> 276\u001b[0m ds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcdist(feat_aug[dv_mask], feat_c)\n\u001b[0;32m    277\u001b[0m feat_aug[dv_mask] \u001b[39m=\u001b[39m feat_c[torch\u001b[39m.\u001b[39margmax(ds, \u001b[39m1\u001b[39m)]\n\u001b[0;32m    279\u001b[0m \u001b[39m# disproportionate\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [15473] at index 0 does not match the shape of the indexed tensor [15480, 1] at index 0"
     ]
    }
   ],
   "source": [
    "conad_model = make_conad_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_conad(label_train_2, conad_model, train_graph_2, train_node_features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [15473] at index 0 does not match the shape of the indexed tensor [15480, 1] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 64\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m conad_model \u001b[39m=\u001b[39m make_conad_model(train_graph_3, train_node_features_3, label_train_3)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_conad(label_test_3, conad_model, train_graph_3, test_node_features_3)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 64\u001b[0m in \u001b[0;36mmake_conad_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m pyG_train\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m train_node_features\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m conad_model \u001b[39m=\u001b[39m CONAD(epoch\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m conad_compile \u001b[39m=\u001b[39m conad_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m conad_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\conad.py:197\u001b[0m, in \u001b[0;36mCONAD.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    194\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    196\u001b[0m     x_aug, edge_index_aug, label_aug \u001b[39m=\u001b[39m \\\n\u001b[1;32m--> 197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_augmentation(data)\n\u001b[0;32m    198\u001b[0m     x_aug \u001b[39m=\u001b[39m x_aug\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    199\u001b[0m     edge_index_aug \u001b[39m=\u001b[39m edge_index_aug\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\conad.py:276\u001b[0m, in \u001b[0;36mCONAD._data_augmentation\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    274\u001b[0m dv_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlogical_and(rate \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m prob, prob \u001b[39m<\u001b[39m rate \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m)\n\u001b[0;32m    275\u001b[0m feat_c \u001b[39m=\u001b[39m feat_aug[torch\u001b[39m.\u001b[39mrandperm(batch_size)[:surround]]\n\u001b[1;32m--> 276\u001b[0m ds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcdist(feat_aug[dv_mask], feat_c)\n\u001b[0;32m    277\u001b[0m feat_aug[dv_mask] \u001b[39m=\u001b[39m feat_c[torch\u001b[39m.\u001b[39margmax(ds, \u001b[39m1\u001b[39m)]\n\u001b[0;32m    279\u001b[0m \u001b[39m# disproportionate\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [15473] at index 0 does not match the shape of the indexed tensor [15480, 1] at index 0"
     ]
    }
   ],
   "source": [
    "conad_model = make_conad_model(train_graph_3, train_node_features_3, label_train_3)\n",
    "precision_score, recall_score, f1_score = predict_conad(label_test_3, conad_model, train_graph_3, test_node_features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOMINANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hdominant(label_test, dominant_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "    dominant_ip_pred_res, dominant_ip_score_res = dominant_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True)\n",
    "    \n",
    "    precision_pygod = eval_precision_at_k(label_test, dominant_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, dominant_ip_score_res)\n",
    "    f1_score = 2 * (precision_pygod * recall_pygod) / (precision_pygod + recall_pygod)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    return precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_hdominant_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    dominant_model = DOMINANT(gpu=0, weight=1, weight_decay=3, num_layers=16, hid_dim=16, contamination=0.37, lr=0.001, verbose=3, epoch=5)\n",
    "    dominant_compile = dominant_model.fit(pyG_train)\n",
    "    return dominant_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\3288320421.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\3288320421.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_train = torch.tensor(label_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 75.1450 |  | Time 7.23\n",
      "Epoch 0001: Loss 75.1340 |  | Time 0.75\n",
      "Epoch 0002: Loss 75.1235 |  | Time 0.68\n",
      "Epoch 0003: Loss 75.1135 |  | Time 0.73\n",
      "Epoch 0004: Loss 75.1038 |  | Time 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\3288320421.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0098 | AUC 0.8024 | Recall 0.6911 | Precision 0.6911 | AP 0.7535 | F1 0.6911 | Time 0.18\n",
      "F1 score:  tensor(0.6911)\n",
      "Precision:  tensor(0.6911)\n",
      "Recall:  tensor(0.6911)\n"
     ]
    }
   ],
   "source": [
    "dominant_model = make_hdominant_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_hdominant(label_test, dominant_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hocgnn(label_test, ocgnn_compile, pyG_test, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(pyG_test)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    ocgnn_ip_pred_res, ocgnn_ip_score_res = ocgnn_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "\n",
    "    precision_pygod = eval_precision_at_k(label_test, ocgnn_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, ocgnn_ip_score_res)\n",
    "    f1_score = 2 * (precision_pygod * recall_pygod) / (precision_pygod + recall_pygod)\n",
    "    f1_pygod = eval_f1(label_test, ocgnn_ip_pred_res)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    print(\"F1 pygod: \", f1_pygod)\n",
    "\n",
    "    return precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_hocgnn_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    ocgnn_model = OCGNN(hid_dim=10, num_layers=14, weight_decay=1, \n",
    "                    contamination=0.37, lr=0.001, epoch=5, gpu=-1, \n",
    "                    verbose=3)\n",
    "    ocgnn_compile = ocgnn_model.fit(pyG_train)\n",
    "    return ocgnn_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\674703128.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 53.9027 |  | Time 0.24\n",
      "Epoch 0001: Loss 29.0920 |  | Time 0.24\n",
      "Epoch 0002: Loss 24.1405 |  | Time 0.24\n",
      "Epoch 0003: Loss 19.9970 |  | Time 0.27\n",
      "Epoch 0004: Loss 16.7464 |  | Time 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\674703128.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0020 | AUC 0.3243 | Recall 0.2886 | Precision 0.2886 | AP 0.5252 | F1 0.2886 | Time 0.05\n",
      "F1 score:  tensor(0.2886)\n",
      "Precision:  tensor(0.2886)\n",
      "Recall:  tensor(0.2886)\n",
      "F1 pygod:  0.2951967173545739\n"
     ]
    }
   ],
   "source": [
    "ocgnn_model = make_hocgnn_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_hocgnn(label_test, ocgnn_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hgae(label_test, gae_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    gae_ip_pred_res, gae_ip_score_res = gae_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "    precision = eval_precision_at_k(label_test, gae_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, gae_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "    \n",
    "def make_hgae_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "    label_train = label_train\n",
    "\n",
    "    gae_model = GAE(hid_dim=12, num_layers=12, weight_decay=3,\n",
    "                contamination=0.37, lr=0.001, epoch=5, gpu=-1,\n",
    "                verbose=3, recon_s=True, sigmoid_s=True)\n",
    "    gae_compile = gae_model.fit(pyG_train, label_train)\n",
    "    return gae_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\969071201.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 0.4540 | AUC 0.9597 | Recall 0.8432 | Precision 0.8432 | AP 0.9354 | F1 0.8432 | Time 6.15\n",
      "Epoch 0001: Loss 0.3589 | AUC 0.9597 | Recall 0.8432 | Precision 0.8432 | AP 0.9355 | F1 0.8432 | Time 4.28\n",
      "Epoch 0002: Loss 0.2840 | AUC 0.9598 | Recall 0.8433 | Precision 0.8433 | AP 0.9356 | F1 0.8433 | Time 4.17\n",
      "Epoch 0003: Loss 0.2510 | AUC 0.9603 | Recall 0.8443 | Precision 0.8443 | AP 0.9362 | F1 0.8443 | Time 3.88\n",
      "Epoch 0004: Loss 0.2500 | AUC 0.4657 | Recall 0.2591 | Precision 0.2591 | AP 0.3375 | F1 0.2848 | Time 3.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\969071201.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0000 | AUC 0.4685 | Recall 0.8513 | Precision 0.8513 | AP 0.4407 | F1 0.0735 | Time 0.92\n",
      "F1 score:  tensor(0.8513)\n",
      "Precision:  tensor(0.8513)\n",
      "Recall:  tensor(0.8513)\n"
     ]
    }
   ],
   "source": [
    "gae_model = make_hgae_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_hgae(label_test, gae_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnomalyDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hanomalydae(label_test, anomalydae_compile, test_graph, test_node_features):\n",
    "\n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "    \n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    anomalydae_ip_pred_res, anomalydae_ip_score_res = anomalydae_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear', )\n",
    "    \n",
    "    precision_pygod = eval_precision_at_k(label_test, anomalydae_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, anomalydae_ip_score_res)\n",
    "    f1_score = 2*(precision_pygod*recall_pygod)/(precision_pygod+recall_pygod)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    return  precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_hanomalydae_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "\n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "    \n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    \n",
    "    anomalydae_model = AnomalyDAE(hid_dim=12, emb_dim=4, \n",
    "                        lr=0.001, contamination=0.37,\n",
    "                        epoch=5, gpu=0,\n",
    "                        weight=1, verbose=3)\n",
    "    anomalydae_compile = anomalydae_model.fit(pyG_train)\n",
    "\n",
    "    return anomalydae_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\1419949797.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 11186.1670 |  | Time 0.64\n",
      "Epoch 0001: Loss 114.6346 |  | Time 0.75\n",
      "Epoch 0002: Loss 114.4019 |  | Time 0.78\n",
      "Epoch 0003: Loss 114.1781 |  | Time 0.75\n",
      "Epoch 0004: Loss 113.9597 |  | Time 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\1419949797.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0239 | AUC 0.8121 | Recall 0.7875 | Precision 0.7875 | AP 0.7600 | F1 0.7875 | Time 0.14\n",
      "F1 score:  tensor(0.7875)\n",
      "Precision:  tensor(0.7875)\n",
      "Recall:  tensor(0.7875)\n"
     ]
    }
   ],
   "source": [
    "anomalydae_model = make_hanomalydae_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_hanomalydae(label_test, anomalydae_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hconad(label_test, conda_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "    \n",
    "    conad_ip_pred_res, conad_ip_score_res = conda_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear')\n",
    "    precision = eval_precision_at_k(label_test, conad_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, conad_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def make_hconad_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    conad_model = CONAD(hid_dim=10, num_layers=16, \n",
    "                        lr=0.001, weight_decay= 1, contamination=0.37,\n",
    "                        epoch=5, gpu=-1,  \n",
    "                        weight=1, dropout=0.2, verbose=3)\n",
    "    conad_compile = conad_model.fit(pyG_train)\n",
    "\n",
    "    return conad_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\1762596898.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 37.6440 |  | Time 4.33\n",
      "Epoch 0001: Loss 37.6382 |  | Time 4.45\n",
      "Epoch 0002: Loss 37.6337 |  | Time 4.22\n",
      "Epoch 0003: Loss 37.6333 |  | Time 4.57\n",
      "Epoch 0004: Loss 37.6260 |  | Time 4.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14424\\1762596898.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0098 | AUC 0.8027 | Recall 0.6913 | Precision 0.6913 | AP 0.7539 | F1 0.6913 | Time 0.57\n",
      "F1 score:  tensor(0.6913)\n",
      "Precision:  tensor(0.6913)\n",
      "Recall:  tensor(0.6913)\n"
     ]
    }
   ],
   "source": [
    "conad_model = make_hconad_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_hconad(label_test, conad_model, test_graph, test_node_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from pygod.detector import DOMINANT, OCGNN, CONAD, GAE, AnomalyDAE\n",
    "from pygod.metric import eval_average_precision, eval_roc_auc, eval_f1, eval_precision_at_k, eval_recall_at_k\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Graph Skenario  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = pickle.load(open('model_graph/train_graph.pkl', 'rb'))\n",
    "label_train = pickle.load(open('model_graph/label_train.pkl', 'rb'))\n",
    "train_node_features = pickle.load(open('model_graph/train_node_features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph = pickle.load(open('model_graph/test_graph.pkl', 'rb'))\n",
    "label_test = pickle.load(open('model_graph/label_test.pkl', 'rb'))\n",
    "test_node_features = pickle.load(open('model_graph/test_node_features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your full graph stored in 'train_graph'\n",
    "# Initialize an empty subgraph\n",
    "sampled_graph = nx.Graph()\n",
    "\n",
    "# Get the nodes with edges until we have 20 nodes\n",
    "desired_num_nodes = 20\n",
    "for node in train_graph.nodes():\n",
    "    # Add the node and its edges to the sampled graph\n",
    "    if len(train_graph.edges(node)) > 0:\n",
    "        sampled_graph.add_node(node)\n",
    "        sampled_graph.add_edges_from(train_graph.edges(node))\n",
    "        \n",
    "    # Break the loop if we have reached the desired number of nodes\n",
    "    if len(sampled_graph.nodes()) >= desired_num_nodes:\n",
    "        break\n",
    "\n",
    "# Visualize the sampled graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "pos = nx.spring_layout(sampled_graph, seed=42)\n",
    "\n",
    "# Draw nodes and edges\n",
    "nx.draw(sampled_graph, pos, with_labels=False, node_color='red', node_size=200, font_size=8, font_weight='bold', alpha=0.8)\n",
    "nx.draw_networkx_edges(sampled_graph, pos, edge_color='black', alpha=0.5)\n",
    "\n",
    "plt.title(\"Sampled Graph with First 20 Nodes (with Edges) from Full NetworkX Graph\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15451\n",
      "9525\n"
     ]
    }
   ],
   "source": [
    "## number of nodes\n",
    "print(train_graph.number_of_nodes())\n",
    "print(test_graph.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(train_graph))))\n",
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(test_graph))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor(0): tensor(10070), tensor(1): tensor(5381)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert label_train to numpy array\n",
    "label_train_np = np.array(label_train)\n",
    "label_train_torch = torch.from_numpy(label_train_np)\n",
    "unique, counts = torch.unique(label_train_torch, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Graph Skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_2 = pickle.load(open('model_graph/train_graph_port.pkl', 'rb'))\n",
    "label_train_2 = pickle.load(open('model_graph/label_train_port.pkl', 'rb'))\n",
    "train_node_features_2 = pickle.load(open('model_graph/train_node_features_ip_port.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph_2 = pickle.load(open('model_graph/test_graph_port.pkl', 'rb'))\n",
    "label_test_2 = pickle.load(open('model_graph/label_test_port.pkl', 'rb'))\n",
    "test_node_features_2 = pickle.load(open('model_graph/test_node_features_port.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11213\n",
      "6999\n"
     ]
    }
   ],
   "source": [
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(train_graph_2))))\n",
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(test_graph_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "## number of nodes\n",
    "print(train_graph_2.number_of_nodes())\n",
    "print(test_graph_2.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "print(len(label_train_2))\n",
    "print(len(label_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Graph Skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_3 = pickle.load(open('model_graph/train_graph_ip_port.pkl', 'rb'))\n",
    "label_train_3 = pickle.load(open('model_graph/label_train_ip_port.pkl', 'rb'))\n",
    "train_node_features_3 = pickle.load(open('model_graph/train_node_features_ip_port.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph_3 = pickle.load(open('model_graph/test_graph_ip_port.pkl', 'rb'))\n",
    "label_test_3 = pickle.load(open('model_graph/label_test_ip_port.pkl', 'rb'))\n",
    "test_node_features_3 = pickle.load(open('model_graph/test_node_features_ip_port.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13629\n",
      "7832\n"
     ]
    }
   ],
   "source": [
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(train_graph_3))))\n",
    "# print num of isolated nodes\n",
    "print(len(list(nx.isolates(test_graph_3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "## number of nodes\n",
    "print(train_graph_3.number_of_nodes())\n",
    "print(test_graph_3.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "print(len(train_node_features_3))\n",
    "print(len(test_node_features_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "print(len(label_train_3))\n",
    "print(len(label_test_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with default parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOMINANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dominant(label_test, dominant_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "    print(pyG_test.x.shape)\n",
    "    dominant_ip_pred_res, dominant_ip_score_res = dominant_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True)\n",
    "    \n",
    "    precision_pygod = eval_precision_at_k(label_test, dominant_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, dominant_ip_score_res)\n",
    "    f1_score = 2 * (precision_pygod * recall_pygod) / (precision_pygod + recall_pygod)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    return precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_dominant_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    print(pyG_train.x.shape)\n",
    "    dominant_model = DOMINANT(epoch=100, verbose=3, gpu=0)\n",
    "    dominant_compile = dominant_model.fit(pyG_train)\n",
    "    return dominant_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3770037017.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3770037017.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_train = torch.tensor(label_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15451, 1])\n",
      "Epoch 0000: Loss 1251037.0000 |  | Time 8.37\n",
      "Epoch 0001: Loss 479677.5000 |  | Time 0.78\n",
      "Epoch 0002: Loss 180111.0625 |  | Time 0.63\n",
      "Epoch 0003: Loss 114140.8672 |  | Time 0.60\n",
      "Epoch 0004: Loss 115923.6719 |  | Time 0.63\n",
      "Epoch 0005: Loss 117729.3906 |  | Time 0.64\n",
      "Epoch 0006: Loss 108468.5703 |  | Time 0.71\n",
      "Epoch 0007: Loss 94579.0234 |  | Time 0.67\n",
      "Epoch 0008: Loss 81210.1719 |  | Time 0.67\n",
      "Epoch 0009: Loss 69354.3203 |  | Time 0.66\n",
      "Epoch 0010: Loss 58392.9844 |  | Time 0.61\n",
      "Epoch 0011: Loss 47929.6328 |  | Time 0.66\n",
      "Epoch 0012: Loss 38240.6328 |  | Time 0.65\n",
      "Epoch 0013: Loss 30103.3320 |  | Time 0.72\n",
      "Epoch 0014: Loss 24169.4395 |  | Time 0.70\n",
      "Epoch 0015: Loss 20446.4707 |  | Time 0.69\n",
      "Epoch 0016: Loss 18301.3789 |  | Time 0.67\n",
      "Epoch 0017: Loss 16929.3477 |  | Time 0.65\n",
      "Epoch 0018: Loss 15790.0986 |  | Time 0.67\n",
      "Epoch 0019: Loss 14681.0840 |  | Time 0.70\n",
      "Epoch 0020: Loss 13584.8516 |  | Time 0.67\n",
      "Epoch 0021: Loss 12515.0049 |  | Time 0.65\n",
      "Epoch 0022: Loss 11453.1934 |  | Time 0.88\n",
      "Epoch 0023: Loss 10355.3105 |  | Time 0.81\n",
      "Epoch 0024: Loss 9182.7988 |  | Time 0.71\n",
      "Epoch 0025: Loss 7936.4717 |  | Time 0.76\n",
      "Epoch 0026: Loss 6676.9365 |  | Time 0.76\n",
      "Epoch 0027: Loss 5513.4634 |  | Time 0.71\n",
      "Epoch 0028: Loss 4559.2891 |  | Time 0.71\n",
      "Epoch 0029: Loss 3878.6145 |  | Time 0.63\n",
      "Epoch 0030: Loss 3460.3313 |  | Time 0.81\n",
      "Epoch 0031: Loss 3232.0273 |  | Time 0.89\n",
      "Epoch 0032: Loss 3096.4709 |  | Time 1.00\n",
      "Epoch 0033: Loss 2963.7651 |  | Time 0.83\n",
      "Epoch 0034: Loss 2769.6106 |  | Time 0.86\n",
      "Epoch 0035: Loss 2486.8359 |  | Time 0.97\n",
      "Epoch 0036: Loss 2133.2529 |  | Time 1.15\n",
      "Epoch 0037: Loss 1765.0864 |  | Time 0.65\n",
      "Epoch 0038: Loss 1448.1559 |  | Time 0.75\n",
      "Epoch 0039: Loss 1221.2581 |  | Time 0.70\n",
      "Epoch 0040: Loss 1079.6407 |  | Time 0.64\n",
      "Epoch 0041: Loss 990.6179 |  | Time 0.65\n",
      "Epoch 0042: Loss 923.7640 |  | Time 0.70\n",
      "Epoch 0043: Loss 867.5059 |  | Time 0.68\n",
      "Epoch 0044: Loss 822.6967 |  | Time 0.65\n",
      "Epoch 0045: Loss 787.7167 |  | Time 0.67\n",
      "Epoch 0046: Loss 752.9802 |  | Time 0.63\n",
      "Epoch 0047: Loss 707.4178 |  | Time 0.72\n",
      "Epoch 0048: Loss 646.8120 |  | Time 0.71\n",
      "Epoch 0049: Loss 575.8005 |  | Time 0.71\n",
      "Epoch 0050: Loss 504.0016 |  | Time 0.63\n",
      "Epoch 0051: Loss 440.4776 |  | Time 0.75\n",
      "Epoch 0052: Loss 389.4038 |  | Time 0.62\n",
      "Epoch 0053: Loss 348.7091 |  | Time 0.85\n",
      "Epoch 0054: Loss 312.8511 |  | Time 0.61\n",
      "Epoch 0055: Loss 278.0056 |  | Time 0.70\n",
      "Epoch 0056: Loss 244.9243 |  | Time 0.74\n",
      "Epoch 0057: Loss 216.7711 |  | Time 0.67\n",
      "Epoch 0058: Loss 194.8561 |  | Time 0.64\n",
      "Epoch 0059: Loss 177.4464 |  | Time 0.65\n",
      "Epoch 0060: Loss 162.3150 |  | Time 0.73\n",
      "Epoch 0061: Loss 148.9348 |  | Time 0.75\n",
      "Epoch 0062: Loss 137.6223 |  | Time 0.81\n",
      "Epoch 0063: Loss 127.6461 |  | Time 0.74\n",
      "Epoch 0064: Loss 117.4292 |  | Time 0.65\n",
      "Epoch 0065: Loss 106.4165 |  | Time 0.68\n",
      "Epoch 0066: Loss 95.8770 |  | Time 0.64\n",
      "Epoch 0067: Loss 87.7569 |  | Time 0.66\n",
      "Epoch 0068: Loss 83.2017 |  | Time 0.61\n",
      "Epoch 0069: Loss 82.0574 |  | Time 0.62\n",
      "Epoch 0070: Loss 83.0302 |  | Time 0.62\n",
      "Epoch 0071: Loss 84.0475 |  | Time 0.62\n",
      "Epoch 0072: Loss 83.0713 |  | Time 0.63\n",
      "Epoch 0073: Loss 79.2286 |  | Time 0.63\n",
      "Epoch 0074: Loss 73.2648 |  | Time 0.61\n",
      "Epoch 0075: Loss 66.8670 |  | Time 0.61\n",
      "Epoch 0076: Loss 61.5498 |  | Time 0.61\n",
      "Epoch 0077: Loss 58.0496 |  | Time 0.59\n",
      "Epoch 0078: Loss 56.2411 |  | Time 0.62\n",
      "Epoch 0079: Loss 55.3021 |  | Time 0.65\n",
      "Epoch 0080: Loss 54.1517 |  | Time 0.69\n",
      "Epoch 0081: Loss 52.1384 |  | Time 0.69\n",
      "Epoch 0082: Loss 49.4116 |  | Time 0.66\n",
      "Epoch 0083: Loss 46.5667 |  | Time 0.62\n",
      "Epoch 0084: Loss 44.0307 |  | Time 0.78\n",
      "Epoch 0085: Loss 41.8995 |  | Time 0.82\n",
      "Epoch 0086: Loss 40.1769 |  | Time 0.69\n",
      "Epoch 0087: Loss 38.8928 |  | Time 0.61\n",
      "Epoch 0088: Loss 38.0123 |  | Time 0.66\n",
      "Epoch 0089: Loss 37.3915 |  | Time 0.65\n",
      "Epoch 0090: Loss 36.8776 |  | Time 0.62\n",
      "Epoch 0091: Loss 36.3915 |  | Time 0.60\n",
      "Epoch 0092: Loss 35.9217 |  | Time 0.61\n",
      "Epoch 0093: Loss 35.4861 |  | Time 0.64\n",
      "Epoch 0094: Loss 35.1130 |  | Time 0.61\n",
      "Epoch 0095: Loss 34.8242 |  | Time 0.60\n",
      "Epoch 0096: Loss 34.6025 |  | Time 0.60\n",
      "Epoch 0097: Loss 34.4009 |  | Time 0.59\n",
      "Epoch 0098: Loss 34.1745 |  | Time 0.61\n",
      "Epoch 0099: Loss 33.9244 |  | Time 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3770037017.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9525, 1])\n",
      "Test: Loss 0.0045 | AUC 0.7327 | Recall 0.6297 | Precision 0.6297 | AP 0.7347 | F1 0.6297 | Time 0.15\n",
      "F1 score:  tensor(0.6297)\n",
      "Precision:  tensor(0.6297)\n",
      "Recall:  tensor(0.6297)\n"
     ]
    }
   ],
   "source": [
    "dominant_model = make_dominant_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_dominant(label_test, dominant_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(dominant_model, open('ml-model/dominant_model_default_1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3770037017.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9525, 1])\n",
      "Test: Loss 0.0045 | AUC 0.7327 | Recall 0.6297 | Precision 0.6297 | AP 0.7347 | F1 0.6297 | Time 0.30\n",
      "F1 score:  tensor(0.6297)\n",
      "Precision:  tensor(0.6297)\n",
      "Recall:  tensor(0.6297)\n"
     ]
    }
   ],
   "source": [
    "dominant_model_default = pickle.load(open('ml-model/dominant_model_default_1.pickle', 'rb'))\n",
    "precision_score, recall_score, f1_score = predict_dominant(label_test, dominant_model_default, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15480, 1])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 15473 is out of bounds for dimension 0 with size 15473",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dominant_model \u001b[39m=\u001b[39m make_dominant_model(train_graph_2, train_node_features_2, label_train_2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_dominant(label_test_2, dominant_model, test_graph_2, test_node_features_2)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 30\u001b[0m in \u001b[0;36mmake_dominant_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(pyG_train\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m dominant_model \u001b[39m=\u001b[39m DOMINANT(epoch\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m dominant_compile \u001b[39m=\u001b[39m dominant_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X41sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dominant_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\dominant.py:164\u001b[0m, in \u001b[0;36mDOMINANT.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    159\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m x_, s_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x, edge_index)\n\u001b[0;32m    162\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    163\u001b[0m                              x_[:batch_size],\n\u001b[1;32m--> 164\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    165\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    166\u001b[0m                              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n\u001b[0;32m    168\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n\u001b[0;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m loss, score\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15473 is out of bounds for dimension 0 with size 15473"
     ]
    }
   ],
   "source": [
    "dominant_model = make_dominant_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_dominant(label_test_2, dominant_model, test_graph_2, test_node_features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15480, 1])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 15473 is out of bounds for dimension 0 with size 15473",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dominant_model \u001b[39m=\u001b[39m make_dominant_model(train_graph_3, train_node_features_3, label_train_3)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_dominant(label_test_3, dominant_model, test_graph_3, test_node_features_3)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 32\u001b[0m in \u001b[0;36mmake_dominant_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(pyG_train\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m dominant_model \u001b[39m=\u001b[39m DOMINANT(epoch\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m dominant_compile \u001b[39m=\u001b[39m dominant_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#X43sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dominant_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\dominant.py:164\u001b[0m, in \u001b[0;36mDOMINANT.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    159\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m x_, s_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x, edge_index)\n\u001b[0;32m    162\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    163\u001b[0m                              x_[:batch_size],\n\u001b[1;32m--> 164\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    165\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    166\u001b[0m                              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n\u001b[0;32m    168\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n\u001b[0;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m loss, score\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15473 is out of bounds for dimension 0 with size 15473"
     ]
    }
   ],
   "source": [
    "dominant_model = make_dominant_model(train_graph_3, train_node_features_3, label_train_3)\n",
    "precision_score, recall_score, f1_score = predict_dominant(label_test_3, dominant_model, test_graph_3, test_node_features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ocgnn(label_test, ocgnn_compile, pyG_test, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(pyG_test)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    ocgnn_ip_pred_res, ocgnn_ip_score_res = ocgnn_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "\n",
    "    precision_pygod = eval_precision_at_k(label_test, ocgnn_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, ocgnn_ip_score_res)\n",
    "    f1_score = 2 * (precision_pygod * recall_pygod) / (precision_pygod + recall_pygod)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "\n",
    "    return precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_ocgnn_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    ocgnn_model = OCGNN(epoch=100, verbose=3)\n",
    "    ocgnn_compile = ocgnn_model.fit(pyG_train)\n",
    "    return ocgnn_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\2431339799.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 84627.2891 |  | Time 0.07\n",
      "Epoch 0001: Loss 48232.7891 |  | Time 0.08\n",
      "Epoch 0002: Loss 32310.5918 |  | Time 0.07\n",
      "Epoch 0003: Loss 21133.0664 |  | Time 0.08\n",
      "Epoch 0004: Loss 13929.8438 |  | Time 0.09\n",
      "Epoch 0005: Loss 9903.6582 |  | Time 0.08\n",
      "Epoch 0006: Loss 8189.5498 |  | Time 0.08\n",
      "Epoch 0007: Loss 8261.3701 |  | Time 0.08\n",
      "Epoch 0008: Loss 9035.1221 |  | Time 0.08\n",
      "Epoch 0009: Loss 9858.8828 |  | Time 0.07\n",
      "Epoch 0010: Loss 10427.3545 |  | Time 0.07\n",
      "Epoch 0011: Loss 10648.3857 |  | Time 0.08\n",
      "Epoch 0012: Loss 10556.7305 |  | Time 0.08\n",
      "Epoch 0013: Loss 10227.5811 |  | Time 0.08\n",
      "Epoch 0014: Loss 9733.3584 |  | Time 0.08\n",
      "Epoch 0015: Loss 9137.4248 |  | Time 0.09\n",
      "Epoch 0016: Loss 8500.4785 |  | Time 0.08\n",
      "Epoch 0017: Loss 7879.9004 |  | Time 0.09\n",
      "Epoch 0018: Loss 7320.9775 |  | Time 0.08\n",
      "Epoch 0019: Loss 6849.3452 |  | Time 0.08\n",
      "Epoch 0020: Loss 6470.0444 |  | Time 0.08\n",
      "Epoch 0021: Loss 6214.6934 |  | Time 0.08\n",
      "Epoch 0022: Loss 6071.5713 |  | Time 0.09\n",
      "Epoch 0023: Loss 5991.9600 |  | Time 0.09\n",
      "Epoch 0024: Loss 5939.7744 |  | Time 0.08\n",
      "Epoch 0025: Loss 5884.9092 |  | Time 0.08\n",
      "Epoch 0026: Loss 5819.6289 |  | Time 0.08\n",
      "Epoch 0027: Loss 5744.4321 |  | Time 0.09\n",
      "Epoch 0028: Loss 5661.1914 |  | Time 0.08\n",
      "Epoch 0029: Loss 5574.3057 |  | Time 0.10\n",
      "Epoch 0030: Loss 5488.9844 |  | Time 0.11\n",
      "Epoch 0031: Loss 5409.4346 |  | Time 0.10\n",
      "Epoch 0032: Loss 5339.3228 |  | Time 0.08\n",
      "Epoch 0033: Loss 5280.3955 |  | Time 0.09\n",
      "Epoch 0034: Loss 5231.4111 |  | Time 0.11\n",
      "Epoch 0035: Loss 5189.6582 |  | Time 0.11\n",
      "Epoch 0036: Loss 5152.5952 |  | Time 0.09\n",
      "Epoch 0037: Loss 5119.1396 |  | Time 0.09\n",
      "Epoch 0038: Loss 5088.9258 |  | Time 0.08\n",
      "Epoch 0039: Loss 5061.4565 |  | Time 0.08\n",
      "Epoch 0040: Loss 5035.1914 |  | Time 0.09\n",
      "Epoch 0041: Loss 5009.8887 |  | Time 0.09\n",
      "Epoch 0042: Loss 4985.0205 |  | Time 0.08\n",
      "Epoch 0043: Loss 4960.4951 |  | Time 0.09\n",
      "Epoch 0044: Loss 4936.4668 |  | Time 0.10\n",
      "Epoch 0045: Loss 4914.4502 |  | Time 0.10\n",
      "Epoch 0046: Loss 4894.0146 |  | Time 0.10\n",
      "Epoch 0047: Loss 4874.5938 |  | Time 0.10\n",
      "Epoch 0048: Loss 4856.4287 |  | Time 0.11\n",
      "Epoch 0049: Loss 4839.8452 |  | Time 0.10\n",
      "Epoch 0050: Loss 4824.9106 |  | Time 0.09\n",
      "Epoch 0051: Loss 4810.8599 |  | Time 0.09\n",
      "Epoch 0052: Loss 4796.8638 |  | Time 0.08\n",
      "Epoch 0053: Loss 4782.6821 |  | Time 0.09\n",
      "Epoch 0054: Loss 4768.3535 |  | Time 0.09\n",
      "Epoch 0055: Loss 4754.1401 |  | Time 0.10\n",
      "Epoch 0056: Loss 4740.3516 |  | Time 0.11\n",
      "Epoch 0057: Loss 4727.2827 |  | Time 0.10\n",
      "Epoch 0058: Loss 4715.1636 |  | Time 0.11\n",
      "Epoch 0059: Loss 4704.4111 |  | Time 0.09\n",
      "Epoch 0060: Loss 4695.1934 |  | Time 0.14\n",
      "Epoch 0061: Loss 4687.3047 |  | Time 0.09\n",
      "Epoch 0062: Loss 4680.3174 |  | Time 0.08\n",
      "Epoch 0063: Loss 4673.7905 |  | Time 0.10\n",
      "Epoch 0064: Loss 4667.2285 |  | Time 0.08\n",
      "Epoch 0065: Loss 4660.3184 |  | Time 0.09\n",
      "Epoch 0066: Loss 4652.8896 |  | Time 0.09\n",
      "Epoch 0067: Loss 4644.9429 |  | Time 0.08\n",
      "Epoch 0068: Loss 4636.5898 |  | Time 0.07\n",
      "Epoch 0069: Loss 4628.0308 |  | Time 0.09\n",
      "Epoch 0070: Loss 4619.5010 |  | Time 0.08\n",
      "Epoch 0071: Loss 4611.1680 |  | Time 0.09\n",
      "Epoch 0072: Loss 4603.1611 |  | Time 0.10\n",
      "Epoch 0073: Loss 4595.5186 |  | Time 0.11\n",
      "Epoch 0074: Loss 4588.1938 |  | Time 0.09\n",
      "Epoch 0075: Loss 4581.0908 |  | Time 0.09\n",
      "Epoch 0076: Loss 4574.0957 |  | Time 0.08\n",
      "Epoch 0077: Loss 4567.1230 |  | Time 0.09\n",
      "Epoch 0078: Loss 4560.1533 |  | Time 0.08\n",
      "Epoch 0079: Loss 4553.2046 |  | Time 0.08\n",
      "Epoch 0080: Loss 4546.2954 |  | Time 0.08\n",
      "Epoch 0081: Loss 4539.4253 |  | Time 0.08\n",
      "Epoch 0082: Loss 4532.5869 |  | Time 0.08\n",
      "Epoch 0083: Loss 4525.7622 |  | Time 0.08\n",
      "Epoch 0084: Loss 4518.9443 |  | Time 0.08\n",
      "Epoch 0085: Loss 4512.1470 |  | Time 0.08\n",
      "Epoch 0086: Loss 4505.4028 |  | Time 0.08\n",
      "Epoch 0087: Loss 4498.7021 |  | Time 0.08\n",
      "Epoch 0088: Loss 4492.0527 |  | Time 0.09\n",
      "Epoch 0089: Loss 4485.4341 |  | Time 0.08\n",
      "Epoch 0090: Loss 4478.8350 |  | Time 0.08\n",
      "Epoch 0091: Loss 4472.2441 |  | Time 0.08\n",
      "Epoch 0092: Loss 4465.6567 |  | Time 0.09\n",
      "Epoch 0093: Loss 4459.0688 |  | Time 0.08\n",
      "Epoch 0094: Loss 4452.4766 |  | Time 0.09\n",
      "Epoch 0095: Loss 4445.8838 |  | Time 0.08\n",
      "Epoch 0096: Loss 4439.2935 |  | Time 0.08\n",
      "Epoch 0097: Loss 4432.7090 |  | Time 0.08\n",
      "Epoch 0098: Loss 4426.1270 |  | Time 0.08\n",
      "Epoch 0099: Loss 4419.5449 |  | Time 0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\2431339799.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.4651 | AUC 0.1470 | Recall 0.1036 | Precision 0.1036 | AP 0.3154 | F1 0.1036 | Time 0.03\n",
      "F1 score:  tensor(0.1036)\n",
      "Precision:  tensor(0.1036)\n",
      "Recall:  tensor(0.1036)\n"
     ]
    }
   ],
   "source": [
    "ocgnn_model = make_ocgnn_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_ocgnn(label_test, ocgnn_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(ocgnn_model, open('ml-model/ocgnn_model_default_1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocgnn_model_default = pickle.load(open('ml-model/ocgnn_model_default_1.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\2431339799.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.4651 | AUC 0.1470 | Recall 0.1036 | Precision 0.1036 | AP 0.3154 | F1 0.1036 | Time 0.02\n",
      "F1 score:  tensor(0.1036)\n",
      "Precision:  tensor(0.1036)\n",
      "Recall:  tensor(0.1036)\n"
     ]
    }
   ],
   "source": [
    "ocgnn_precision, ocgnn_recall, ocgnn_f1 = predict_ocgnn(label_test, ocgnn_model_default, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 305987.4375 |  | Time 0.05\n",
      "Epoch 0001: Loss 227769.3438 |  | Time 0.06\n",
      "Epoch 0002: Loss 177402.9531 |  | Time 0.05\n",
      "Epoch 0003: Loss 138141.0156 |  | Time 0.05\n",
      "Epoch 0004: Loss 107332.7891 |  | Time 0.05\n",
      "Epoch 0005: Loss 83414.3516 |  | Time 0.09\n",
      "Epoch 0006: Loss 65243.4141 |  | Time 0.06\n",
      "Epoch 0007: Loss 51573.9883 |  | Time 0.07\n",
      "Epoch 0008: Loss 41482.0547 |  | Time 0.06\n",
      "Epoch 0009: Loss 34151.5859 |  | Time 0.06\n",
      "Epoch 0010: Loss 28940.9961 |  | Time 0.07\n",
      "Epoch 0011: Loss 25246.7578 |  | Time 0.05\n",
      "Epoch 0012: Loss 22592.5078 |  | Time 0.05\n",
      "Epoch 0013: Loss 20666.0078 |  | Time 0.05\n",
      "Epoch 0014: Loss 19221.0703 |  | Time 0.06\n",
      "Epoch 0015: Loss 18078.3516 |  | Time 0.05\n",
      "Epoch 0016: Loss 17126.9902 |  | Time 0.06\n",
      "Epoch 0017: Loss 16304.9697 |  | Time 0.06\n",
      "Epoch 0018: Loss 15573.3018 |  | Time 0.06\n",
      "Epoch 0019: Loss 14903.1270 |  | Time 0.05\n",
      "Epoch 0020: Loss 14277.9668 |  | Time 0.05\n",
      "Epoch 0021: Loss 13697.8721 |  | Time 0.05\n",
      "Epoch 0022: Loss 13169.3184 |  | Time 0.06\n",
      "Epoch 0023: Loss 12702.4209 |  | Time 0.05\n",
      "Epoch 0024: Loss 12306.3428 |  | Time 0.05\n",
      "Epoch 0025: Loss 11974.5723 |  | Time 0.05\n",
      "Epoch 0026: Loss 11694.2334 |  | Time 0.06\n",
      "Epoch 0027: Loss 11450.6055 |  | Time 0.05\n",
      "Epoch 0028: Loss 11229.6699 |  | Time 0.04\n",
      "Epoch 0029: Loss 11019.5508 |  | Time 0.05\n",
      "Epoch 0030: Loss 10811.9824 |  | Time 0.06\n",
      "Epoch 0031: Loss 10603.4434 |  | Time 0.06\n",
      "Epoch 0032: Loss 10394.9238 |  | Time 0.06\n",
      "Epoch 0033: Loss 10189.3770 |  | Time 0.06\n",
      "Epoch 0034: Loss 9989.8223 |  | Time 0.08\n",
      "Epoch 0035: Loss 9798.0293 |  | Time 0.08\n",
      "Epoch 0036: Loss 9614.8359 |  | Time 0.07\n",
      "Epoch 0037: Loss 9441.2539 |  | Time 0.06\n",
      "Epoch 0038: Loss 9279.1973 |  | Time 0.06\n",
      "Epoch 0039: Loss 9131.5527 |  | Time 0.07\n",
      "Epoch 0040: Loss 8996.5996 |  | Time 0.08\n",
      "Epoch 0041: Loss 8873.9375 |  | Time 0.07\n",
      "Epoch 0042: Loss 8761.8926 |  | Time 0.06\n",
      "Epoch 0043: Loss 8660.5938 |  | Time 0.07\n",
      "Epoch 0044: Loss 8570.1133 |  | Time 0.07\n",
      "Epoch 0045: Loss 8491.1133 |  | Time 0.07\n",
      "Epoch 0046: Loss 8423.9375 |  | Time 0.08\n",
      "Epoch 0047: Loss 8368.1445 |  | Time 0.07\n",
      "Epoch 0048: Loss 8322.3027 |  | Time 0.06\n",
      "Epoch 0049: Loss 8284.8359 |  | Time 0.05\n",
      "Epoch 0050: Loss 8252.5859 |  | Time 0.05\n",
      "Epoch 0051: Loss 8223.4092 |  | Time 0.05\n",
      "Epoch 0052: Loss 8195.7207 |  | Time 0.06\n",
      "Epoch 0053: Loss 8168.4844 |  | Time 0.06\n",
      "Epoch 0054: Loss 8141.3105 |  | Time 0.07\n",
      "Epoch 0055: Loss 8114.3750 |  | Time 0.08\n",
      "Epoch 0056: Loss 8087.7134 |  | Time 0.06\n",
      "Epoch 0057: Loss 8061.5889 |  | Time 0.06\n",
      "Epoch 0058: Loss 8035.9849 |  | Time 0.06\n",
      "Epoch 0059: Loss 8011.0547 |  | Time 0.06\n",
      "Epoch 0060: Loss 7986.8779 |  | Time 0.12\n",
      "Epoch 0061: Loss 7963.7876 |  | Time 0.08\n",
      "Epoch 0062: Loss 7942.5615 |  | Time 0.07\n",
      "Epoch 0063: Loss 7922.5508 |  | Time 0.07\n",
      "Epoch 0064: Loss 7903.6704 |  | Time 0.06\n",
      "Epoch 0065: Loss 7885.8438 |  | Time 0.07\n",
      "Epoch 0066: Loss 7868.9922 |  | Time 0.07\n",
      "Epoch 0067: Loss 7853.0205 |  | Time 0.08\n",
      "Epoch 0068: Loss 7837.7837 |  | Time 0.07\n",
      "Epoch 0069: Loss 7823.0928 |  | Time 0.08\n",
      "Epoch 0070: Loss 7808.7578 |  | Time 0.08\n",
      "Epoch 0071: Loss 7794.5898 |  | Time 0.07\n",
      "Epoch 0072: Loss 7780.4385 |  | Time 0.06\n",
      "Epoch 0073: Loss 7766.2578 |  | Time 0.06\n",
      "Epoch 0074: Loss 7751.8896 |  | Time 0.06\n",
      "Epoch 0075: Loss 7737.2959 |  | Time 0.07\n",
      "Epoch 0076: Loss 7722.4702 |  | Time 0.07\n",
      "Epoch 0077: Loss 7707.4683 |  | Time 0.07\n",
      "Epoch 0078: Loss 7692.3604 |  | Time 0.07\n",
      "Epoch 0079: Loss 7677.2607 |  | Time 0.07\n",
      "Epoch 0080: Loss 7662.2314 |  | Time 0.08\n",
      "Epoch 0081: Loss 7647.3643 |  | Time 0.07\n",
      "Epoch 0082: Loss 7632.7183 |  | Time 0.07\n",
      "Epoch 0083: Loss 7618.3521 |  | Time 0.07\n",
      "Epoch 0084: Loss 7604.2832 |  | Time 0.07\n",
      "Epoch 0085: Loss 7590.5146 |  | Time 0.07\n",
      "Epoch 0086: Loss 7577.0308 |  | Time 0.08\n",
      "Epoch 0087: Loss 7563.7974 |  | Time 0.13\n",
      "Epoch 0088: Loss 7550.7759 |  | Time 0.07\n",
      "Epoch 0089: Loss 7537.9146 |  | Time 0.07\n",
      "Epoch 0090: Loss 7525.1543 |  | Time 0.06\n",
      "Epoch 0091: Loss 7512.4448 |  | Time 0.05\n",
      "Epoch 0092: Loss 7499.7461 |  | Time 0.07\n",
      "Epoch 0093: Loss 7487.0278 |  | Time 0.08\n",
      "Epoch 0094: Loss 7474.2520 |  | Time 0.07\n",
      "Epoch 0095: Loss 7461.4072 |  | Time 0.07\n",
      "Epoch 0096: Loss 7448.4961 |  | Time 0.07\n",
      "Epoch 0097: Loss 7435.5303 |  | Time 0.07\n",
      "Epoch 0098: Loss 7422.5264 |  | Time 0.08\n",
      "Epoch 0099: Loss 7409.5020 |  | Time 0.07\n",
      "Test: Loss 0.8608 | AUC 0.2344 | Recall 0.3335 | Precision 0.3335 | AP 0.3665 | F1 0.3539 | Time 0.02\n",
      "F1 score:  tensor(0.3335)\n",
      "Precision:  tensor(0.3335)\n",
      "Recall:  tensor(0.3335)\n"
     ]
    }
   ],
   "source": [
    "ocgnn_model = make_ocgnn_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_ocgnn(label_test_2, ocgnn_model, test_graph_2, test_node_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(ocgnn_model, open('ml-model/ocgnn_model_default_2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocgnn_model_default_2 = pickle.load(open('ml-model/ocgnn_model_default_2.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\2431339799.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.7272 | AUC 0.1208 | Recall 0.1022 | Precision 0.1022 | AP 0.2874 | F1 0.1022 | Time 0.02\n",
      "F1 score:  tensor(0.1022)\n",
      "Precision:  tensor(0.1022)\n",
      "Recall:  tensor(0.1022)\n"
     ]
    }
   ],
   "source": [
    "ocgnn_precision, ocgnn_recall, ocgnn_f1 = predict_ocgnn(label_test, ocgnn_model_default_2, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 144234.2500 |  | Time 0.05\n",
      "Epoch 0001: Loss 101070.1719 |  | Time 0.04\n",
      "Epoch 0002: Loss 73365.2031 |  | Time 0.04\n",
      "Epoch 0003: Loss 53031.4844 |  | Time 0.05\n",
      "Epoch 0004: Loss 38156.2695 |  | Time 0.05\n",
      "Epoch 0005: Loss 27504.9160 |  | Time 0.05\n",
      "Epoch 0006: Loss 20110.7754 |  | Time 0.07\n",
      "Epoch 0007: Loss 15119.7480 |  | Time 0.06\n",
      "Epoch 0008: Loss 11996.8633 |  | Time 0.07\n",
      "Epoch 0009: Loss 10192.8271 |  | Time 0.09\n",
      "Epoch 0010: Loss 9277.3184 |  | Time 0.06\n",
      "Epoch 0011: Loss 8926.6875 |  | Time 0.07\n",
      "Epoch 0012: Loss 8879.2900 |  | Time 0.05\n",
      "Epoch 0013: Loss 8943.8096 |  | Time 0.06\n",
      "Epoch 0014: Loss 8996.3516 |  | Time 0.04\n",
      "Epoch 0015: Loss 8971.2051 |  | Time 0.05\n",
      "Epoch 0016: Loss 8845.5469 |  | Time 0.05\n",
      "Epoch 0017: Loss 8622.3555 |  | Time 0.05\n",
      "Epoch 0018: Loss 8315.9355 |  | Time 0.05\n",
      "Epoch 0019: Loss 7945.6392 |  | Time 0.05\n",
      "Epoch 0020: Loss 7535.1831 |  | Time 0.06\n",
      "Epoch 0021: Loss 7110.9917 |  | Time 0.05\n",
      "Epoch 0022: Loss 6696.8374 |  | Time 0.06\n",
      "Epoch 0023: Loss 6307.9629 |  | Time 0.05\n",
      "Epoch 0024: Loss 5949.6538 |  | Time 0.06\n",
      "Epoch 0025: Loss 5621.0493 |  | Time 0.10\n",
      "Epoch 0026: Loss 5320.3545 |  | Time 0.07\n",
      "Epoch 0027: Loss 5047.4565 |  | Time 0.05\n",
      "Epoch 0028: Loss 4803.4312 |  | Time 0.06\n",
      "Epoch 0029: Loss 4588.9253 |  | Time 0.05\n",
      "Epoch 0030: Loss 4403.3569 |  | Time 0.06\n",
      "Epoch 0031: Loss 4245.1719 |  | Time 0.04\n",
      "Epoch 0032: Loss 4112.4268 |  | Time 0.05\n",
      "Epoch 0033: Loss 4003.1992 |  | Time 0.06\n",
      "Epoch 0034: Loss 3916.6157 |  | Time 0.06\n",
      "Epoch 0035: Loss 3848.4751 |  | Time 0.05\n",
      "Epoch 0036: Loss 3795.2012 |  | Time 0.05\n",
      "Epoch 0037: Loss 3750.7275 |  | Time 0.05\n",
      "Epoch 0038: Loss 3709.0205 |  | Time 0.05\n",
      "Epoch 0039: Loss 3666.5344 |  | Time 0.06\n",
      "Epoch 0040: Loss 3621.2131 |  | Time 0.05\n",
      "Epoch 0041: Loss 3572.1753 |  | Time 0.06\n",
      "Epoch 0042: Loss 3519.3589 |  | Time 0.06\n",
      "Epoch 0043: Loss 3463.7651 |  | Time 0.05\n",
      "Epoch 0044: Loss 3407.1719 |  | Time 0.05\n",
      "Epoch 0045: Loss 3351.7693 |  | Time 0.05\n",
      "Epoch 0046: Loss 3300.2080 |  | Time 0.05\n",
      "Epoch 0047: Loss 3254.7952 |  | Time 0.06\n",
      "Epoch 0048: Loss 3217.3413 |  | Time 0.05\n",
      "Epoch 0049: Loss 3188.7642 |  | Time 0.06\n",
      "Epoch 0050: Loss 3168.0791 |  | Time 0.06\n",
      "Epoch 0051: Loss 3154.1040 |  | Time 0.06\n",
      "Epoch 0052: Loss 3145.0857 |  | Time 0.06\n",
      "Epoch 0053: Loss 3139.0078 |  | Time 0.05\n",
      "Epoch 0054: Loss 3134.0391 |  | Time 0.05\n",
      "Epoch 0055: Loss 3128.7312 |  | Time 0.05\n",
      "Epoch 0056: Loss 3122.1260 |  | Time 0.06\n",
      "Epoch 0057: Loss 3113.7471 |  | Time 0.05\n",
      "Epoch 0058: Loss 3103.5171 |  | Time 0.04\n",
      "Epoch 0059: Loss 3091.6260 |  | Time 0.05\n",
      "Epoch 0060: Loss 3078.4004 |  | Time 0.05\n",
      "Epoch 0061: Loss 3064.5928 |  | Time 0.05\n",
      "Epoch 0062: Loss 3050.4570 |  | Time 0.06\n",
      "Epoch 0063: Loss 3036.3770 |  | Time 0.05\n",
      "Epoch 0064: Loss 3022.7039 |  | Time 0.06\n",
      "Epoch 0065: Loss 3009.7261 |  | Time 0.07\n",
      "Epoch 0066: Loss 2997.6348 |  | Time 0.07\n",
      "Epoch 0067: Loss 2986.5171 |  | Time 0.06\n",
      "Epoch 0068: Loss 2976.3340 |  | Time 0.05\n",
      "Epoch 0069: Loss 2967.0356 |  | Time 0.05\n",
      "Epoch 0070: Loss 2958.5173 |  | Time 0.05\n",
      "Epoch 0071: Loss 2950.6448 |  | Time 0.05\n",
      "Epoch 0072: Loss 2943.2690 |  | Time 0.05\n",
      "Epoch 0073: Loss 2936.1963 |  | Time 0.05\n",
      "Epoch 0074: Loss 2929.2795 |  | Time 0.05\n",
      "Epoch 0075: Loss 2922.3716 |  | Time 0.05\n",
      "Epoch 0076: Loss 2915.3555 |  | Time 0.05\n",
      "Epoch 0077: Loss 2908.2222 |  | Time 0.06\n",
      "Epoch 0078: Loss 2900.9895 |  | Time 0.06\n",
      "Epoch 0079: Loss 2893.7168 |  | Time 0.05\n",
      "Epoch 0080: Loss 2886.4487 |  | Time 0.05\n",
      "Epoch 0081: Loss 2879.2502 |  | Time 0.05\n",
      "Epoch 0082: Loss 2872.1909 |  | Time 0.07\n",
      "Epoch 0083: Loss 2865.3169 |  | Time 0.07\n",
      "Epoch 0084: Loss 2858.6267 |  | Time 0.06\n",
      "Epoch 0085: Loss 2852.0654 |  | Time 0.07\n",
      "Epoch 0086: Loss 2845.5876 |  | Time 0.07\n",
      "Epoch 0087: Loss 2839.1511 |  | Time 0.06\n",
      "Epoch 0088: Loss 2832.7305 |  | Time 0.06\n",
      "Epoch 0089: Loss 2826.3174 |  | Time 0.06\n",
      "Epoch 0090: Loss 2819.9062 |  | Time 0.06\n",
      "Epoch 0091: Loss 2813.4949 |  | Time 0.07\n",
      "Epoch 0092: Loss 2807.0791 |  | Time 0.07\n",
      "Epoch 0093: Loss 2800.6570 |  | Time 0.07\n",
      "Epoch 0094: Loss 2794.2314 |  | Time 0.07\n",
      "Epoch 0095: Loss 2787.8081 |  | Time 0.06\n",
      "Epoch 0096: Loss 2781.3965 |  | Time 0.06\n",
      "Epoch 0097: Loss 2775.0039 |  | Time 0.06\n",
      "Epoch 0098: Loss 2768.6353 |  | Time 0.05\n",
      "Epoch 0099: Loss 2762.3154 |  | Time 0.05\n",
      "Test: Loss 0.3190 | AUC 0.2203 | Recall 0.3328 | Precision 0.3328 | AP 0.3619 | F1 0.3539 | Time 0.02\n",
      "F1 score:  tensor(0.3328)\n",
      "Precision:  tensor(0.3328)\n",
      "Recall:  tensor(0.3328)\n"
     ]
    }
   ],
   "source": [
    "ocgnn_model = make_ocgnn_model(train_graph_3, train_node_features_3, label_train_3)\n",
    "precision_score, recall_score, f1_score = predict_ocgnn(label_test_3, ocgnn_model, test_graph_3, test_node_features_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(ocgnn_model, open('ml-model/ocgnn_model_default_3.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocgnn_model_default_3 = pickle.load(open('ml-model/ocgnn_model_default_3.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\2431339799.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.2758 | AUC 0.1216 | Recall 0.1024 | Precision 0.1024 | AP 0.2876 | F1 0.1018 | Time 0.03\n",
      "F1 score:  tensor(0.1024)\n",
      "Precision:  tensor(0.1024)\n",
      "Recall:  tensor(0.1024)\n"
     ]
    }
   ],
   "source": [
    "ocgnn_precision, ocgnn_recall, ocgnn_f1 = predict_ocgnn(label_test, ocgnn_model_default_3, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gae(label_test, gae_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    gae_ip_pred_res, gae_ip_score_res = gae_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "    precision = eval_precision_at_k(label_test, gae_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, gae_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "    \n",
    "def make_gae_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "    label_train = label_train\n",
    "\n",
    "    gae_model = GAE(epoch=100, verbose=3, gpu=-1)\n",
    "    gae_compile = gae_model.fit(pyG_train, label_train)\n",
    "    return gae_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 109439.8125 | AUC 0.6821 | Recall 0.5376 | Precision 0.5376 | AP 0.5872 | F1 0.5376 | Time 0.18\n",
      "Epoch 0001: Loss 97385.5312 | AUC 0.6782 | Recall 0.5341 | Precision 0.5341 | AP 0.5960 | F1 0.5341 | Time 0.12\n",
      "Epoch 0002: Loss 88636.7422 | AUC 0.7068 | Recall 0.5835 | Precision 0.5835 | AP 0.6405 | F1 0.5836 | Time 0.11\n",
      "Epoch 0003: Loss 82179.5078 | AUC 0.7484 | Recall 0.6478 | Precision 0.6478 | AP 0.6770 | F1 0.6479 | Time 0.12\n",
      "Epoch 0004: Loss 77310.2500 | AUC 0.7879 | Recall 0.6333 | Precision 0.6333 | AP 0.7094 | F1 0.6333 | Time 0.12\n",
      "Epoch 0005: Loss 74287.6094 | AUC 0.8143 | Recall 0.6231 | Precision 0.6231 | AP 0.7244 | F1 0.6231 | Time 0.12\n",
      "Epoch 0006: Loss 73743.0312 | AUC 0.8096 | Recall 0.6136 | Precision 0.6136 | AP 0.7307 | F1 0.6136 | Time 0.13\n",
      "Epoch 0007: Loss 75513.5000 | AUC 0.8057 | Recall 0.6140 | Precision 0.6140 | AP 0.7424 | F1 0.6140 | Time 0.13\n",
      "Epoch 0008: Loss 77708.5391 | AUC 0.8072 | Recall 0.6172 | Precision 0.6172 | AP 0.7509 | F1 0.6172 | Time 0.13\n",
      "Epoch 0009: Loss 78431.9531 | AUC 0.8081 | Recall 0.6183 | Precision 0.6183 | AP 0.7535 | F1 0.6183 | Time 0.13\n",
      "Epoch 0010: Loss 77577.9922 | AUC 0.8072 | Recall 0.6172 | Precision 0.6172 | AP 0.7505 | F1 0.6172 | Time 0.12\n",
      "Epoch 0011: Loss 76009.1406 | AUC 0.8061 | Recall 0.6144 | Precision 0.6144 | AP 0.7446 | F1 0.6144 | Time 0.15\n",
      "Epoch 0012: Loss 74575.7109 | AUC 0.8068 | Recall 0.6131 | Precision 0.6131 | AP 0.7378 | F1 0.6131 | Time 0.15\n",
      "Epoch 0013: Loss 73759.7266 | AUC 0.8102 | Recall 0.6136 | Precision 0.6136 | AP 0.7314 | F1 0.6136 | Time 0.13\n",
      "Epoch 0014: Loss 73648.7031 | AUC 0.8135 | Recall 0.6164 | Precision 0.6164 | AP 0.7277 | F1 0.6164 | Time 0.12\n",
      "Epoch 0015: Loss 74038.6016 | AUC 0.8146 | Recall 0.6220 | Precision 0.6220 | AP 0.7253 | F1 0.6220 | Time 0.13\n",
      "Epoch 0016: Loss 74607.2266 | AUC 0.8130 | Recall 0.6240 | Precision 0.6240 | AP 0.7238 | F1 0.6240 | Time 0.12\n",
      "Epoch 0017: Loss 75051.1328 | AUC 0.8097 | Recall 0.6255 | Precision 0.6255 | AP 0.7223 | F1 0.6255 | Time 0.12\n",
      "Epoch 0018: Loss 75228.9609 | AUC 0.8083 | Recall 0.6253 | Precision 0.6253 | AP 0.7215 | F1 0.6253 | Time 0.12\n",
      "Epoch 0019: Loss 75115.5000 | AUC 0.8091 | Recall 0.6250 | Precision 0.6250 | AP 0.7220 | F1 0.6250 | Time 0.12\n",
      "Epoch 0020: Loss 74778.8672 | AUC 0.8116 | Recall 0.6246 | Precision 0.6246 | AP 0.7233 | F1 0.6246 | Time 0.12\n",
      "Epoch 0021: Loss 74354.2578 | AUC 0.8143 | Recall 0.6235 | Precision 0.6235 | AP 0.7247 | F1 0.6235 | Time 0.12\n",
      "Epoch 0022: Loss 73951.6406 | AUC 0.8149 | Recall 0.6214 | Precision 0.6214 | AP 0.7260 | F1 0.6214 | Time 0.12\n",
      "Epoch 0023: Loss 73680.5391 | AUC 0.8147 | Recall 0.6177 | Precision 0.6177 | AP 0.7276 | F1 0.6177 | Time 0.12\n",
      "Epoch 0024: Loss 73602.5312 | AUC 0.8131 | Recall 0.6159 | Precision 0.6159 | AP 0.7294 | F1 0.6159 | Time 0.14\n",
      "Epoch 0025: Loss 73707.5391 | AUC 0.8119 | Recall 0.6142 | Precision 0.6142 | AP 0.7315 | F1 0.6142 | Time 0.12\n",
      "Epoch 0026: Loss 73912.3047 | AUC 0.8107 | Recall 0.6144 | Precision 0.6144 | AP 0.7337 | F1 0.6144 | Time 0.13\n",
      "Epoch 0027: Loss 74096.5859 | AUC 0.8099 | Recall 0.6131 | Precision 0.6131 | AP 0.7353 | F1 0.6131 | Time 0.14\n",
      "Epoch 0028: Loss 74166.1953 | AUC 0.8098 | Recall 0.6135 | Precision 0.6135 | AP 0.7359 | F1 0.6135 | Time 0.12\n",
      "Epoch 0029: Loss 74097.3672 | AUC 0.8102 | Recall 0.6131 | Precision 0.6131 | AP 0.7354 | F1 0.6131 | Time 0.13\n",
      "Epoch 0030: Loss 73935.0234 | AUC 0.8111 | Recall 0.6142 | Precision 0.6142 | AP 0.7342 | F1 0.6142 | Time 0.14\n",
      "Epoch 0031: Loss 73756.7188 | AUC 0.8123 | Recall 0.6142 | Precision 0.6142 | AP 0.7325 | F1 0.6142 | Time 0.15\n",
      "Epoch 0032: Loss 73630.0781 | AUC 0.8133 | Recall 0.6149 | Precision 0.6149 | AP 0.7310 | F1 0.6149 | Time 0.13\n",
      "Epoch 0033: Loss 73586.4297 | AUC 0.8146 | Recall 0.6164 | Precision 0.6164 | AP 0.7298 | F1 0.6164 | Time 0.14\n",
      "Epoch 0034: Loss 73617.3828 | AUC 0.8155 | Recall 0.6174 | Precision 0.6174 | AP 0.7288 | F1 0.6174 | Time 0.12\n",
      "Epoch 0035: Loss 73688.2422 | AUC 0.8158 | Recall 0.6190 | Precision 0.6190 | AP 0.7280 | F1 0.6190 | Time 0.16\n",
      "Epoch 0036: Loss 73757.9531 | AUC 0.8160 | Recall 0.6200 | Precision 0.6200 | AP 0.7276 | F1 0.6200 | Time 0.16\n",
      "Epoch 0037: Loss 73795.5781 | AUC 0.8160 | Recall 0.6205 | Precision 0.6205 | AP 0.7274 | F1 0.6205 | Time 0.14\n",
      "Epoch 0038: Loss 73788.0078 | AUC 0.8161 | Recall 0.6205 | Precision 0.6205 | AP 0.7275 | F1 0.6205 | Time 0.15\n",
      "Epoch 0039: Loss 73741.2812 | AUC 0.8162 | Recall 0.6201 | Precision 0.6201 | AP 0.7278 | F1 0.6201 | Time 0.17\n",
      "Epoch 0040: Loss 73674.9922 | AUC 0.8162 | Recall 0.6196 | Precision 0.6196 | AP 0.7284 | F1 0.6196 | Time 0.14\n",
      "Epoch 0041: Loss 73613.5391 | AUC 0.8162 | Recall 0.6185 | Precision 0.6185 | AP 0.7291 | F1 0.6185 | Time 0.13\n",
      "Epoch 0042: Loss 73576.6406 | AUC 0.8160 | Recall 0.6170 | Precision 0.6170 | AP 0.7299 | F1 0.6170 | Time 0.13\n",
      "Epoch 0043: Loss 73572.0938 | AUC 0.8155 | Recall 0.6166 | Precision 0.6166 | AP 0.7307 | F1 0.6166 | Time 0.17\n",
      "Epoch 0044: Loss 73593.5234 | AUC 0.8149 | Recall 0.6162 | Precision 0.6162 | AP 0.7314 | F1 0.6162 | Time 0.14\n",
      "Epoch 0045: Loss 73624.2578 | AUC 0.8148 | Recall 0.6153 | Precision 0.6153 | AP 0.7320 | F1 0.6153 | Time 0.14\n",
      "Epoch 0046: Loss 73645.8516 | AUC 0.8148 | Recall 0.6153 | Precision 0.6153 | AP 0.7324 | F1 0.6153 | Time 0.17\n",
      "Epoch 0047: Loss 73646.9453 | AUC 0.8149 | Recall 0.6153 | Precision 0.6153 | AP 0.7325 | F1 0.6153 | Time 0.14\n",
      "Epoch 0048: Loss 73627.7891 | AUC 0.8151 | Recall 0.6155 | Precision 0.6155 | AP 0.7323 | F1 0.6155 | Time 0.12\n",
      "Epoch 0049: Loss 73598.1875 | AUC 0.8155 | Recall 0.6162 | Precision 0.6162 | AP 0.7320 | F1 0.6162 | Time 0.14\n",
      "Epoch 0050: Loss 73571.1953 | AUC 0.8159 | Recall 0.6164 | Precision 0.6164 | AP 0.7315 | F1 0.6164 | Time 0.13\n",
      "Epoch 0051: Loss 73556.3125 | AUC 0.8166 | Recall 0.6172 | Precision 0.6172 | AP 0.7311 | F1 0.6172 | Time 0.13\n",
      "Epoch 0052: Loss 73555.8047 | AUC 0.8171 | Recall 0.6177 | Precision 0.6177 | AP 0.7307 | F1 0.6177 | Time 0.12\n",
      "Epoch 0053: Loss 73565.1641 | AUC 0.8172 | Recall 0.6192 | Precision 0.6192 | AP 0.7303 | F1 0.6192 | Time 0.13\n",
      "Epoch 0054: Loss 73576.5078 | AUC 0.8173 | Recall 0.6192 | Precision 0.6192 | AP 0.7301 | F1 0.6192 | Time 0.18\n",
      "Epoch 0055: Loss 73582.6406 | AUC 0.8174 | Recall 0.6192 | Precision 0.6192 | AP 0.7300 | F1 0.6192 | Time 0.18\n",
      "Epoch 0056: Loss 73580.1094 | AUC 0.8175 | Recall 0.6192 | Precision 0.6192 | AP 0.7301 | F1 0.6192 | Time 0.13\n",
      "Epoch 0057: Loss 73569.9219 | AUC 0.8175 | Recall 0.6194 | Precision 0.6194 | AP 0.7303 | F1 0.6194 | Time 0.13\n",
      "Epoch 0058: Loss 73556.4609 | AUC 0.8176 | Recall 0.6192 | Precision 0.6192 | AP 0.7306 | F1 0.6192 | Time 0.14\n",
      "Epoch 0059: Loss 73544.9531 | AUC 0.8178 | Recall 0.6188 | Precision 0.6188 | AP 0.7310 | F1 0.6188 | Time 0.17\n",
      "Epoch 0060: Loss 73539.0312 | AUC 0.8178 | Recall 0.6183 | Precision 0.6183 | AP 0.7315 | F1 0.6183 | Time 0.16\n",
      "Epoch 0061: Loss 73539.1641 | AUC 0.8177 | Recall 0.6179 | Precision 0.6179 | AP 0.7318 | F1 0.6179 | Time 0.21\n",
      "Epoch 0062: Loss 73542.8984 | AUC 0.8176 | Recall 0.6177 | Precision 0.6177 | AP 0.7321 | F1 0.6177 | Time 0.14\n",
      "Epoch 0063: Loss 73546.3828 | AUC 0.8176 | Recall 0.6179 | Precision 0.6179 | AP 0.7323 | F1 0.6179 | Time 0.14\n",
      "Epoch 0064: Loss 73546.5781 | AUC 0.8177 | Recall 0.6179 | Precision 0.6179 | AP 0.7324 | F1 0.6179 | Time 0.13\n",
      "Epoch 0065: Loss 73542.6016 | AUC 0.8178 | Recall 0.6179 | Precision 0.6179 | AP 0.7324 | F1 0.6179 | Time 0.14\n",
      "Epoch 0066: Loss 73535.9219 | AUC 0.8181 | Recall 0.6181 | Precision 0.6181 | AP 0.7324 | F1 0.6181 | Time 0.17\n",
      "Epoch 0067: Loss 73529.1875 | AUC 0.8183 | Recall 0.6183 | Precision 0.6183 | AP 0.7323 | F1 0.6183 | Time 0.13\n",
      "Epoch 0068: Loss 73524.6016 | AUC 0.8185 | Recall 0.6188 | Precision 0.6188 | AP 0.7321 | F1 0.6188 | Time 0.14\n",
      "Epoch 0069: Loss 73522.8984 | AUC 0.8187 | Recall 0.6194 | Precision 0.6194 | AP 0.7319 | F1 0.6194 | Time 0.15\n",
      "Epoch 0070: Loss 73523.2578 | AUC 0.8187 | Recall 0.6194 | Precision 0.6194 | AP 0.7318 | F1 0.6194 | Time 0.16\n",
      "Epoch 0071: Loss 73523.9375 | AUC 0.8188 | Recall 0.6194 | Precision 0.6194 | AP 0.7317 | F1 0.6194 | Time 0.12\n",
      "Epoch 0072: Loss 73523.4062 | AUC 0.8188 | Recall 0.6200 | Precision 0.6200 | AP 0.7317 | F1 0.6200 | Time 0.13\n",
      "Epoch 0073: Loss 73521.0000 | AUC 0.8189 | Recall 0.6200 | Precision 0.6200 | AP 0.7317 | F1 0.6200 | Time 0.14\n",
      "Epoch 0074: Loss 73517.1953 | AUC 0.8190 | Recall 0.6194 | Precision 0.6194 | AP 0.7319 | F1 0.6194 | Time 0.13\n",
      "Epoch 0075: Loss 73513.1094 | AUC 0.8191 | Recall 0.6194 | Precision 0.6194 | AP 0.7321 | F1 0.6194 | Time 0.13\n",
      "Epoch 0076: Loss 73509.8281 | AUC 0.8192 | Recall 0.6196 | Precision 0.6196 | AP 0.7323 | F1 0.6196 | Time 0.14\n",
      "Epoch 0077: Loss 73507.8516 | AUC 0.8193 | Recall 0.6196 | Precision 0.6196 | AP 0.7326 | F1 0.6196 | Time 0.12\n",
      "Epoch 0078: Loss 73506.9453 | AUC 0.8194 | Recall 0.6190 | Precision 0.6190 | AP 0.7328 | F1 0.6190 | Time 0.14\n",
      "Epoch 0079: Loss 73506.2656 | AUC 0.8194 | Recall 0.6190 | Precision 0.6190 | AP 0.7329 | F1 0.6190 | Time 0.13\n",
      "Epoch 0080: Loss 73505.0547 | AUC 0.8195 | Recall 0.6192 | Precision 0.6192 | AP 0.7330 | F1 0.6192 | Time 0.13\n",
      "Epoch 0081: Loss 73502.9531 | AUC 0.8196 | Recall 0.6192 | Precision 0.6192 | AP 0.7330 | F1 0.6192 | Time 0.13\n",
      "Epoch 0082: Loss 73500.1797 | AUC 0.8197 | Recall 0.6198 | Precision 0.6198 | AP 0.7330 | F1 0.6198 | Time 0.15\n",
      "Epoch 0083: Loss 73497.3203 | AUC 0.8197 | Recall 0.6201 | Precision 0.6201 | AP 0.7330 | F1 0.6201 | Time 0.15\n",
      "Epoch 0084: Loss 73494.8750 | AUC 0.8198 | Recall 0.6200 | Precision 0.6200 | AP 0.7329 | F1 0.6200 | Time 0.17\n",
      "Epoch 0085: Loss 73493.0469 | AUC 0.8198 | Recall 0.6200 | Precision 0.6200 | AP 0.7328 | F1 0.6200 | Time 0.14\n",
      "Epoch 0086: Loss 73491.6797 | AUC 0.8198 | Recall 0.6203 | Precision 0.6203 | AP 0.7328 | F1 0.6203 | Time 0.16\n",
      "Epoch 0087: Loss 73490.3438 | AUC 0.8198 | Recall 0.6205 | Precision 0.6205 | AP 0.7328 | F1 0.6205 | Time 0.18\n",
      "Epoch 0088: Loss 73488.7109 | AUC 0.8199 | Recall 0.6207 | Precision 0.6207 | AP 0.7328 | F1 0.6207 | Time 0.16\n",
      "Epoch 0089: Loss 73486.6484 | AUC 0.8199 | Recall 0.6207 | Precision 0.6207 | AP 0.7328 | F1 0.6207 | Time 0.16\n",
      "Epoch 0090: Loss 73484.3359 | AUC 0.8200 | Recall 0.6207 | Precision 0.6207 | AP 0.7329 | F1 0.6207 | Time 0.16\n",
      "Epoch 0091: Loss 73482.0469 | AUC 0.8201 | Recall 0.6207 | Precision 0.6207 | AP 0.7331 | F1 0.6207 | Time 0.14\n",
      "Epoch 0092: Loss 73480.0000 | AUC 0.8202 | Recall 0.6209 | Precision 0.6209 | AP 0.7332 | F1 0.6209 | Time 0.13\n",
      "Epoch 0093: Loss 73478.2500 | AUC 0.8203 | Recall 0.6211 | Precision 0.6211 | AP 0.7333 | F1 0.6211 | Time 0.13\n",
      "Epoch 0094: Loss 73476.6484 | AUC 0.8203 | Recall 0.6207 | Precision 0.6207 | AP 0.7334 | F1 0.6207 | Time 0.15\n",
      "Epoch 0095: Loss 73475.0000 | AUC 0.8204 | Recall 0.6209 | Precision 0.6209 | AP 0.7335 | F1 0.6209 | Time 0.14\n",
      "Epoch 0096: Loss 73473.1797 | AUC 0.8204 | Recall 0.6211 | Precision 0.6211 | AP 0.7336 | F1 0.6211 | Time 0.12\n",
      "Epoch 0097: Loss 73471.1562 | AUC 0.8205 | Recall 0.6213 | Precision 0.6213 | AP 0.7336 | F1 0.6213 | Time 0.13\n",
      "Epoch 0098: Loss 73469.0781 | AUC 0.8205 | Recall 0.6214 | Precision 0.6214 | AP 0.7336 | F1 0.6214 | Time 0.12\n",
      "Epoch 0099: Loss 73467.0625 | AUC 0.8204 | Recall 0.6213 | Precision 0.6213 | AP 0.7335 | F1 0.6213 | Time 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 11.3603 | AUC 0.8269 | Recall 0.7079 | Precision 0.7079 | AP 0.8049 | F1 0.7079 | Time 0.04\n",
      "F1 score:  tensor(0.7079)\n",
      "Precision:  tensor(0.7079)\n",
      "Recall:  tensor(0.7079)\n"
     ]
    }
   ],
   "source": [
    "gae_model = make_gae_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_gae(label_test, gae_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(gae_model, open('ml-model/gae_model_default_1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_model_default = pickle.load(open('ml-model/gae_model_default_1.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 11.3603 | AUC 0.8269 | Recall 0.7079 | Precision 0.7079 | AP 0.8049 | F1 0.7079 | Time 0.06\n",
      "F1 score:  tensor(0.7079)\n",
      "Precision:  tensor(0.7079)\n",
      "Recall:  tensor(0.7079)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = predict_gae(label_test, gae_model_default, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 150562.7031 | AUC 0.7831 | Recall 0.6683 | Precision 0.6683 | AP 0.6491 | F1 0.7163 | Time 0.08\n",
      "Epoch 0001: Loss 81834.0156 | AUC 0.7770 | Recall 0.6520 | Precision 0.6520 | AP 0.6427 | F1 0.6718 | Time 0.09\n",
      "Epoch 0002: Loss 46966.7656 | AUC 0.7625 | Recall 0.6566 | Precision 0.6566 | AP 0.6212 | F1 0.6682 | Time 0.09\n",
      "Epoch 0003: Loss 27543.0078 | AUC 0.7513 | Recall 0.6707 | Precision 0.6707 | AP 0.5956 | F1 0.6703 | Time 0.09\n",
      "Epoch 0004: Loss 17735.6562 | AUC 0.6986 | Recall 0.5250 | Precision 0.5250 | AP 0.5343 | F1 0.5250 | Time 0.07\n",
      "Epoch 0005: Loss 17373.9395 | AUC 0.6759 | Recall 0.5080 | Precision 0.5080 | AP 0.5064 | F1 0.5069 | Time 0.07\n",
      "Epoch 0006: Loss 22986.9766 | AUC 0.6917 | Recall 0.6186 | Precision 0.6186 | AP 0.5338 | F1 0.6031 | Time 0.07\n",
      "Epoch 0007: Loss 28433.4512 | AUC 0.6881 | Recall 0.6130 | Precision 0.6130 | AP 0.5356 | F1 0.6149 | Time 0.07\n",
      "Epoch 0008: Loss 29441.6836 | AUC 0.6882 | Recall 0.6117 | Precision 0.6117 | AP 0.5367 | F1 0.6140 | Time 0.07\n",
      "Epoch 0009: Loss 26619.1523 | AUC 0.6896 | Recall 0.6171 | Precision 0.6171 | AP 0.5350 | F1 0.6175 | Time 0.07\n",
      "Epoch 0010: Loss 22303.4043 | AUC 0.6900 | Recall 0.6182 | Precision 0.6182 | AP 0.5310 | F1 0.6054 | Time 0.09\n",
      "Epoch 0011: Loss 18631.1621 | AUC 0.6940 | Recall 0.6156 | Precision 0.6156 | AP 0.5273 | F1 0.6036 | Time 0.09\n",
      "Epoch 0012: Loss 16734.6641 | AUC 0.6740 | Recall 0.5026 | Precision 0.5026 | AP 0.4905 | F1 0.5026 | Time 0.07\n",
      "Epoch 0013: Loss 16710.9824 | AUC 0.6835 | Recall 0.4972 | Precision 0.4972 | AP 0.5009 | F1 0.4972 | Time 0.08\n",
      "Epoch 0014: Loss 17952.7363 | AUC 0.7012 | Recall 0.5280 | Precision 0.5280 | AP 0.5391 | F1 0.5280 | Time 0.07\n",
      "Epoch 0015: Loss 19551.7266 | AUC 0.7332 | Recall 0.6481 | Precision 0.6481 | AP 0.5699 | F1 0.6341 | Time 0.08\n",
      "Epoch 0016: Loss 20778.2344 | AUC 0.7391 | Recall 0.6572 | Precision 0.6572 | AP 0.5758 | F1 0.6527 | Time 0.11\n",
      "Epoch 0017: Loss 21207.3613 | AUC 0.7406 | Recall 0.6588 | Precision 0.6588 | AP 0.5784 | F1 0.6529 | Time 0.09\n",
      "Epoch 0018: Loss 20807.2324 | AUC 0.7393 | Recall 0.6570 | Precision 0.6570 | AP 0.5759 | F1 0.6526 | Time 0.10\n",
      "Epoch 0019: Loss 19791.3027 | AUC 0.7347 | Recall 0.6501 | Precision 0.6501 | AP 0.5714 | F1 0.6510 | Time 0.10\n",
      "Epoch 0020: Loss 18515.9746 | AUC 0.7174 | Recall 0.6364 | Precision 0.6364 | AP 0.5578 | F1 0.6334 | Time 0.09\n",
      "Epoch 0021: Loss 17367.9727 | AUC 0.6916 | Recall 0.5222 | Precision 0.5222 | AP 0.5223 | F1 0.5222 | Time 0.08\n",
      "Epoch 0022: Loss 16660.4082 | AUC 0.6820 | Recall 0.4974 | Precision 0.4974 | AP 0.4981 | F1 0.4974 | Time 0.07\n",
      "Epoch 0023: Loss 16541.4277 | AUC 0.6772 | Recall 0.4867 | Precision 0.4867 | AP 0.4810 | F1 0.4864 | Time 0.09\n",
      "Epoch 0024: Loss 16935.7227 | AUC 0.6750 | Recall 0.5044 | Precision 0.5044 | AP 0.4971 | F1 0.5036 | Time 0.08\n",
      "Epoch 0025: Loss 17561.3809 | AUC 0.6777 | Recall 0.5074 | Precision 0.5074 | AP 0.5092 | F1 0.5074 | Time 0.09\n",
      "Epoch 0026: Loss 18061.1816 | AUC 0.6861 | Recall 0.6136 | Precision 0.6136 | AP 0.5221 | F1 0.6095 | Time 0.07\n",
      "Epoch 0027: Loss 18190.5254 | AUC 0.6913 | Recall 0.6134 | Precision 0.6134 | AP 0.5251 | F1 0.6080 | Time 0.08\n",
      "Epoch 0028: Loss 17923.0566 | AUC 0.6826 | Recall 0.6130 | Precision 0.6130 | AP 0.5190 | F1 0.6098 | Time 0.08\n",
      "Epoch 0029: Loss 17420.3906 | AUC 0.6766 | Recall 0.5080 | Precision 0.5080 | AP 0.5070 | F1 0.5068 | Time 0.07\n",
      "Epoch 0030: Loss 16915.4238 | AUC 0.6754 | Recall 0.5044 | Precision 0.5044 | AP 0.4968 | F1 0.5043 | Time 0.08\n",
      "Epoch 0031: Loss 16592.2188 | AUC 0.6768 | Recall 0.4985 | Precision 0.4985 | AP 0.4885 | F1 0.4964 | Time 0.08\n",
      "Epoch 0032: Loss 16519.9414 | AUC 0.5407 | Recall 0.4778 | Precision 0.4778 | AP 0.4160 | F1 0.4769 | Time 0.11\n",
      "Epoch 0033: Loss 16652.7578 | AUC 0.6819 | Recall 0.4974 | Precision 0.4974 | AP 0.4977 | F1 0.4898 | Time 0.10\n",
      "Epoch 0034: Loss 16878.7734 | AUC 0.6844 | Recall 0.5085 | Precision 0.5085 | AP 0.5056 | F1 0.5084 | Time 0.08\n",
      "Epoch 0035: Loss 17076.9688 | AUC 0.6885 | Recall 0.5117 | Precision 0.5117 | AP 0.5142 | F1 0.5113 | Time 0.07\n",
      "Epoch 0036: Loss 17163.2324 | AUC 0.6896 | Recall 0.5132 | Precision 0.5132 | AP 0.5167 | F1 0.5149 | Time 0.09\n",
      "Epoch 0037: Loss 17112.2285 | AUC 0.6893 | Recall 0.5119 | Precision 0.5119 | AP 0.5156 | F1 0.5108 | Time 0.08\n",
      "Epoch 0038: Loss 16954.9258 | AUC 0.6859 | Recall 0.5100 | Precision 0.5100 | AP 0.5088 | F1 0.5100 | Time 0.09\n",
      "Epoch 0039: Loss 16758.2578 | AUC 0.6828 | Recall 0.5035 | Precision 0.5035 | AP 0.5020 | F1 0.4987 | Time 0.10\n",
      "Epoch 0040: Loss 16595.7637 | AUC 0.6823 | Recall 0.4883 | Precision 0.4883 | AP 0.4919 | F1 0.4850 | Time 0.09\n",
      "Epoch 0041: Loss 16518.8516 | AUC 0.4996 | Recall 0.4778 | Precision 0.4778 | AP 0.4003 | F1 0.4769 | Time 0.09\n",
      "Epoch 0042: Loss 16538.2246 | AUC 0.6780 | Recall 0.4865 | Precision 0.4865 | AP 0.4808 | F1 0.4855 | Time 0.09\n",
      "Epoch 0043: Loss 16622.7715 | AUC 0.6774 | Recall 0.4985 | Precision 0.4985 | AP 0.4906 | F1 0.4975 | Time 0.10\n",
      "Epoch 0044: Loss 16717.0859 | AUC 0.6759 | Recall 0.5022 | Precision 0.5022 | AP 0.4910 | F1 0.5006 | Time 0.09\n",
      "Epoch 0045: Loss 16769.6191 | AUC 0.6748 | Recall 0.5028 | Precision 0.5028 | AP 0.4919 | F1 0.5018 | Time 0.08\n",
      "Epoch 0046: Loss 16756.7227 | AUC 0.6748 | Recall 0.5028 | Precision 0.5028 | AP 0.4913 | F1 0.5019 | Time 0.08\n",
      "Epoch 0047: Loss 16690.0000 | AUC 0.6769 | Recall 0.5011 | Precision 0.5011 | AP 0.4911 | F1 0.5008 | Time 0.09\n",
      "Epoch 0048: Loss 16604.8594 | AUC 0.6773 | Recall 0.4987 | Precision 0.4987 | AP 0.4895 | F1 0.4983 | Time 0.07\n",
      "Epoch 0049: Loss 16539.2441 | AUC 0.6781 | Recall 0.4865 | Precision 0.4865 | AP 0.4810 | F1 0.4855 | Time 0.07\n",
      "Epoch 0050: Loss 16515.2402 | AUC 0.6829 | Recall 0.4807 | Precision 0.4807 | AP 0.4784 | F1 0.4803 | Time 0.07\n",
      "Epoch 0051: Loss 16532.0098 | AUC 0.6846 | Recall 0.4826 | Precision 0.4826 | AP 0.4839 | F1 0.4826 | Time 0.11\n",
      "Epoch 0052: Loss 16570.7891 | AUC 0.6829 | Recall 0.4878 | Precision 0.4878 | AP 0.4878 | F1 0.4878 | Time 0.10\n",
      "Epoch 0053: Loss 16606.8438 | AUC 0.6823 | Recall 0.4904 | Precision 0.4904 | AP 0.4931 | F1 0.4841 | Time 0.10\n",
      "Epoch 0054: Loss 16621.5352 | AUC 0.6821 | Recall 0.4956 | Precision 0.4956 | AP 0.4950 | F1 0.4922 | Time 0.09\n",
      "Epoch 0055: Loss 16609.3945 | AUC 0.6824 | Recall 0.4904 | Precision 0.4904 | AP 0.4936 | F1 0.4839 | Time 0.08\n",
      "Epoch 0056: Loss 16578.2500 | AUC 0.6824 | Recall 0.4881 | Precision 0.4881 | AP 0.4889 | F1 0.4865 | Time 0.09\n",
      "Epoch 0057: Loss 16543.5723 | AUC 0.6838 | Recall 0.4841 | Precision 0.4841 | AP 0.4851 | F1 0.4841 | Time 0.07\n",
      "Epoch 0058: Loss 16520.3047 | AUC 0.6080 | Recall 0.4779 | Precision 0.4779 | AP 0.4570 | F1 0.4767 | Time 0.08\n",
      "Epoch 0059: Loss 16515.8945 | AUC 0.6819 | Recall 0.4817 | Precision 0.4817 | AP 0.4791 | F1 0.4817 | Time 0.09\n",
      "Epoch 0060: Loss 16527.7285 | AUC 0.6789 | Recall 0.4852 | Precision 0.4852 | AP 0.4800 | F1 0.4849 | Time 0.10\n",
      "Epoch 0061: Loss 16545.7051 | AUC 0.6780 | Recall 0.4889 | Precision 0.4889 | AP 0.4823 | F1 0.4865 | Time 0.14\n",
      "Epoch 0062: Loss 16558.3320 | AUC 0.6774 | Recall 0.4957 | Precision 0.4957 | AP 0.4848 | F1 0.4937 | Time 0.08\n",
      "Epoch 0063: Loss 16558.8184 | AUC 0.6774 | Recall 0.4961 | Precision 0.4961 | AP 0.4848 | F1 0.4937 | Time 0.10\n",
      "Epoch 0064: Loss 16547.8711 | AUC 0.6777 | Recall 0.4889 | Precision 0.4889 | AP 0.4826 | F1 0.4862 | Time 0.07\n",
      "Epoch 0065: Loss 16532.0977 | AUC 0.6788 | Recall 0.4863 | Precision 0.4863 | AP 0.4806 | F1 0.4859 | Time 0.07\n",
      "Epoch 0066: Loss 16519.5312 | AUC 0.6805 | Recall 0.4835 | Precision 0.4835 | AP 0.4799 | F1 0.4829 | Time 0.08\n",
      "Epoch 0067: Loss 16515.1934 | AUC 0.6828 | Recall 0.4781 | Precision 0.4781 | AP 0.4775 | F1 0.4769 | Time 0.08\n",
      "Epoch 0068: Loss 16519.0254 | AUC 0.5845 | Recall 0.4778 | Precision 0.4778 | AP 0.4412 | F1 0.4766 | Time 0.09\n",
      "Epoch 0069: Loss 16526.8379 | AUC 0.6843 | Recall 0.4818 | Precision 0.4818 | AP 0.4827 | F1 0.4814 | Time 0.08\n",
      "Epoch 0070: Loss 16533.2305 | AUC 0.6847 | Recall 0.4826 | Precision 0.4826 | AP 0.4842 | F1 0.4822 | Time 0.09\n",
      "Epoch 0071: Loss 16534.5508 | AUC 0.6847 | Recall 0.4824 | Precision 0.4824 | AP 0.4844 | F1 0.4818 | Time 0.15\n",
      "Epoch 0072: Loss 16530.4746 | AUC 0.6845 | Recall 0.4826 | Precision 0.4826 | AP 0.4837 | F1 0.4826 | Time 0.09\n",
      "Epoch 0073: Loss 16523.6035 | AUC 0.6840 | Recall 0.4781 | Precision 0.4781 | AP 0.4811 | F1 0.4777 | Time 0.09\n",
      "Epoch 0074: Loss 16517.6289 | AUC 0.4934 | Recall 0.4770 | Precision 0.4770 | AP 0.3941 | F1 0.4772 | Time 0.10\n",
      "Epoch 0075: Loss 16515.1992 | AUC 0.6827 | Recall 0.4781 | Precision 0.4781 | AP 0.4774 | F1 0.4777 | Time 0.11\n",
      "Epoch 0076: Loss 16516.6543 | AUC 0.6823 | Recall 0.4817 | Precision 0.4817 | AP 0.4799 | F1 0.4811 | Time 0.09\n",
      "Epoch 0077: Loss 16520.1855 | AUC 0.6805 | Recall 0.4835 | Precision 0.4835 | AP 0.4800 | F1 0.4829 | Time 0.08\n",
      "Epoch 0078: Loss 16523.1699 | AUC 0.6801 | Recall 0.4839 | Precision 0.4839 | AP 0.4801 | F1 0.4826 | Time 0.08\n",
      "Epoch 0079: Loss 16523.7539 | AUC 0.6803 | Recall 0.4839 | Precision 0.4839 | AP 0.4804 | F1 0.4826 | Time 0.08\n",
      "Epoch 0080: Loss 16521.7793 | AUC 0.6805 | Recall 0.4839 | Precision 0.4839 | AP 0.4802 | F1 0.4826 | Time 0.08\n",
      "Epoch 0081: Loss 16518.5879 | AUC 0.6809 | Recall 0.4817 | Precision 0.4817 | AP 0.4799 | F1 0.4809 | Time 0.14\n",
      "Epoch 0082: Loss 16516.0020 | AUC 0.6832 | Recall 0.4817 | Precision 0.4817 | AP 0.4800 | F1 0.4801 | Time 0.09\n",
      "Epoch 0083: Loss 16515.1875 | AUC 0.6827 | Recall 0.4781 | Precision 0.4781 | AP 0.4774 | F1 0.4778 | Time 0.08\n",
      "Epoch 0084: Loss 16516.1172 | AUC 0.6804 | Recall 0.4778 | Precision 0.4778 | AP 0.4723 | F1 0.4771 | Time 0.08\n",
      "Epoch 0085: Loss 16517.7891 | AUC 0.5807 | Recall 0.4778 | Precision 0.4778 | AP 0.4302 | F1 0.4769 | Time 0.07\n",
      "Epoch 0086: Loss 16518.9648 | AUC 0.6833 | Recall 0.4778 | Precision 0.4778 | AP 0.4783 | F1 0.4766 | Time 0.07\n",
      "Epoch 0087: Loss 16518.9277 | AUC 0.6833 | Recall 0.4778 | Precision 0.4778 | AP 0.4781 | F1 0.4766 | Time 0.08\n",
      "Epoch 0088: Loss 16517.7988 | AUC 0.5992 | Recall 0.4778 | Precision 0.4778 | AP 0.4495 | F1 0.4769 | Time 0.09\n",
      "Epoch 0089: Loss 16516.3262 | AUC 0.6803 | Recall 0.4778 | Precision 0.4778 | AP 0.4721 | F1 0.4772 | Time 0.09\n",
      "Epoch 0090: Loss 16515.3379 | AUC 0.6824 | Recall 0.4781 | Precision 0.4781 | AP 0.4766 | F1 0.4767 | Time 0.09\n",
      "Epoch 0091: Loss 16515.2461 | AUC 0.6833 | Recall 0.4783 | Precision 0.4783 | AP 0.4787 | F1 0.4773 | Time 0.08\n",
      "Epoch 0092: Loss 16515.8633 | AUC 0.6834 | Recall 0.4817 | Precision 0.4817 | AP 0.4801 | F1 0.4801 | Time 0.09\n",
      "Epoch 0093: Loss 16516.6270 | AUC 0.6832 | Recall 0.4815 | Precision 0.4815 | AP 0.4805 | F1 0.4815 | Time 0.08\n",
      "Epoch 0094: Loss 16516.9785 | AUC 0.6824 | Recall 0.4815 | Precision 0.4815 | AP 0.4802 | F1 0.4809 | Time 0.08\n",
      "Epoch 0095: Loss 16516.7168 | AUC 0.6832 | Recall 0.4817 | Precision 0.4817 | AP 0.4805 | F1 0.4814 | Time 0.08\n",
      "Epoch 0096: Loss 16516.0586 | AUC 0.6834 | Recall 0.4817 | Precision 0.4817 | AP 0.4802 | F1 0.4801 | Time 0.08\n",
      "Epoch 0097: Loss 16515.4297 | AUC 0.6835 | Recall 0.4809 | Precision 0.4809 | AP 0.4793 | F1 0.4808 | Time 0.08\n",
      "Epoch 0098: Loss 16515.1699 | AUC 0.6828 | Recall 0.4781 | Precision 0.4781 | AP 0.4777 | F1 0.4778 | Time 0.07\n",
      "Epoch 0099: Loss 16515.3281 | AUC 0.6824 | Recall 0.4781 | Precision 0.4781 | AP 0.4766 | F1 0.4767 | Time 0.08\n",
      "Test: Loss 3.2199 | AUC 0.7310 | Recall 0.7166 | Precision 0.7166 | AP 0.6616 | F1 0.7457 | Time 0.03\n",
      "F1 score:  tensor(0.7166)\n",
      "Precision:  tensor(0.7166)\n",
      "Recall:  tensor(0.7166)\n"
     ]
    }
   ],
   "source": [
    "gae_model = make_gae_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_gae(label_test_2, gae_model, test_graph_2, test_node_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(gae_model, open('ml-model/gae_model_default_2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_model_default_2 = pickle.load(open('ml-model/gae_model_default_2.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 3.2199 | AUC 0.7310 | Recall 0.7166 | Precision 0.7166 | AP 0.6616 | F1 0.7457 | Time 0.02\n",
      "F1 score:  tensor(0.7166)\n",
      "Precision:  tensor(0.7166)\n",
      "Recall:  tensor(0.7166)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = predict_gae(label_test_2, gae_model_default_2, test_graph_2, test_node_features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 127949.6953 | AUC 0.7872 | Recall 0.6603 | Precision 0.6603 | AP 0.6569 | F1 0.7314 | Time 0.07\n",
      "Epoch 0001: Loss 78714.2578 | AUC 0.7806 | Recall 0.6533 | Precision 0.6533 | AP 0.6496 | F1 0.6937 | Time 0.06\n",
      "Epoch 0002: Loss 46185.1367 | AUC 0.7718 | Recall 0.6492 | Precision 0.6492 | AP 0.6404 | F1 0.6908 | Time 0.06\n",
      "Epoch 0003: Loss 22419.0508 | AUC 0.7640 | Recall 0.6510 | Precision 0.6510 | AP 0.6284 | F1 0.6914 | Time 0.09\n",
      "Epoch 0004: Loss 9922.5371 | AUC 0.7373 | Recall 0.6540 | Precision 0.6540 | AP 0.5978 | F1 0.6649 | Time 0.07\n",
      "Epoch 0005: Loss 9217.3730 | AUC 0.7307 | Recall 0.6531 | Precision 0.6531 | AP 0.5857 | F1 0.6644 | Time 0.07\n",
      "Epoch 0006: Loss 16505.4883 | AUC 0.7587 | Recall 0.6446 | Precision 0.6446 | AP 0.6228 | F1 0.6881 | Time 0.07\n",
      "Epoch 0007: Loss 22773.4805 | AUC 0.7605 | Recall 0.6433 | Precision 0.6433 | AP 0.6253 | F1 0.6881 | Time 0.08\n",
      "Epoch 0008: Loss 22723.8906 | AUC 0.7604 | Recall 0.6433 | Precision 0.6433 | AP 0.6253 | F1 0.6881 | Time 0.07\n",
      "Epoch 0009: Loss 18126.6465 | AUC 0.7595 | Recall 0.6455 | Precision 0.6455 | AP 0.6236 | F1 0.6884 | Time 0.08\n",
      "Epoch 0010: Loss 12701.7354 | AUC 0.7545 | Recall 0.6455 | Precision 0.6455 | AP 0.6141 | F1 0.6886 | Time 0.08\n",
      "Epoch 0011: Loss 9101.8369 | AUC 0.7299 | Recall 0.6553 | Precision 0.6553 | AP 0.5834 | F1 0.6655 | Time 0.10\n",
      "Epoch 0012: Loss 8175.6943 | AUC 0.7256 | Recall 0.6838 | Precision 0.6838 | AP 0.5492 | F1 0.6845 | Time 0.10\n",
      "Epoch 0013: Loss 9351.7988 | AUC 0.7325 | Recall 0.6583 | Precision 0.6583 | AP 0.5858 | F1 0.6675 | Time 0.08\n",
      "Epoch 0014: Loss 11400.9180 | AUC 0.7569 | Recall 0.6542 | Precision 0.6542 | AP 0.6161 | F1 0.6935 | Time 0.07\n",
      "Epoch 0015: Loss 13131.3711 | AUC 0.7583 | Recall 0.6529 | Precision 0.6529 | AP 0.6169 | F1 0.6924 | Time 0.07\n",
      "Epoch 0016: Loss 13854.2490 | AUC 0.7602 | Recall 0.6536 | Precision 0.6536 | AP 0.6207 | F1 0.6923 | Time 0.07\n",
      "Epoch 0017: Loss 13504.4873 | AUC 0.7592 | Recall 0.6525 | Precision 0.6525 | AP 0.6188 | F1 0.6922 | Time 0.10\n",
      "Epoch 0018: Loss 12319.7422 | AUC 0.7573 | Recall 0.6536 | Precision 0.6536 | AP 0.6164 | F1 0.6930 | Time 0.09\n",
      "Epoch 0019: Loss 10745.0771 | AUC 0.7524 | Recall 0.6546 | Precision 0.6546 | AP 0.6133 | F1 0.6937 | Time 0.09\n",
      "Epoch 0020: Loss 9284.1162 | AUC 0.7320 | Recall 0.6579 | Precision 0.6579 | AP 0.5842 | F1 0.6675 | Time 0.08\n",
      "Epoch 0021: Loss 8359.0293 | AUC 0.7269 | Recall 0.6731 | Precision 0.6731 | AP 0.5636 | F1 0.6774 | Time 0.08\n",
      "Epoch 0022: Loss 8181.6616 | AUC 0.7257 | Recall 0.6768 | Precision 0.6768 | AP 0.5491 | F1 0.6796 | Time 0.07\n",
      "Epoch 0023: Loss 8655.1562 | AUC 0.7255 | Recall 0.6588 | Precision 0.6588 | AP 0.5700 | F1 0.6681 | Time 0.09\n",
      "Epoch 0024: Loss 9410.5869 | AUC 0.7342 | Recall 0.6521 | Precision 0.6521 | AP 0.5931 | F1 0.6634 | Time 0.08\n",
      "Epoch 0025: Loss 10015.6465 | AUC 0.7473 | Recall 0.6509 | Precision 0.6509 | AP 0.6093 | F1 0.6934 | Time 0.10\n",
      "Epoch 0026: Loss 10164.0645 | AUC 0.7504 | Recall 0.6521 | Precision 0.6521 | AP 0.6107 | F1 0.6931 | Time 0.08\n",
      "Epoch 0027: Loss 9821.7559 | AUC 0.7434 | Recall 0.6547 | Precision 0.6547 | AP 0.6062 | F1 0.6938 | Time 0.08\n",
      "Epoch 0028: Loss 9197.5361 | AUC 0.7308 | Recall 0.6544 | Precision 0.6544 | AP 0.5853 | F1 0.6652 | Time 0.08\n",
      "Epoch 0029: Loss 8589.0205 | AUC 0.7251 | Recall 0.6623 | Precision 0.6623 | AP 0.5689 | F1 0.6692 | Time 0.08\n",
      "Epoch 0030: Loss 8222.4502 | AUC 0.7247 | Recall 0.6735 | Precision 0.6735 | AP 0.5563 | F1 0.6777 | Time 0.11\n",
      "Epoch 0031: Loss 8170.6431 | AUC 0.7269 | Recall 0.6838 | Precision 0.6838 | AP 0.5492 | F1 0.6845 | Time 0.07\n",
      "Epoch 0032: Loss 8362.3506 | AUC 0.7267 | Recall 0.6723 | Precision 0.6723 | AP 0.5634 | F1 0.6769 | Time 0.08\n",
      "Epoch 0033: Loss 8648.2314 | AUC 0.7259 | Recall 0.6644 | Precision 0.6644 | AP 0.5663 | F1 0.6715 | Time 0.08\n",
      "Epoch 0034: Loss 8877.7344 | AUC 0.7281 | Recall 0.6622 | Precision 0.6622 | AP 0.5727 | F1 0.6700 | Time 0.07\n",
      "Epoch 0035: Loss 8954.9062 | AUC 0.7286 | Recall 0.6614 | Precision 0.6614 | AP 0.5745 | F1 0.6693 | Time 0.07\n",
      "Epoch 0036: Loss 8861.0127 | AUC 0.7279 | Recall 0.6623 | Precision 0.6623 | AP 0.5724 | F1 0.6702 | Time 0.08\n",
      "Epoch 0037: Loss 8630.2061 | AUC 0.7258 | Recall 0.6649 | Precision 0.6649 | AP 0.5661 | F1 0.6719 | Time 0.07\n",
      "Epoch 0038: Loss 8379.0840 | AUC 0.7261 | Recall 0.6711 | Precision 0.6711 | AP 0.5628 | F1 0.6763 | Time 0.07\n",
      "Epoch 0039: Loss 8201.0391 | AUC 0.7295 | Recall 0.6803 | Precision 0.6803 | AP 0.5523 | F1 0.6822 | Time 0.08\n",
      "Epoch 0040: Loss 8157.8564 | AUC 0.7287 | Recall 0.6801 | Precision 0.6801 | AP 0.5482 | F1 0.6819 | Time 0.08\n",
      "Epoch 0041: Loss 8241.5527 | AUC 0.7247 | Recall 0.6725 | Precision 0.6725 | AP 0.5581 | F1 0.6771 | Time 0.07\n",
      "Epoch 0042: Loss 8373.5322 | AUC 0.7228 | Recall 0.6653 | Precision 0.6653 | AP 0.5614 | F1 0.6723 | Time 0.07\n",
      "Epoch 0043: Loss 8461.7930 | AUC 0.7233 | Recall 0.6635 | Precision 0.6635 | AP 0.5636 | F1 0.6710 | Time 0.14\n",
      "Epoch 0044: Loss 8459.3408 | AUC 0.7229 | Recall 0.6635 | Precision 0.6635 | AP 0.5628 | F1 0.6710 | Time 0.11\n",
      "Epoch 0045: Loss 8377.7676 | AUC 0.7228 | Recall 0.6653 | Precision 0.6653 | AP 0.5614 | F1 0.6723 | Time 0.10\n",
      "Epoch 0046: Loss 8267.5547 | AUC 0.7246 | Recall 0.6705 | Precision 0.6705 | AP 0.5605 | F1 0.6759 | Time 0.10\n",
      "Epoch 0047: Loss 8183.2651 | AUC 0.7260 | Recall 0.6774 | Precision 0.6774 | AP 0.5496 | F1 0.6800 | Time 0.08\n",
      "Epoch 0048: Loss 8154.9565 | AUC 0.7311 | Recall 0.6831 | Precision 0.6831 | AP 0.5437 | F1 0.6840 | Time 0.09\n",
      "Epoch 0049: Loss 8179.1143 | AUC 0.7313 | Recall 0.6831 | Precision 0.6831 | AP 0.5508 | F1 0.6840 | Time 0.08\n",
      "Epoch 0050: Loss 8224.4775 | AUC 0.7277 | Recall 0.6790 | Precision 0.6790 | AP 0.5534 | F1 0.6808 | Time 0.08\n",
      "Epoch 0051: Loss 8261.7354 | AUC 0.7271 | Recall 0.6764 | Precision 0.6764 | AP 0.5582 | F1 0.6796 | Time 0.08\n",
      "Epoch 0052: Loss 8272.1826 | AUC 0.7271 | Recall 0.6762 | Precision 0.6762 | AP 0.5595 | F1 0.6793 | Time 0.08\n",
      "Epoch 0053: Loss 8252.7363 | AUC 0.7274 | Recall 0.6768 | Precision 0.6768 | AP 0.5575 | F1 0.6799 | Time 0.08\n",
      "Epoch 0054: Loss 8215.0566 | AUC 0.7282 | Recall 0.6790 | Precision 0.6790 | AP 0.5528 | F1 0.6813 | Time 0.08\n",
      "Epoch 0055: Loss 8177.8501 | AUC 0.7315 | Recall 0.6833 | Precision 0.6833 | AP 0.5510 | F1 0.6841 | Time 0.08\n",
      "Epoch 0056: Loss 8157.0098 | AUC 0.3271 | Recall 0.2128 | Precision 0.2128 | AP 0.3333 | F1 0.2809 | Time 0.08\n",
      "Epoch 0057: Loss 8158.2178 | AUC 0.7288 | Recall 0.6803 | Precision 0.6803 | AP 0.5483 | F1 0.6820 | Time 0.07\n",
      "Epoch 0058: Loss 8175.3853 | AUC 0.7267 | Recall 0.6775 | Precision 0.6775 | AP 0.5487 | F1 0.6803 | Time 0.07\n",
      "Epoch 0059: Loss 8195.2686 | AUC 0.7255 | Recall 0.6755 | Precision 0.6755 | AP 0.5511 | F1 0.6790 | Time 0.09\n",
      "Epoch 0060: Loss 8205.3965 | AUC 0.7252 | Recall 0.6748 | Precision 0.6748 | AP 0.5527 | F1 0.6785 | Time 0.08\n",
      "Epoch 0061: Loss 8200.6318 | AUC 0.7253 | Recall 0.6751 | Precision 0.6751 | AP 0.5522 | F1 0.6785 | Time 0.09\n",
      "Epoch 0062: Loss 8184.8125 | AUC 0.7260 | Recall 0.6774 | Precision 0.6774 | AP 0.5498 | F1 0.6799 | Time 0.10\n",
      "Epoch 0063: Loss 8167.1758 | AUC 0.7275 | Recall 0.6785 | Precision 0.6785 | AP 0.5483 | F1 0.6809 | Time 0.09\n",
      "Epoch 0064: Loss 8156.3970 | AUC 0.7292 | Recall 0.6805 | Precision 0.6805 | AP 0.5479 | F1 0.6822 | Time 0.08\n",
      "Epoch 0065: Loss 8155.9995 | AUC 0.3139 | Recall 0.1484 | Precision 0.1484 | AP 0.3173 | F1 0.2201 | Time 0.09\n",
      "Epoch 0066: Loss 8163.4058 | AUC 0.7245 | Recall 0.6840 | Precision 0.6840 | AP 0.5459 | F1 0.6846 | Time 0.09\n",
      "Epoch 0067: Loss 8172.4268 | AUC 0.7317 | Recall 0.6837 | Precision 0.6837 | AP 0.5508 | F1 0.6844 | Time 0.07\n",
      "Epoch 0068: Loss 8177.2085 | AUC 0.7313 | Recall 0.6829 | Precision 0.6829 | AP 0.5509 | F1 0.6839 | Time 0.09\n",
      "Epoch 0069: Loss 8175.2642 | AUC 0.7316 | Recall 0.6835 | Precision 0.6835 | AP 0.5508 | F1 0.6842 | Time 0.07\n",
      "Epoch 0070: Loss 8168.2275 | AUC 0.7320 | Recall 0.6838 | Precision 0.6838 | AP 0.5508 | F1 0.6845 | Time 0.07\n",
      "Epoch 0071: Loss 8160.2842 | AUC 0.5971 | Recall 0.6231 | Precision 0.6231 | AP 0.4926 | F1 0.6442 | Time 0.08\n",
      "Epoch 0072: Loss 8155.4902 | AUC 0.7300 | Recall 0.6838 | Precision 0.6838 | AP 0.5339 | F1 0.6844 | Time 0.11\n",
      "Epoch 0073: Loss 8155.5361 | AUC 0.7302 | Recall 0.6814 | Precision 0.6814 | AP 0.5466 | F1 0.6828 | Time 0.08\n",
      "Epoch 0074: Loss 8159.1465 | AUC 0.7288 | Recall 0.6803 | Precision 0.6803 | AP 0.5485 | F1 0.6820 | Time 0.10\n",
      "Epoch 0075: Loss 8163.2549 | AUC 0.7282 | Recall 0.6794 | Precision 0.6794 | AP 0.5484 | F1 0.6815 | Time 0.09\n",
      "Epoch 0076: Loss 8165.0454 | AUC 0.7279 | Recall 0.6794 | Precision 0.6794 | AP 0.5478 | F1 0.6814 | Time 0.07\n",
      "Epoch 0077: Loss 8163.5640 | AUC 0.7281 | Recall 0.6792 | Precision 0.6792 | AP 0.5483 | F1 0.6814 | Time 0.07\n",
      "Epoch 0078: Loss 8159.9736 | AUC 0.7286 | Recall 0.6799 | Precision 0.6799 | AP 0.5484 | F1 0.6819 | Time 0.08\n",
      "Epoch 0079: Loss 8156.5039 | AUC 0.7297 | Recall 0.6805 | Precision 0.6805 | AP 0.5482 | F1 0.6822 | Time 0.08\n",
      "Epoch 0080: Loss 8154.9434 | AUC 0.7314 | Recall 0.6838 | Precision 0.6838 | AP 0.5444 | F1 0.6845 | Time 0.08\n",
      "Epoch 0081: Loss 8155.6528 | AUC 0.7302 | Recall 0.6838 | Precision 0.6838 | AP 0.5338 | F1 0.6845 | Time 0.08\n",
      "Epoch 0082: Loss 8157.6069 | AUC 0.4806 | Recall 0.3693 | Precision 0.3693 | AP 0.4088 | F1 0.4441 | Time 0.08\n",
      "Epoch 0083: Loss 8159.2378 | AUC 0.6060 | Recall 0.6521 | Precision 0.6521 | AP 0.5000 | F1 0.6636 | Time 0.07\n",
      "Epoch 0084: Loss 8159.4697 | AUC 0.6344 | Recall 0.6844 | Precision 0.6844 | AP 0.5145 | F1 0.6848 | Time 0.08\n",
      "Epoch 0085: Loss 8158.2661 | AUC 0.6327 | Recall 0.4744 | Precision 0.4744 | AP 0.4758 | F1 0.5354 | Time 0.08\n",
      "Epoch 0086: Loss 8156.4780 | AUC 0.3227 | Recall 0.2048 | Precision 0.2048 | AP 0.3312 | F1 0.2728 | Time 0.08\n",
      "Epoch 0087: Loss 8155.1772 | AUC 0.7300 | Recall 0.6838 | Precision 0.6838 | AP 0.5338 | F1 0.6844 | Time 0.07\n",
      "Epoch 0088: Loss 8154.9678 | AUC 0.7314 | Recall 0.6838 | Precision 0.6838 | AP 0.5447 | F1 0.6845 | Time 0.09\n",
      "Epoch 0089: Loss 8155.6870 | AUC 0.7309 | Recall 0.6824 | Precision 0.6824 | AP 0.5473 | F1 0.6835 | Time 0.09\n",
      "Epoch 0090: Loss 8156.6313 | AUC 0.7301 | Recall 0.6809 | Precision 0.6809 | AP 0.5485 | F1 0.6825 | Time 0.10\n",
      "Epoch 0091: Loss 8157.0830 | AUC 0.7297 | Recall 0.6803 | Precision 0.6803 | AP 0.5485 | F1 0.6822 | Time 0.09\n",
      "Epoch 0092: Loss 8156.7690 | AUC 0.7300 | Recall 0.6807 | Precision 0.6807 | AP 0.5485 | F1 0.6824 | Time 0.07\n",
      "Epoch 0093: Loss 8155.9590 | AUC 0.7309 | Recall 0.6822 | Precision 0.6822 | AP 0.5478 | F1 0.6834 | Time 0.07\n",
      "Epoch 0094: Loss 8155.1968 | AUC 0.7317 | Recall 0.6838 | Precision 0.6838 | AP 0.5470 | F1 0.6845 | Time 0.08\n",
      "Epoch 0095: Loss 8154.9043 | AUC 0.7314 | Recall 0.6838 | Precision 0.6838 | AP 0.5443 | F1 0.6844 | Time 0.08\n",
      "Epoch 0096: Loss 8155.1357 | AUC 0.7310 | Recall 0.6838 | Precision 0.6838 | AP 0.5405 | F1 0.6844 | Time 0.11\n",
      "Epoch 0097: Loss 8155.6001 | AUC 0.7302 | Recall 0.6838 | Precision 0.6838 | AP 0.5339 | F1 0.6845 | Time 0.09\n",
      "Epoch 0098: Loss 8155.9111 | AUC 0.7303 | Recall 0.6840 | Precision 0.6840 | AP 0.5339 | F1 0.6846 | Time 0.10\n",
      "Epoch 0099: Loss 8155.8569 | AUC 0.7303 | Recall 0.6840 | Precision 0.6840 | AP 0.5339 | F1 0.6846 | Time 0.08\n",
      "Test: Loss 2.3155 | AUC 0.7714 | Recall 0.6863 | Precision 0.6863 | AP 0.7220 | F1 0.7584 | Time 0.02\n",
      "F1 score:  tensor(0.6863)\n",
      "Precision:  tensor(0.6863)\n",
      "Recall:  tensor(0.6863)\n"
     ]
    }
   ],
   "source": [
    "gae_model = make_gae_model(train_graph_3, train_node_features_3, label_train_3)\n",
    "precision_score, recall_score, f1_score = predict_gae(label_test_3, gae_model, test_graph_3, test_node_features_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(gae_model, open('ml-model/gae_model_default_3.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_model_default_3 = pickle.load(open('ml-model/gae_model_default_3.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 11.6649 | AUC 0.8091 | Recall 0.7160 | Precision 0.7160 | AP 0.7916 | F1 0.7160 | Time 0.07\n",
      "F1 score:  tensor(0.7160)\n",
      "Precision:  tensor(0.7160)\n",
      "Recall:  tensor(0.7160)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = predict_gae(label_test, gae_model_default_3, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnomalyDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_anomalydae(label_test, anomalydae_compile, test_graph, test_node_features):\n",
    "\n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "    \n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    anomalydae_ip_pred_res, anomalydae_ip_score_res = anomalydae_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear', )\n",
    "    \n",
    "    precision_pygod = eval_precision_at_k(label_test, anomalydae_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, anomalydae_ip_score_res)\n",
    "    f1_score = 2*(precision_pygod*recall_pygod)/(precision_pygod+recall_pygod)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    return  precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_anomalydae_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "\n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "    \n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    \n",
    "    anomalydae_model = AnomalyDAE(epoch=100, verbose=3, gpu=0)\n",
    "    anomalydae_compile = anomalydae_model.fit(pyG_train)\n",
    "\n",
    "    return anomalydae_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\1129063167.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 37765.2578 |  | Time 1.12\n",
      "Epoch 0001: Loss 2203407.5000 |  | Time 0.99\n",
      "Epoch 0002: Loss 1124721.3750 |  | Time 0.94\n",
      "Epoch 0003: Loss 254842.3438 |  | Time 0.99\n",
      "Epoch 0004: Loss 124510.8359 |  | Time 0.94\n",
      "Epoch 0005: Loss 98369.0938 |  | Time 0.99\n",
      "Epoch 0006: Loss 25654.1113 |  | Time 0.95\n",
      "Epoch 0007: Loss 106.2671 |  | Time 0.98\n",
      "Epoch 0008: Loss 102.2662 |  | Time 0.95\n",
      "Epoch 0009: Loss 98.2484 |  | Time 0.98\n",
      "Epoch 0010: Loss 97.4040 |  | Time 0.95\n",
      "Epoch 0011: Loss 97.3132 |  | Time 0.98\n",
      "Epoch 0012: Loss 97.5725 |  | Time 0.94\n",
      "Epoch 0013: Loss 98.0501 |  | Time 0.96\n",
      "Epoch 0014: Loss 98.5144 |  | Time 0.93\n",
      "Epoch 0015: Loss 98.9321 |  | Time 0.96\n",
      "Epoch 0016: Loss 99.2841 |  | Time 0.94\n",
      "Epoch 0017: Loss 99.5602 |  | Time 0.97\n",
      "Epoch 0018: Loss 99.7515 |  | Time 0.96\n",
      "Epoch 0019: Loss 99.8564 |  | Time 0.98\n",
      "Epoch 0020: Loss 99.8746 |  | Time 0.93\n",
      "Epoch 0021: Loss 99.8098 |  | Time 0.94\n",
      "Epoch 0022: Loss 99.6724 |  | Time 0.98\n",
      "Epoch 0023: Loss 99.4704 |  | Time 0.98\n",
      "Epoch 0024: Loss 99.2143 |  | Time 0.95\n",
      "Epoch 0025: Loss 98.9181 |  | Time 0.94\n",
      "Epoch 0026: Loss 98.5916 |  | Time 1.16\n",
      "Epoch 0027: Loss 98.2478 |  | Time 1.18\n",
      "Epoch 0028: Loss 97.9038 |  | Time 0.96\n",
      "Epoch 0029: Loss 97.5725 |  | Time 0.96\n",
      "Epoch 0030: Loss 97.3510 |  | Time 0.92\n",
      "Epoch 0031: Loss 97.3049 |  | Time 0.95\n",
      "Epoch 0032: Loss 97.3264 |  | Time 0.91\n",
      "Epoch 0033: Loss 97.5117 |  | Time 0.94\n",
      "Epoch 0034: Loss 97.7409 |  | Time 0.92\n",
      "Epoch 0035: Loss 97.9244 |  | Time 0.92\n",
      "Epoch 0036: Loss 97.9847 |  | Time 0.92\n",
      "Epoch 0037: Loss 97.8934 |  | Time 0.92\n",
      "Epoch 0038: Loss 97.7179 |  | Time 0.91\n",
      "Epoch 0039: Loss 97.5241 |  | Time 0.91\n",
      "Epoch 0040: Loss 97.3547 |  | Time 0.91\n",
      "Epoch 0041: Loss 97.3033 |  | Time 0.92\n",
      "Epoch 0042: Loss 97.2962 |  | Time 0.92\n",
      "Epoch 0043: Loss 97.3135 |  | Time 0.91\n",
      "Epoch 0044: Loss 97.3531 |  | Time 0.91\n",
      "Epoch 0045: Loss 97.4113 |  | Time 0.91\n",
      "Epoch 0046: Loss 97.4756 |  | Time 0.91\n",
      "Epoch 0047: Loss 97.5012 |  | Time 0.91\n",
      "Epoch 0048: Loss 97.4796 |  | Time 0.92\n",
      "Epoch 0049: Loss 97.4229 |  | Time 0.91\n",
      "Epoch 0050: Loss 97.3641 |  | Time 0.92\n",
      "Epoch 0051: Loss 97.3231 |  | Time 0.91\n",
      "Epoch 0052: Loss 97.2975 |  | Time 0.91\n",
      "Epoch 0053: Loss 97.2846 |  | Time 0.92\n",
      "Epoch 0054: Loss 97.2801 |  | Time 0.91\n",
      "Epoch 0055: Loss 97.2838 |  | Time 0.92\n",
      "Epoch 0056: Loss 97.2893 |  | Time 0.91\n",
      "Epoch 0057: Loss 97.2972 |  | Time 0.91\n",
      "Epoch 0058: Loss 97.3120 |  | Time 0.91\n",
      "Epoch 0059: Loss 97.3224 |  | Time 0.91\n",
      "Epoch 0060: Loss 97.3114 |  | Time 0.92\n",
      "Epoch 0061: Loss 97.2940 |  | Time 0.91\n",
      "Epoch 0062: Loss 97.2842 |  | Time 0.91\n",
      "Epoch 0063: Loss 97.2768 |  | Time 0.91\n",
      "Epoch 0064: Loss 97.2710 |  | Time 0.91\n",
      "Epoch 0065: Loss 97.2661 |  | Time 0.91\n",
      "Epoch 0066: Loss 97.2622 |  | Time 0.91\n",
      "Epoch 0067: Loss 97.2608 |  | Time 0.92\n",
      "Epoch 0068: Loss 97.2615 |  | Time 0.92\n",
      "Epoch 0069: Loss 97.2623 |  | Time 0.93\n",
      "Epoch 0070: Loss 97.2629 |  | Time 0.91\n",
      "Epoch 0071: Loss 97.2630 |  | Time 0.91\n",
      "Epoch 0072: Loss 97.2624 |  | Time 0.91\n",
      "Epoch 0073: Loss 97.2610 |  | Time 0.92\n",
      "Epoch 0074: Loss 97.2588 |  | Time 0.91\n",
      "Epoch 0075: Loss 97.2559 |  | Time 0.92\n",
      "Epoch 0076: Loss 97.2527 |  | Time 0.91\n",
      "Epoch 0077: Loss 97.2493 |  | Time 0.91\n",
      "Epoch 0078: Loss 97.2458 |  | Time 0.91\n",
      "Epoch 0079: Loss 97.2424 |  | Time 0.97\n",
      "Epoch 0080: Loss 97.2393 |  | Time 0.91\n",
      "Epoch 0081: Loss 97.2372 |  | Time 1.00\n",
      "Epoch 0082: Loss 97.2360 |  | Time 1.01\n",
      "Epoch 0083: Loss 97.2351 |  | Time 0.93\n",
      "Epoch 0084: Loss 97.2341 |  | Time 0.99\n",
      "Epoch 0085: Loss 97.2329 |  | Time 0.96\n",
      "Epoch 0086: Loss 97.2315 |  | Time 0.93\n",
      "Epoch 0087: Loss 97.2298 |  | Time 0.92\n",
      "Epoch 0088: Loss 97.2279 |  | Time 0.92\n",
      "Epoch 0089: Loss 97.2257 |  | Time 0.92\n",
      "Epoch 0090: Loss 97.2235 |  | Time 0.93\n",
      "Epoch 0091: Loss 97.2210 |  | Time 0.93\n",
      "Epoch 0092: Loss 97.2186 |  | Time 0.93\n",
      "Epoch 0093: Loss 97.2164 |  | Time 0.92\n",
      "Epoch 0094: Loss 97.2142 |  | Time 0.93\n",
      "Epoch 0095: Loss 97.2123 |  | Time 0.94\n",
      "Epoch 0096: Loss 97.2109 |  | Time 0.92\n",
      "Epoch 0097: Loss 97.2094 |  | Time 0.91\n",
      "Epoch 0098: Loss 97.2077 |  | Time 0.92\n",
      "Epoch 0099: Loss 97.2058 |  | Time 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\1129063167.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 1.8639 | AUC 0.8477 | Recall 0.7945 | Precision 0.7945 | AP 0.7930 | F1 0.7945 | Time 0.15\n",
      "F1 score:  tensor(0.7945)\n",
      "Precision:  tensor(0.7945)\n",
      "Recall:  tensor(0.7945)\n"
     ]
    }
   ],
   "source": [
    "anomalydae_model = make_anomalydae_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_anomalydae(label_test, anomalydae_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(anomalydae_model, open('ml-model/anomalydae_model_default_1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalydae_model_default = pickle.load(open('ml-model/anomalydae_model_default_1.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 1.8639 | AUC 0.8477 | Recall 0.7945 | Precision 0.7945 | AP 0.7930 | F1 0.7945 | Time 0.22\n",
      "F1 score:  tensor(0.7945)\n",
      "Precision:  tensor(0.7945)\n",
      "Recall:  tensor(0.7945)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = predict_gae(label_test, anomalydae_model_default, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15473 is out of bounds for dimension 0 with size 15473",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 54\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m anomalydae_model \u001b[39m=\u001b[39m make_anomalydae_model(train_graph_2, train_node_features_2, label_train_2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_anomalydae(label_train_2, anomalydae_model, train_graph_2, train_node_features_2)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 54\u001b[0m in \u001b[0;36mmake_anomalydae_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m pyG_train\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m train_node_features\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m anomalydae_model \u001b[39m=\u001b[39m AnomalyDAE(epoch\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m anomalydae_compile \u001b[39m=\u001b[39m anomalydae_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y104sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m anomalydae_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\anomalydae.py:187\u001b[0m, in \u001b[0;36mAnomalyDAE.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    182\u001b[0m pos_weight_a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meta \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meta)\n\u001b[0;32m    183\u001b[0m pos_weight_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta)\n\u001b[0;32m    185\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    186\u001b[0m                              x_[:batch_size],\n\u001b[1;32m--> 187\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    188\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    189\u001b[0m                              weight,\n\u001b[0;32m    190\u001b[0m                              pos_weight_a,\n\u001b[0;32m    191\u001b[0m                              pos_weight_s)\n\u001b[0;32m    193\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n\u001b[0;32m    195\u001b[0m \u001b[39mreturn\u001b[39;00m loss, score\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15473 is out of bounds for dimension 0 with size 15473"
     ]
    }
   ],
   "source": [
    "anomalydae_model = make_anomalydae_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_anomalydae(label_train_2, anomalydae_model, train_graph_2, train_node_features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15473 is out of bounds for dimension 0 with size 15473",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 56\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m anomalydae_model \u001b[39m=\u001b[39m make_anomalydae_model(train_graph_3, train_node_features_3, label_train_3)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_anomalydae(label_test_3, anomalydae_model, test_graph_3, test_node_features_3)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 56\u001b[0m in \u001b[0;36mmake_anomalydae_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m pyG_train\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m train_node_features\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m anomalydae_model \u001b[39m=\u001b[39m AnomalyDAE(epoch\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m anomalydae_compile \u001b[39m=\u001b[39m anomalydae_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y106sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m anomalydae_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\anomalydae.py:187\u001b[0m, in \u001b[0;36mAnomalyDAE.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    182\u001b[0m pos_weight_a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meta \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meta)\n\u001b[0;32m    183\u001b[0m pos_weight_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta)\n\u001b[0;32m    185\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    186\u001b[0m                              x_[:batch_size],\n\u001b[1;32m--> 187\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    188\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    189\u001b[0m                              weight,\n\u001b[0;32m    190\u001b[0m                              pos_weight_a,\n\u001b[0;32m    191\u001b[0m                              pos_weight_s)\n\u001b[0;32m    193\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n\u001b[0;32m    195\u001b[0m \u001b[39mreturn\u001b[39;00m loss, score\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15473 is out of bounds for dimension 0 with size 15473"
     ]
    }
   ],
   "source": [
    "anomalydae_model = make_anomalydae_model(train_graph_3, train_node_features_3, label_train_3)\n",
    "precision_score, recall_score, f1_score = predict_anomalydae(label_test_3, anomalydae_model, test_graph_3, test_node_features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_conad(label_test, conda_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "    \n",
    "    conad_ip_pred_res, conad_ip_score_res = conda_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear')\n",
    "    precision = eval_precision_at_k(label_test, conad_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, conad_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def make_conad_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    conad_model = CONAD(epoch=100, gpu=0, verbose=3)\n",
    "    conad_compile = conad_model.fit(pyG_train)\n",
    "\n",
    "    return conad_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3495099882.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 291766.5000 |  | Time 2.59\n",
      "Epoch 0001: Loss 133874.0938 |  | Time 2.21\n",
      "Epoch 0002: Loss 56467.2500 |  | Time 2.30\n",
      "Epoch 0003: Loss 26133.0645 |  | Time 2.26\n",
      "Epoch 0004: Loss 19144.1543 |  | Time 2.38\n",
      "Epoch 0005: Loss 20232.7656 |  | Time 3.09\n",
      "Epoch 0006: Loss 21601.3320 |  | Time 2.97\n",
      "Epoch 0007: Loss 20718.3594 |  | Time 2.39\n",
      "Epoch 0008: Loss 17903.6816 |  | Time 2.35\n",
      "Epoch 0009: Loss 14351.8242 |  | Time 2.29\n",
      "Epoch 0010: Loss 11061.7803 |  | Time 2.58\n",
      "Epoch 0011: Loss 8524.6270 |  | Time 2.38\n",
      "Epoch 0012: Loss 6802.6177 |  | Time 2.39\n",
      "Epoch 0013: Loss 5732.8354 |  | Time 2.42\n",
      "Epoch 0014: Loss 5066.7974 |  | Time 2.50\n",
      "Epoch 0015: Loss 4568.9165 |  | Time 2.37\n",
      "Epoch 0016: Loss 4090.6641 |  | Time 2.37\n",
      "Epoch 0017: Loss 3593.7039 |  | Time 2.28\n",
      "Epoch 0018: Loss 3110.5083 |  | Time 2.45\n",
      "Epoch 0019: Loss 2682.5774 |  | Time 2.41\n",
      "Epoch 0020: Loss 2327.7378 |  | Time 2.51\n",
      "Epoch 0021: Loss 2042.2389 |  | Time 2.39\n",
      "Epoch 0022: Loss 1813.1781 |  | Time 2.62\n",
      "Epoch 0023: Loss 1626.5831 |  | Time 2.32\n",
      "Epoch 0024: Loss 1470.9149 |  | Time 2.32\n",
      "Epoch 0025: Loss 1338.3184 |  | Time 2.58\n",
      "Epoch 0026: Loss 1222.9937 |  | Time 2.48\n",
      "Epoch 0027: Loss 1117.3987 |  | Time 2.55\n",
      "Epoch 0028: Loss 1012.6671 |  | Time 2.34\n",
      "Epoch 0029: Loss 905.5457 |  | Time 2.26\n",
      "Epoch 0030: Loss 802.5681 |  | Time 2.28\n",
      "Epoch 0031: Loss 714.0566 |  | Time 2.66\n",
      "Epoch 0032: Loss 644.3005 |  | Time 2.60\n",
      "Epoch 0033: Loss 588.7028 |  | Time 2.44\n",
      "Epoch 0034: Loss 538.6115 |  | Time 2.54\n",
      "Epoch 0035: Loss 487.1689 |  | Time 2.32\n",
      "Epoch 0036: Loss 432.1977 |  | Time 2.46\n",
      "Epoch 0037: Loss 376.3813 |  | Time 2.39\n",
      "Epoch 0038: Loss 325.0846 |  | Time 2.65\n",
      "Epoch 0039: Loss 282.5484 |  | Time 2.41\n",
      "Epoch 0040: Loss 249.5041 |  | Time 2.46\n",
      "Epoch 0041: Loss 224.4067 |  | Time 2.31\n",
      "Epoch 0042: Loss 205.8495 |  | Time 2.46\n",
      "Epoch 0043: Loss 192.6266 |  | Time 2.38\n",
      "Epoch 0044: Loss 182.4462 |  | Time 2.32\n",
      "Epoch 0045: Loss 172.4513 |  | Time 2.54\n",
      "Epoch 0046: Loss 161.1237 |  | Time 2.41\n",
      "Epoch 0047: Loss 148.6645 |  | Time 2.56\n",
      "Epoch 0048: Loss 135.7267 |  | Time 2.48\n",
      "Epoch 0049: Loss 122.5846 |  | Time 2.52\n",
      "Epoch 0050: Loss 109.3731 |  | Time 2.33\n",
      "Epoch 0051: Loss 96.2940 |  | Time 2.26\n",
      "Epoch 0052: Loss 83.6946 |  | Time 2.32\n",
      "Epoch 0053: Loss 72.4378 |  | Time 2.50\n",
      "Epoch 0054: Loss 63.7869 |  | Time 2.60\n",
      "Epoch 0055: Loss 58.3396 |  | Time 2.43\n",
      "Epoch 0056: Loss 55.2108 |  | Time 2.32\n",
      "Epoch 0057: Loss 52.7081 |  | Time 2.29\n",
      "Epoch 0058: Loss 49.6229 |  | Time 2.28\n",
      "Epoch 0059: Loss 45.8004 |  | Time 2.34\n",
      "Epoch 0060: Loss 41.8993 |  | Time 2.45\n",
      "Epoch 0061: Loss 38.7838 |  | Time 2.36\n",
      "Epoch 0062: Loss 36.9041 |  | Time 2.29\n",
      "Epoch 0063: Loss 36.0315 |  | Time 2.29\n",
      "Epoch 0064: Loss 35.6550 |  | Time 2.29\n",
      "Epoch 0065: Loss 35.4241 |  | Time 2.23\n",
      "Epoch 0066: Loss 35.1440 |  | Time 2.51\n",
      "Epoch 0067: Loss 34.5683 |  | Time 2.67\n",
      "Epoch 0068: Loss 33.5204 |  | Time 2.54\n",
      "Epoch 0069: Loss 31.9996 |  | Time 2.78\n",
      "Epoch 0070: Loss 30.0989 |  | Time 2.83\n",
      "Epoch 0071: Loss 27.9450 |  | Time 2.56\n",
      "Epoch 0072: Loss 25.7991 |  | Time 2.49\n",
      "Epoch 0073: Loss 23.9595 |  | Time 2.45\n",
      "Epoch 0074: Loss 22.5519 |  | Time 2.42\n",
      "Epoch 0075: Loss 21.5091 |  | Time 2.36\n",
      "Epoch 0076: Loss 20.7045 |  | Time 2.52\n",
      "Epoch 0077: Loss 20.0329 |  | Time 2.59\n",
      "Epoch 0078: Loss 19.4348 |  | Time 2.65\n",
      "Epoch 0079: Loss 18.9100 |  | Time 2.52\n",
      "Epoch 0080: Loss 18.4813 |  | Time 2.63\n",
      "Epoch 0081: Loss 18.1551 |  | Time 2.97\n",
      "Epoch 0082: Loss 17.9222 |  | Time 2.52\n",
      "Epoch 0083: Loss 17.7892 |  | Time 2.69\n",
      "Epoch 0084: Loss 17.7567 |  | Time 2.48\n",
      "Epoch 0085: Loss 17.7974 |  | Time 2.61\n",
      "Epoch 0086: Loss 17.8713 |  | Time 2.36\n",
      "Epoch 0087: Loss 17.9310 |  | Time 2.35\n",
      "Epoch 0088: Loss 17.9291 |  | Time 2.31\n",
      "Epoch 0089: Loss 17.8275 |  | Time 2.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3495099882.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0047 | AUC 0.7606 | Recall 0.6548 | Precision 0.6548 | AP 0.7528 | F1 0.6548 | Time 0.14\n",
      "F1 score:  tensor(0.6548)\n",
      "Precision:  tensor(0.6548)\n",
      "Recall:  tensor(0.6548)\n"
     ]
    }
   ],
   "source": [
    "conad_model = make_conad_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_conad(label_test, conad_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(conad_model, open('ml-model/conad_model_default_1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "conad_model_default = pickle.load(open('ml-model/conad_model_default_1.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0047 | AUC 0.7606 | Recall 0.6548 | Precision 0.6548 | AP 0.7528 | F1 0.6548 | Time 0.30\n",
      "F1 score:  tensor(0.6548)\n",
      "Precision:  tensor(0.6548)\n",
      "Recall:  tensor(0.6548)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = predict_gae(label_test, conad_model_default, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [15473] at index 0 does not match the shape of the indexed tensor [15480, 1] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 62\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m conad_model \u001b[39m=\u001b[39m make_conad_model(train_graph_2, train_node_features_2, label_train_2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_conad(label_train_2, conad_model, train_graph_2, train_node_features_2)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 62\u001b[0m in \u001b[0;36mmake_conad_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m pyG_train\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m train_node_features\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m conad_model \u001b[39m=\u001b[39m CONAD(epoch\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m conad_compile \u001b[39m=\u001b[39m conad_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y115sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m conad_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\conad.py:197\u001b[0m, in \u001b[0;36mCONAD.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    194\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    196\u001b[0m     x_aug, edge_index_aug, label_aug \u001b[39m=\u001b[39m \\\n\u001b[1;32m--> 197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_augmentation(data)\n\u001b[0;32m    198\u001b[0m     x_aug \u001b[39m=\u001b[39m x_aug\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    199\u001b[0m     edge_index_aug \u001b[39m=\u001b[39m edge_index_aug\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\conad.py:276\u001b[0m, in \u001b[0;36mCONAD._data_augmentation\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    274\u001b[0m dv_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlogical_and(rate \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m prob, prob \u001b[39m<\u001b[39m rate \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m)\n\u001b[0;32m    275\u001b[0m feat_c \u001b[39m=\u001b[39m feat_aug[torch\u001b[39m.\u001b[39mrandperm(batch_size)[:surround]]\n\u001b[1;32m--> 276\u001b[0m ds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcdist(feat_aug[dv_mask], feat_c)\n\u001b[0;32m    277\u001b[0m feat_aug[dv_mask] \u001b[39m=\u001b[39m feat_c[torch\u001b[39m.\u001b[39margmax(ds, \u001b[39m1\u001b[39m)]\n\u001b[0;32m    279\u001b[0m \u001b[39m# disproportionate\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [15473] at index 0 does not match the shape of the indexed tensor [15480, 1] at index 0"
     ]
    }
   ],
   "source": [
    "conad_model = make_conad_model(train_graph_2, train_node_features_2, label_train_2)\n",
    "precision_score, recall_score, f1_score = predict_conad(label_train_2, conad_model, train_graph_2, train_node_features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [15473] at index 0 does not match the shape of the indexed tensor [15480, 1] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 64\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m conad_model \u001b[39m=\u001b[39m make_conad_model(train_graph_3, train_node_features_3, label_train_3)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m precision_score, recall_score, f1_score \u001b[39m=\u001b[39m predict_conad(label_test_3, conad_model, train_graph_3, test_node_features_3)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_hyper_tune.ipynb Cell 64\u001b[0m in \u001b[0;36mmake_conad_model\u001b[1;34m(train_graph, train_node_features, label_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m pyG_train\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m train_node_features\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m conad_model \u001b[39m=\u001b[39m CONAD(epoch\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m conad_compile \u001b[39m=\u001b[39m conad_model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_hyper_tune.ipynb#Y120sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m conad_compile\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:464\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 464\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    465\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\conad.py:197\u001b[0m, in \u001b[0;36mCONAD.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    194\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    196\u001b[0m     x_aug, edge_index_aug, label_aug \u001b[39m=\u001b[39m \\\n\u001b[1;32m--> 197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_augmentation(data)\n\u001b[0;32m    198\u001b[0m     x_aug \u001b[39m=\u001b[39m x_aug\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    199\u001b[0m     edge_index_aug \u001b[39m=\u001b[39m edge_index_aug\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\conad.py:276\u001b[0m, in \u001b[0;36mCONAD._data_augmentation\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    274\u001b[0m dv_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlogical_and(rate \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m prob, prob \u001b[39m<\u001b[39m rate \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m)\n\u001b[0;32m    275\u001b[0m feat_c \u001b[39m=\u001b[39m feat_aug[torch\u001b[39m.\u001b[39mrandperm(batch_size)[:surround]]\n\u001b[1;32m--> 276\u001b[0m ds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcdist(feat_aug[dv_mask], feat_c)\n\u001b[0;32m    277\u001b[0m feat_aug[dv_mask] \u001b[39m=\u001b[39m feat_c[torch\u001b[39m.\u001b[39margmax(ds, \u001b[39m1\u001b[39m)]\n\u001b[0;32m    279\u001b[0m \u001b[39m# disproportionate\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [15473] at index 0 does not match the shape of the indexed tensor [15480, 1] at index 0"
     ]
    }
   ],
   "source": [
    "conad_model = make_conad_model(train_graph_3, train_node_features_3, label_train_3)\n",
    "precision_score, recall_score, f1_score = predict_conad(label_test_3, conad_model, train_graph_3, test_node_features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOMINANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hdominant(label_test, dominant_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "    dominant_ip_pred_res, dominant_ip_score_res = dominant_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True)\n",
    "    \n",
    "    precision_pygod = eval_precision_at_k(label_test, dominant_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, dominant_ip_score_res)\n",
    "    f1_score = 2 * (precision_pygod * recall_pygod) / (precision_pygod + recall_pygod)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    return precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_hdominant_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    dominant_model = DOMINANT(gpu=0, weight=1, weight_decay=3, num_layers=16, hid_dim=16, contamination=0.37, lr=0.001, verbose=3, epoch=100)\n",
    "    dominant_compile = dominant_model.fit(pyG_train)\n",
    "    return dominant_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\529420083.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\529420083.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_train = torch.tensor(label_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 75.0951 |  | Time 1.49\n",
      "Epoch 0001: Loss 75.0937 |  | Time 10.23\n",
      "Epoch 0002: Loss 75.0925 |  | Time 10.23\n",
      "Epoch 0003: Loss 75.0912 |  | Time 10.44\n",
      "Epoch 0004: Loss 75.0900 |  | Time 10.30\n",
      "Epoch 0005: Loss 75.0888 |  | Time 10.26\n",
      "Epoch 0006: Loss 75.0875 |  | Time 10.27\n",
      "Epoch 0007: Loss 75.0863 |  | Time 10.26\n",
      "Epoch 0008: Loss 75.0851 |  | Time 10.23\n",
      "Epoch 0009: Loss 75.0839 |  | Time 10.33\n",
      "Epoch 0010: Loss 75.0828 |  | Time 10.21\n",
      "Epoch 0011: Loss 75.0816 |  | Time 10.24\n",
      "Epoch 0012: Loss 75.0804 |  | Time 10.26\n",
      "Epoch 0013: Loss 75.0793 |  | Time 10.32\n",
      "Epoch 0014: Loss 75.0781 |  | Time 10.25\n",
      "Epoch 0015: Loss 75.0769 |  | Time 10.27\n",
      "Epoch 0016: Loss 75.0757 |  | Time 10.19\n",
      "Epoch 0017: Loss 75.0745 |  | Time 10.31\n",
      "Epoch 0018: Loss 75.0733 |  | Time 10.26\n",
      "Epoch 0019: Loss 75.0721 |  | Time 10.37\n",
      "Epoch 0020: Loss 75.0709 |  | Time 10.19\n",
      "Epoch 0021: Loss 75.0697 |  | Time 10.30\n",
      "Epoch 0022: Loss 75.0686 |  | Time 10.22\n",
      "Epoch 0023: Loss 75.0673 |  | Time 10.28\n",
      "Epoch 0024: Loss 75.0660 |  | Time 10.20\n",
      "Epoch 0025: Loss 75.0647 |  | Time 10.27\n",
      "Epoch 0026: Loss 75.0632 |  | Time 10.20\n",
      "Epoch 0027: Loss 75.0613 |  | Time 10.26\n",
      "Epoch 0028: Loss 75.0591 |  | Time 10.19\n",
      "Epoch 0029: Loss 75.0568 |  | Time 10.27\n",
      "Epoch 0030: Loss 75.0543 |  | Time 10.21\n",
      "Epoch 0031: Loss 75.0518 |  | Time 10.25\n",
      "Epoch 0032: Loss 75.0493 |  | Time 10.21\n",
      "Epoch 0033: Loss 75.0468 |  | Time 10.26\n",
      "Epoch 0034: Loss 75.0443 |  | Time 10.21\n",
      "Epoch 0035: Loss 75.0418 |  | Time 10.26\n",
      "Epoch 0036: Loss 75.0393 |  | Time 10.20\n",
      "Epoch 0037: Loss 75.0368 |  | Time 10.26\n",
      "Epoch 0038: Loss 75.0343 |  | Time 10.20\n",
      "Epoch 0039: Loss 75.0319 |  | Time 10.25\n",
      "Epoch 0040: Loss 75.0296 |  | Time 10.20\n",
      "Epoch 0041: Loss 75.0272 |  | Time 10.25\n",
      "Epoch 0042: Loss 75.0249 |  | Time 10.21\n",
      "Epoch 0043: Loss 75.0226 |  | Time 10.26\n",
      "Epoch 0044: Loss 75.0203 |  | Time 10.20\n",
      "Epoch 0045: Loss 75.0182 |  | Time 10.23\n",
      "Epoch 0046: Loss 75.0161 |  | Time 10.21\n",
      "Epoch 0047: Loss 75.0140 |  | Time 10.26\n",
      "Epoch 0048: Loss 75.0121 |  | Time 10.21\n",
      "Epoch 0049: Loss 75.0103 |  | Time 10.64\n",
      "Epoch 0050: Loss 75.0085 |  | Time 10.16\n",
      "Epoch 0051: Loss 75.0067 |  | Time 10.25\n",
      "Epoch 0052: Loss 75.0049 |  | Time 10.19\n",
      "Epoch 0053: Loss 75.0032 |  | Time 10.26\n",
      "Epoch 0054: Loss 75.0015 |  | Time 10.20\n",
      "Epoch 0055: Loss 74.9999 |  | Time 10.28\n",
      "Epoch 0056: Loss 74.9984 |  | Time 10.20\n",
      "Epoch 0057: Loss 74.9969 |  | Time 10.24\n",
      "Epoch 0058: Loss 74.9955 |  | Time 10.19\n",
      "Epoch 0059: Loss 74.9941 |  | Time 10.26\n",
      "Epoch 0060: Loss 74.9928 |  | Time 10.19\n",
      "Epoch 0061: Loss 74.9915 |  | Time 10.25\n",
      "Epoch 0062: Loss 74.9902 |  | Time 10.19\n",
      "Epoch 0063: Loss 74.9889 |  | Time 10.25\n",
      "Epoch 0064: Loss 74.9877 |  | Time 10.19\n",
      "Epoch 0065: Loss 74.9865 |  | Time 10.25\n",
      "Epoch 0066: Loss 74.9853 |  | Time 10.18\n",
      "Epoch 0067: Loss 74.9842 |  | Time 10.24\n",
      "Epoch 0068: Loss 74.9831 |  | Time 10.20\n",
      "Epoch 0069: Loss 74.9819 |  | Time 10.27\n",
      "Epoch 0070: Loss 74.9808 |  | Time 10.20\n",
      "Epoch 0071: Loss 74.9796 |  | Time 10.28\n",
      "Epoch 0072: Loss 74.9785 |  | Time 10.18\n",
      "Epoch 0073: Loss 74.9774 |  | Time 10.25\n",
      "Epoch 0074: Loss 74.9763 |  | Time 10.23\n",
      "Epoch 0075: Loss 74.9752 |  | Time 10.24\n",
      "Epoch 0076: Loss 74.9742 |  | Time 10.21\n",
      "Epoch 0077: Loss 74.9732 |  | Time 10.23\n",
      "Epoch 0078: Loss 74.9722 |  | Time 10.20\n",
      "Epoch 0079: Loss 74.9712 |  | Time 10.24\n",
      "Epoch 0080: Loss 74.9702 |  | Time 10.20\n",
      "Epoch 0081: Loss 74.9693 |  | Time 10.24\n",
      "Epoch 0082: Loss 74.9684 |  | Time 10.18\n",
      "Epoch 0083: Loss 74.9675 |  | Time 10.27\n",
      "Epoch 0084: Loss 74.9667 |  | Time 10.21\n",
      "Epoch 0085: Loss 74.9659 |  | Time 10.27\n",
      "Epoch 0086: Loss 74.9652 |  | Time 10.19\n",
      "Epoch 0087: Loss 74.9645 |  | Time 10.25\n",
      "Epoch 0088: Loss 74.9638 |  | Time 10.20\n",
      "Epoch 0089: Loss 74.9632 |  | Time 10.25\n",
      "Epoch 0090: Loss 74.9626 |  | Time 10.20\n",
      "Epoch 0091: Loss 74.9619 |  | Time 10.24\n",
      "Epoch 0092: Loss 74.9613 |  | Time 10.20\n",
      "Epoch 0093: Loss 74.9607 |  | Time 10.24\n",
      "Epoch 0094: Loss 74.9601 |  | Time 10.20\n",
      "Epoch 0095: Loss 74.9595 |  | Time 10.23\n",
      "Epoch 0096: Loss 74.9589 |  | Time 10.20\n",
      "Epoch 0097: Loss 74.9584 |  | Time 10.25\n",
      "Epoch 0098: Loss 74.9578 |  | Time 10.20\n",
      "Epoch 0099: Loss 74.9573 |  | Time 10.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\529420083.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0098 | AUC 0.7927 | Recall 0.6709 | Precision 0.6709 | AP 0.7455 | F1 0.6792 | Time 9.18\n",
      "F1 score:  tensor(0.6709)\n",
      "Precision:  tensor(0.6709)\n",
      "Recall:  tensor(0.6709)\n"
     ]
    }
   ],
   "source": [
    "dominant_model = make_hdominant_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_hdominant(label_test, dominant_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(dominant_model, open('ml-model/dominant_model_hyper_1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_model_hyper = pickle.load(open('ml-model/dominant_model_hyper_1.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0098 | AUC 0.7927 | Recall 0.6709 | Precision 0.6709 | AP 0.7455 | F1 0.6792 | Time 0.42\n",
      "F1 score:  tensor(0.6709)\n",
      "Precision:  tensor(0.6709)\n",
      "Recall:  tensor(0.6709)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = predict_gae(label_test, dominant_model_hyper, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hocgnn(label_test, ocgnn_compile, pyG_test, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(pyG_test)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    ocgnn_ip_pred_res, ocgnn_ip_score_res = ocgnn_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "\n",
    "    precision_pygod = eval_precision_at_k(label_test, ocgnn_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, ocgnn_ip_score_res)\n",
    "    f1_score = 2 * (precision_pygod * recall_pygod) / (precision_pygod + recall_pygod)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "\n",
    "    return precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_hocgnn_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    ocgnn_model = OCGNN(hid_dim=6, num_layers=10, weight_decay=1, \n",
    "                    contamination=0.37, lr=0.001, epoch=100, gpu=-1, \n",
    "                    verbose=3)\n",
    "    ocgnn_compile = ocgnn_model.fit(pyG_train)\n",
    "    return ocgnn_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3159774169.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 1.4814 |  | Time 0.22\n",
      "Epoch 0001: Loss 0.8971 |  | Time 0.18\n",
      "Epoch 0002: Loss 0.7936 |  | Time 0.18\n",
      "Epoch 0003: Loss 0.7004 |  | Time 0.15\n",
      "Epoch 0004: Loss 0.6194 |  | Time 0.17\n",
      "Epoch 0005: Loss 0.5492 |  | Time 0.14\n",
      "Epoch 0006: Loss 0.4888 |  | Time 0.15\n",
      "Epoch 0007: Loss 0.4370 |  | Time 0.14\n",
      "Epoch 0008: Loss 0.3929 |  | Time 0.15\n",
      "Epoch 0009: Loss 0.3557 |  | Time 0.14\n",
      "Epoch 0010: Loss 0.3245 |  | Time 0.14\n",
      "Epoch 0011: Loss 0.2985 |  | Time 0.14\n",
      "Epoch 0012: Loss 0.2765 |  | Time 0.14\n",
      "Epoch 0013: Loss 0.2577 |  | Time 0.14\n",
      "Epoch 0014: Loss 0.2457 |  | Time 0.16\n",
      "Epoch 0015: Loss 0.2354 |  | Time 0.13\n",
      "Epoch 0016: Loss 0.2253 |  | Time 0.14\n",
      "Epoch 0017: Loss 0.2157 |  | Time 0.15\n",
      "Epoch 0018: Loss 0.2064 |  | Time 0.14\n",
      "Epoch 0019: Loss 0.1976 |  | Time 0.13\n",
      "Epoch 0020: Loss 0.1892 |  | Time 0.14\n",
      "Epoch 0021: Loss 0.1812 |  | Time 0.14\n",
      "Epoch 0022: Loss 0.1735 |  | Time 0.15\n",
      "Epoch 0023: Loss 0.1663 |  | Time 0.17\n",
      "Epoch 0024: Loss 0.1596 |  | Time 0.16\n",
      "Epoch 0025: Loss 0.1533 |  | Time 0.17\n",
      "Epoch 0026: Loss 0.1475 |  | Time 0.18\n",
      "Epoch 0027: Loss 0.1421 |  | Time 0.17\n",
      "Epoch 0028: Loss 0.1372 |  | Time 0.19\n",
      "Epoch 0029: Loss 0.1326 |  | Time 0.15\n",
      "Epoch 0030: Loss 0.1284 |  | Time 0.15\n",
      "Epoch 0031: Loss 0.1246 |  | Time 0.17\n",
      "Epoch 0032: Loss 0.1212 |  | Time 0.15\n",
      "Epoch 0033: Loss 0.1184 |  | Time 0.15\n",
      "Epoch 0034: Loss 0.1164 |  | Time 0.17\n",
      "Epoch 0035: Loss 0.1148 |  | Time 0.14\n",
      "Epoch 0036: Loss 0.1136 |  | Time 0.13\n",
      "Epoch 0037: Loss 0.1123 |  | Time 0.11\n",
      "Epoch 0038: Loss 0.1112 |  | Time 0.14\n",
      "Epoch 0039: Loss 0.1101 |  | Time 0.14\n",
      "Epoch 0040: Loss 0.1090 |  | Time 0.13\n",
      "Epoch 0041: Loss 0.1080 |  | Time 0.15\n",
      "Epoch 0042: Loss 0.1071 |  | Time 0.17\n",
      "Epoch 0043: Loss 0.1061 |  | Time 0.15\n",
      "Epoch 0044: Loss 0.1053 |  | Time 0.16\n",
      "Epoch 0045: Loss 0.1044 |  | Time 0.18\n",
      "Epoch 0046: Loss 0.1036 |  | Time 0.18\n",
      "Epoch 0047: Loss 0.1029 |  | Time 0.17\n",
      "Epoch 0048: Loss 0.1022 |  | Time 0.17\n",
      "Epoch 0049: Loss 0.1015 |  | Time 0.16\n",
      "Epoch 0050: Loss 0.1008 |  | Time 0.18\n",
      "Epoch 0051: Loss 0.1002 |  | Time 0.12\n",
      "Epoch 0052: Loss 0.0997 |  | Time 0.14\n",
      "Epoch 0053: Loss 0.0991 |  | Time 0.11\n",
      "Epoch 0054: Loss 0.0987 |  | Time 0.13\n",
      "Epoch 0055: Loss 0.0982 |  | Time 0.17\n",
      "Epoch 0056: Loss 0.0978 |  | Time 0.18\n",
      "Epoch 0057: Loss 0.0973 |  | Time 0.13\n",
      "Epoch 0058: Loss 0.0970 |  | Time 0.14\n",
      "Epoch 0059: Loss 0.0966 |  | Time 0.16\n",
      "Epoch 0060: Loss 0.0963 |  | Time 0.18\n",
      "Epoch 0061: Loss 0.0960 |  | Time 0.17\n",
      "Epoch 0062: Loss 0.0957 |  | Time 0.13\n",
      "Epoch 0063: Loss 0.0954 |  | Time 0.14\n",
      "Epoch 0064: Loss 0.0951 |  | Time 0.16\n",
      "Epoch 0065: Loss 0.0949 |  | Time 0.11\n",
      "Epoch 0066: Loss 0.0947 |  | Time 0.26\n",
      "Epoch 0067: Loss 0.0945 |  | Time 0.17\n",
      "Epoch 0068: Loss 0.0944 |  | Time 0.15\n",
      "Epoch 0069: Loss 0.0943 |  | Time 0.13\n",
      "Epoch 0070: Loss 0.0942 |  | Time 0.14\n",
      "Epoch 0071: Loss 0.0941 |  | Time 0.20\n",
      "Epoch 0072: Loss 0.0941 |  | Time 0.17\n",
      "Epoch 0073: Loss 0.0940 |  | Time 0.15\n",
      "Epoch 0074: Loss 0.0940 |  | Time 0.14\n",
      "Epoch 0075: Loss 0.0938 |  | Time 0.14\n",
      "Epoch 0076: Loss 0.0937 |  | Time 0.20\n",
      "Epoch 0077: Loss 0.0935 |  | Time 0.17\n",
      "Epoch 0078: Loss 0.0933 |  | Time 0.15\n",
      "Epoch 0079: Loss 0.0932 |  | Time 0.17\n",
      "Epoch 0080: Loss 0.0931 |  | Time 0.27\n",
      "Epoch 0081: Loss 0.0930 |  | Time 0.14\n",
      "Epoch 0082: Loss 0.0929 |  | Time 0.21\n",
      "Epoch 0083: Loss 0.0928 |  | Time 0.15\n",
      "Epoch 0084: Loss 0.0928 |  | Time 0.15\n",
      "Epoch 0085: Loss 0.0927 |  | Time 0.16\n",
      "Epoch 0086: Loss 0.0927 |  | Time 0.21\n",
      "Epoch 0087: Loss 0.0926 |  | Time 0.18\n",
      "Epoch 0088: Loss 0.0926 |  | Time 0.19\n",
      "Epoch 0089: Loss 0.0925 |  | Time 0.19\n",
      "Epoch 0090: Loss 0.0925 |  | Time 0.19\n",
      "Epoch 0091: Loss 0.0925 |  | Time 0.14\n",
      "Epoch 0092: Loss 0.0924 |  | Time 0.19\n",
      "Epoch 0093: Loss 0.0924 |  | Time 0.27\n",
      "Epoch 0094: Loss 0.0924 |  | Time 0.18\n",
      "Epoch 0095: Loss 0.0924 |  | Time 0.18\n",
      "Epoch 0096: Loss 0.0924 |  | Time 0.21\n",
      "Epoch 0097: Loss 0.0924 |  | Time 0.19\n",
      "Epoch 0098: Loss 0.0924 |  | Time 0.15\n",
      "Epoch 0099: Loss 0.0923 |  | Time 0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3159774169.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0000 | AUC 0.0608 | Recall 0.0838 | Precision 0.0838 | AP 0.2798 | F1 0.0838 | Time 0.04\n",
      "F1 score:  tensor(0.0838)\n",
      "Precision:  tensor(0.0838)\n",
      "Recall:  tensor(0.0838)\n"
     ]
    }
   ],
   "source": [
    "ocgnn_model = make_hocgnn_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_hocgnn(label_test, ocgnn_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(ocgnn_model, open('ml-model/ocgnn_model_hyper_1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocgnn_model_hyper = pickle.load(open('ml-model/ocgnn_model_hyper_1.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0000 | AUC 0.4643 | Recall 0.6136 | Precision 0.6136 | AP 0.4504 | F1 0.3123 | Time 0.03\n",
      "F1 score:  tensor(0.6136)\n",
      "Precision:  tensor(0.6136)\n",
      "Recall:  tensor(0.6136)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = predict_gae(label_test, ocgnn_model_hyper, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hgae(label_test, gae_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    gae_ip_pred_res, gae_ip_score_res = gae_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "    precision = eval_precision_at_k(label_test, gae_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, gae_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "    \n",
    "def make_hgae_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "    label_train = label_train\n",
    "\n",
    "    gae_model = GAE(hid_dim=12, num_layers=12, weight_decay=3,\n",
    "                contamination=0.37, lr=0.001, epoch=100, gpu=-1,\n",
    "                verbose=3, recon_s=True, sigmoid_s=True)\n",
    "    gae_compile = gae_model.fit(pyG_train, label_train)\n",
    "    return gae_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\53983549.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 0.8492 | AUC 0.9597 | Recall 0.8432 | Precision 0.8432 | AP 0.9354 | F1 0.8432 | Time 5.14\n",
      "Epoch 0001: Loss 0.8207 | AUC 0.9597 | Recall 0.8432 | Precision 0.8432 | AP 0.9354 | F1 0.8432 | Time 4.50\n",
      "Epoch 0002: Loss 0.7904 | AUC 0.9597 | Recall 0.8432 | Precision 0.8432 | AP 0.9354 | F1 0.8432 | Time 5.59\n",
      "Epoch 0003: Loss 0.7587 | AUC 0.9598 | Recall 0.8432 | Precision 0.8432 | AP 0.9355 | F1 0.8432 | Time 3.83\n",
      "Epoch 0004: Loss 0.7261 | AUC 0.9598 | Recall 0.8433 | Precision 0.8433 | AP 0.9355 | F1 0.8433 | Time 4.17\n",
      "Epoch 0005: Loss 0.6929 | AUC 0.9598 | Recall 0.8433 | Precision 0.8433 | AP 0.9355 | F1 0.8433 | Time 3.84\n",
      "Epoch 0006: Loss 0.6596 | AUC 0.9598 | Recall 0.8433 | Precision 0.8433 | AP 0.9356 | F1 0.8433 | Time 3.78\n",
      "Epoch 0007: Loss 0.6267 | AUC 0.9598 | Recall 0.8433 | Precision 0.8433 | AP 0.9356 | F1 0.8433 | Time 3.86\n",
      "Epoch 0008: Loss 0.5945 | AUC 0.9598 | Recall 0.8433 | Precision 0.8433 | AP 0.9356 | F1 0.8433 | Time 3.85\n",
      "Epoch 0009: Loss 0.5633 | AUC 0.9599 | Recall 0.8432 | Precision 0.8432 | AP 0.9356 | F1 0.8432 | Time 3.86\n",
      "Epoch 0010: Loss 0.5333 | AUC 0.9599 | Recall 0.8432 | Precision 0.8432 | AP 0.9357 | F1 0.8432 | Time 3.68\n",
      "Epoch 0011: Loss 0.5046 | AUC 0.9599 | Recall 0.8432 | Precision 0.8432 | AP 0.9357 | F1 0.8432 | Time 3.69\n",
      "Epoch 0012: Loss 0.4774 | AUC 0.9599 | Recall 0.8433 | Precision 0.8433 | AP 0.9357 | F1 0.8433 | Time 3.75\n",
      "Epoch 0013: Loss 0.4517 | AUC 0.9599 | Recall 0.8433 | Precision 0.8433 | AP 0.9358 | F1 0.8433 | Time 3.88\n",
      "Epoch 0014: Loss 0.4276 | AUC 0.9600 | Recall 0.8433 | Precision 0.8433 | AP 0.9358 | F1 0.8433 | Time 3.84\n",
      "Epoch 0015: Loss 0.4053 | AUC 0.9600 | Recall 0.8437 | Precision 0.8437 | AP 0.9359 | F1 0.8437 | Time 3.79\n",
      "Epoch 0016: Loss 0.3849 | AUC 0.9600 | Recall 0.8437 | Precision 0.8437 | AP 0.9359 | F1 0.8437 | Time 3.77\n",
      "Epoch 0017: Loss 0.3667 | AUC 0.9600 | Recall 0.8439 | Precision 0.8439 | AP 0.9359 | F1 0.8439 | Time 4.39\n",
      "Epoch 0018: Loss 0.3504 | AUC 0.9600 | Recall 0.8439 | Precision 0.8439 | AP 0.9359 | F1 0.8439 | Time 4.26\n",
      "Epoch 0019: Loss 0.3361 | AUC 0.9600 | Recall 0.8437 | Precision 0.8437 | AP 0.9359 | F1 0.8437 | Time 4.39\n",
      "Epoch 0020: Loss 0.3237 | AUC 0.9601 | Recall 0.8437 | Precision 0.8437 | AP 0.9360 | F1 0.8437 | Time 4.36\n",
      "Epoch 0021: Loss 0.3131 | AUC 0.9602 | Recall 0.8441 | Precision 0.8441 | AP 0.9361 | F1 0.8441 | Time 3.96\n",
      "Epoch 0022: Loss 0.3042 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.20\n",
      "Epoch 0023: Loss 0.2967 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9362 | F1 0.8443 | Time 4.18\n",
      "Epoch 0024: Loss 0.2904 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9362 | F1 0.8443 | Time 4.04\n",
      "Epoch 0025: Loss 0.2851 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9362 | F1 0.8443 | Time 3.98\n",
      "Epoch 0026: Loss 0.2807 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.06\n",
      "Epoch 0027: Loss 0.2770 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.01\n",
      "Epoch 0028: Loss 0.2738 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.08\n",
      "Epoch 0029: Loss 0.2712 | AUC 0.9601 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.68\n",
      "Epoch 0030: Loss 0.2690 | AUC 0.9601 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 13.71\n",
      "Epoch 0031: Loss 0.2672 | AUC 0.9601 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 7.39\n",
      "Epoch 0032: Loss 0.2656 | AUC 0.9601 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 8.88\n",
      "Epoch 0033: Loss 0.2642 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 6.97\n",
      "Epoch 0034: Loss 0.2631 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 7.12\n",
      "Epoch 0035: Loss 0.2621 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 6.09\n",
      "Epoch 0036: Loss 0.2613 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 5.91\n",
      "Epoch 0037: Loss 0.2606 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 4.91\n",
      "Epoch 0038: Loss 0.2600 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 4.18\n",
      "Epoch 0039: Loss 0.2594 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 4.22\n",
      "Epoch 0040: Loss 0.2589 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 4.17\n",
      "Epoch 0041: Loss 0.2585 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 4.41\n",
      "Epoch 0042: Loss 0.2581 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 4.03\n",
      "Epoch 0043: Loss 0.2578 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 4.21\n",
      "Epoch 0044: Loss 0.2575 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 4.28\n",
      "Epoch 0045: Loss 0.2573 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 4.22\n",
      "Epoch 0046: Loss 0.2570 | AUC 0.9600 | Recall 0.8446 | Precision 0.8446 | AP 0.9361 | F1 0.8446 | Time 4.03\n",
      "Epoch 0047: Loss 0.2568 | AUC 0.9600 | Recall 0.8448 | Precision 0.8448 | AP 0.9361 | F1 0.8448 | Time 4.10\n",
      "Epoch 0048: Loss 0.2566 | AUC 0.9600 | Recall 0.8448 | Precision 0.8448 | AP 0.9361 | F1 0.8448 | Time 4.13\n",
      "Epoch 0049: Loss 0.2564 | AUC 0.9600 | Recall 0.8448 | Precision 0.8448 | AP 0.9362 | F1 0.8448 | Time 4.34\n",
      "Epoch 0050: Loss 0.2562 | AUC 0.9601 | Recall 0.8448 | Precision 0.8448 | AP 0.9362 | F1 0.8448 | Time 4.23\n",
      "Epoch 0051: Loss 0.2561 | AUC 0.9601 | Recall 0.8448 | Precision 0.8448 | AP 0.9362 | F1 0.8448 | Time 4.09\n",
      "Epoch 0052: Loss 0.2559 | AUC 0.9601 | Recall 0.8448 | Precision 0.8448 | AP 0.9362 | F1 0.8448 | Time 4.09\n",
      "Epoch 0053: Loss 0.2558 | AUC 0.9601 | Recall 0.8448 | Precision 0.8448 | AP 0.9362 | F1 0.8448 | Time 4.05\n",
      "Epoch 0054: Loss 0.2556 | AUC 0.9601 | Recall 0.8448 | Precision 0.8448 | AP 0.9362 | F1 0.8448 | Time 4.17\n",
      "Epoch 0055: Loss 0.2555 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9362 | F1 0.8448 | Time 4.18\n",
      "Epoch 0056: Loss 0.2554 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9363 | F1 0.8445 | Time 4.25\n",
      "Epoch 0057: Loss 0.2553 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9363 | F1 0.8446 | Time 4.24\n",
      "Epoch 0058: Loss 0.2551 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9363 | F1 0.8446 | Time 4.10\n",
      "Epoch 0059: Loss 0.2549 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9363 | F1 0.8446 | Time 4.14\n",
      "Epoch 0060: Loss 0.2547 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9363 | F1 0.8446 | Time 4.10\n",
      "Epoch 0061: Loss 0.2545 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9363 | F1 0.8446 | Time 4.09\n",
      "Epoch 0062: Loss 0.2544 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.06\n",
      "Epoch 0063: Loss 0.2542 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.39\n",
      "Epoch 0064: Loss 0.2540 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.04\n",
      "Epoch 0065: Loss 0.2539 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.11\n",
      "Epoch 0066: Loss 0.2537 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.20\n",
      "Epoch 0067: Loss 0.2536 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 3.95\n",
      "Epoch 0068: Loss 0.2534 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 3.95\n",
      "Epoch 0069: Loss 0.2533 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.19\n",
      "Epoch 0070: Loss 0.2532 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.12\n",
      "Epoch 0071: Loss 0.2530 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.35\n",
      "Epoch 0072: Loss 0.2529 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.11\n",
      "Epoch 0073: Loss 0.2528 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.01\n",
      "Epoch 0074: Loss 0.2527 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.06\n",
      "Epoch 0075: Loss 0.2526 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.04\n",
      "Epoch 0076: Loss 0.2524 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.09\n",
      "Epoch 0077: Loss 0.2523 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.01\n",
      "Epoch 0078: Loss 0.2522 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.04\n",
      "Epoch 0079: Loss 0.2521 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.04\n",
      "Epoch 0080: Loss 0.2520 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 3.84\n",
      "Epoch 0081: Loss 0.2519 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 3.92\n",
      "Epoch 0082: Loss 0.2518 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9362 | F1 0.8442 | Time 3.88\n",
      "Epoch 0083: Loss 0.2517 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 3.92\n",
      "Epoch 0084: Loss 0.2517 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 3.93\n",
      "Epoch 0085: Loss 0.2516 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 3.95\n",
      "Epoch 0086: Loss 0.2515 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.00\n",
      "Epoch 0087: Loss 0.2514 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.04\n",
      "Epoch 0088: Loss 0.2513 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 3.95\n",
      "Epoch 0089: Loss 0.2513 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 3.87\n",
      "Epoch 0090: Loss 0.2512 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 3.99\n",
      "Epoch 0091: Loss 0.2511 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 3.95\n",
      "Epoch 0092: Loss 0.2511 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9361 | F1 0.8443 | Time 3.96\n",
      "Epoch 0093: Loss 0.2510 | AUC 0.9601 | Recall 0.8445 | Precision 0.8445 | AP 0.9361 | F1 0.8445 | Time 4.13\n",
      "Epoch 0094: Loss 0.2510 | AUC 0.9601 | Recall 0.8445 | Precision 0.8445 | AP 0.9361 | F1 0.8445 | Time 3.90\n",
      "Epoch 0095: Loss 0.2509 | AUC 0.9601 | Recall 0.8445 | Precision 0.8445 | AP 0.9360 | F1 0.8445 | Time 3.90\n",
      "Epoch 0096: Loss 0.2508 | AUC 0.9601 | Recall 0.8445 | Precision 0.8445 | AP 0.9360 | F1 0.8445 | Time 4.00\n",
      "Epoch 0097: Loss 0.2508 | AUC 0.9601 | Recall 0.8445 | Precision 0.8445 | AP 0.9360 | F1 0.8445 | Time 3.99\n",
      "Epoch 0098: Loss 0.2508 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9360 | F1 0.8443 | Time 4.01\n",
      "Epoch 0099: Loss 0.2507 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8441 | Time 4.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\53983549.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0000 | AUC 0.9582 | Recall 0.8727 | Precision 0.8727 | AP 0.9498 | F1 0.8727 | Time 0.79\n",
      "F1 score:  tensor(0.8727)\n",
      "Precision:  tensor(0.8727)\n",
      "Recall:  tensor(0.8727)\n"
     ]
    }
   ],
   "source": [
    "gae_model = make_hgae_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_hgae(label_test, gae_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(gae_model, open('ml-model/gae_model_hyper_1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_model_hyper = pickle.load(open('ml-model/gae_model_hyper_1.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0000 | AUC 0.9582 | Recall 0.8727 | Precision 0.8727 | AP 0.9498 | F1 0.8727 | Time 1.76\n",
      "F1 score:  tensor(0.8727)\n",
      "Precision:  tensor(0.8727)\n",
      "Recall:  tensor(0.8727)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = predict_gae(label_test, gae_model_hyper, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnomalyDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hanomalydae(label_test, anomalydae_compile, test_graph, test_node_features):\n",
    "\n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "    \n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    anomalydae_ip_pred_res, anomalydae_ip_score_res = anomalydae_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear', )\n",
    "    \n",
    "    precision_pygod = eval_precision_at_k(label_test, anomalydae_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, anomalydae_ip_score_res)\n",
    "    f1_score = 2*(precision_pygod*recall_pygod)/(precision_pygod+recall_pygod)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    return  precision_pygod, recall_pygod, f1_score\n",
    "\n",
    "def make_hanomalydae_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "\n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "    \n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    \n",
    "    anomalydae_model = AnomalyDAE(hid_dim=12, emb_dim=4, \n",
    "                        lr=0.001, contamination=0.37,\n",
    "                        epoch=100, gpu=0,\n",
    "                        weight=1, verbose=3)\n",
    "    anomalydae_compile = anomalydae_model.fit(pyG_train)\n",
    "\n",
    "    return anomalydae_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\1664125968.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 68.6395 |  | Time 1.05\n",
      "Epoch 0001: Loss 68.3423 |  | Time 1.18\n",
      "Epoch 0002: Loss 64.1930 |  | Time 1.19\n",
      "Epoch 0003: Loss 59.1883 |  | Time 1.14\n",
      "Epoch 0004: Loss 65.4923 |  | Time 1.10\n",
      "Epoch 0005: Loss 64.7353 |  | Time 1.13\n",
      "Epoch 0006: Loss 61.0819 |  | Time 1.07\n",
      "Epoch 0007: Loss 59.4911 |  | Time 1.06\n",
      "Epoch 0008: Loss 60.8083 |  | Time 1.07\n",
      "Epoch 0009: Loss 61.5021 |  | Time 1.05\n",
      "Epoch 0010: Loss 60.7198 |  | Time 1.08\n",
      "Epoch 0011: Loss 59.9525 |  | Time 1.04\n",
      "Epoch 0012: Loss 59.0901 |  | Time 1.05\n",
      "Epoch 0013: Loss 60.2699 |  | Time 1.04\n",
      "Epoch 0014: Loss 60.7941 |  | Time 1.05\n",
      "Epoch 0015: Loss 60.0000 |  | Time 1.05\n",
      "Epoch 0016: Loss 59.0612 |  | Time 1.05\n",
      "Epoch 0017: Loss 59.5197 |  | Time 1.05\n",
      "Epoch 0018: Loss 59.8135 |  | Time 1.06\n",
      "Epoch 0019: Loss 59.7712 |  | Time 1.05\n",
      "Epoch 0020: Loss 59.4333 |  | Time 1.04\n",
      "Epoch 0021: Loss 59.0541 |  | Time 1.04\n",
      "Epoch 0022: Loss 59.6608 |  | Time 1.04\n",
      "Epoch 0023: Loss 59.8672 |  | Time 1.04\n",
      "Epoch 0024: Loss 59.2810 |  | Time 1.05\n",
      "Epoch 0025: Loss 59.2202 |  | Time 1.05\n",
      "Epoch 0026: Loss 59.6446 |  | Time 1.04\n",
      "Epoch 0027: Loss 59.8088 |  | Time 1.06\n",
      "Epoch 0028: Loss 59.6637 |  | Time 1.06\n",
      "Epoch 0029: Loss 59.2861 |  | Time 1.04\n",
      "Epoch 0030: Loss 58.9940 |  | Time 1.04\n",
      "Epoch 0031: Loss 59.8010 |  | Time 1.03\n",
      "Epoch 0032: Loss 59.9267 |  | Time 1.08\n",
      "Epoch 0033: Loss 59.2789 |  | Time 1.04\n",
      "Epoch 0034: Loss 59.2409 |  | Time 1.04\n",
      "Epoch 0035: Loss 59.7064 |  | Time 1.06\n",
      "Epoch 0036: Loss 59.8911 |  | Time 1.05\n",
      "Epoch 0037: Loss 59.7573 |  | Time 1.07\n",
      "Epoch 0038: Loss 59.3563 |  | Time 1.06\n",
      "Epoch 0039: Loss 58.9974 |  | Time 1.04\n",
      "Epoch 0040: Loss 59.9147 |  | Time 1.05\n",
      "Epoch 0041: Loss 60.1678 |  | Time 1.05\n",
      "Epoch 0042: Loss 59.5476 |  | Time 1.05\n",
      "Epoch 0043: Loss 59.1376 |  | Time 1.05\n",
      "Epoch 0044: Loss 59.5520 |  | Time 1.03\n",
      "Epoch 0045: Loss 59.7635 |  | Time 1.04\n",
      "Epoch 0046: Loss 59.6651 |  | Time 1.04\n",
      "Epoch 0047: Loss 59.3150 |  | Time 1.06\n",
      "Epoch 0048: Loss 59.0286 |  | Time 1.06\n",
      "Epoch 0049: Loss 59.5947 |  | Time 1.05\n",
      "Epoch 0050: Loss 59.6933 |  | Time 1.08\n",
      "Epoch 0051: Loss 59.0652 |  | Time 1.13\n",
      "Epoch 0052: Loss 59.3327 |  | Time 1.04\n",
      "Epoch 0053: Loss 59.7428 |  | Time 1.04\n",
      "Epoch 0054: Loss 59.8307 |  | Time 1.04\n",
      "Epoch 0055: Loss 59.6184 |  | Time 1.04\n",
      "Epoch 0056: Loss 59.2149 |  | Time 1.04\n",
      "Epoch 0057: Loss 59.1072 |  | Time 1.03\n",
      "Epoch 0058: Loss 59.3290 |  | Time 1.03\n",
      "Epoch 0059: Loss 58.9802 |  | Time 1.04\n",
      "Epoch 0060: Loss 59.0401 |  | Time 1.02\n",
      "Epoch 0061: Loss 59.0360 |  | Time 1.05\n",
      "Epoch 0062: Loss 58.9754 |  | Time 1.04\n",
      "Epoch 0063: Loss 59.2946 |  | Time 1.03\n",
      "Epoch 0064: Loss 58.9988 |  | Time 1.04\n",
      "Epoch 0065: Loss 59.2496 |  | Time 1.04\n",
      "Epoch 0066: Loss 59.5434 |  | Time 1.04\n",
      "Epoch 0067: Loss 59.5592 |  | Time 1.03\n",
      "Epoch 0068: Loss 59.3010 |  | Time 1.04\n",
      "Epoch 0069: Loss 59.0503 |  | Time 1.03\n",
      "Epoch 0070: Loss 59.4669 |  | Time 1.08\n",
      "Epoch 0071: Loss 59.4928 |  | Time 1.04\n",
      "Epoch 0072: Loss 59.0050 |  | Time 1.03\n",
      "Epoch 0073: Loss 59.1460 |  | Time 1.04\n",
      "Epoch 0074: Loss 59.2012 |  | Time 1.03\n",
      "Epoch 0075: Loss 59.1576 |  | Time 1.04\n",
      "Epoch 0076: Loss 59.0356 |  | Time 1.04\n",
      "Epoch 0077: Loss 59.2378 |  | Time 1.04\n",
      "Epoch 0078: Loss 59.0653 |  | Time 1.05\n",
      "Epoch 0079: Loss 59.1832 |  | Time 1.03\n",
      "Epoch 0080: Loss 59.4042 |  | Time 1.02\n",
      "Epoch 0081: Loss 59.3924 |  | Time 1.03\n",
      "Epoch 0082: Loss 59.1797 |  | Time 1.06\n",
      "Epoch 0083: Loss 58.9684 |  | Time 1.03\n",
      "Epoch 0084: Loss 59.7537 |  | Time 1.05\n",
      "Epoch 0085: Loss 59.6909 |  | Time 1.05\n",
      "Epoch 0086: Loss 58.9882 |  | Time 1.05\n",
      "Epoch 0087: Loss 59.1615 |  | Time 1.04\n",
      "Epoch 0088: Loss 59.2553 |  | Time 1.04\n",
      "Epoch 0089: Loss 59.2172 |  | Time 1.04\n",
      "Epoch 0090: Loss 59.0793 |  | Time 1.04\n",
      "Epoch 0091: Loss 59.1307 |  | Time 1.03\n",
      "Epoch 0092: Loss 59.0021 |  | Time 1.03\n",
      "Epoch 0093: Loss 59.1988 |  | Time 1.04\n",
      "Epoch 0094: Loss 59.4118 |  | Time 1.04\n",
      "Epoch 0095: Loss 59.3762 |  | Time 1.02\n",
      "Epoch 0096: Loss 59.1534 |  | Time 1.05\n",
      "Epoch 0097: Loss 59.0666 |  | Time 1.03\n",
      "Epoch 0098: Loss 59.0576 |  | Time 1.04\n",
      "Epoch 0099: Loss 59.1358 |  | Time 1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\1664125968.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0065 | AUC 0.6965 | Recall 0.6878 | Precision 0.6878 | AP 0.7104 | F1 0.7543 | Time 0.16\n",
      "F1 score:  tensor(0.6878)\n",
      "Precision:  tensor(0.6878)\n",
      "Recall:  tensor(0.6878)\n"
     ]
    }
   ],
   "source": [
    "anomalydae_model = make_hanomalydae_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_hanomalydae(label_test, anomalydae_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(anomalydae_model, open('ml-model/anomalydae_model_hyper_1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalydae_model_hyper = pickle.load(open('ml-model/anomalydae_model_hyper_1.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0065 | AUC 0.6965 | Recall 0.6878 | Precision 0.6878 | AP 0.7104 | F1 0.7543 | Time 0.46\n",
      "F1 score:  tensor(0.6878)\n",
      "Precision:  tensor(0.6878)\n",
      "Recall:  tensor(0.6878)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = predict_gae(label_test, anomalydae_model_hyper, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hconad(label_test, conda_compile, test_graph, test_node_features):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = np.array(label_test)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "    \n",
    "    conad_ip_pred_res, conad_ip_score_res = conda_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear')\n",
    "    precision = eval_precision_at_k(label_test, conad_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, conad_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def make_hconad_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = np.array(label_train)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    conad_model = CONAD(hid_dim=10, num_layers=16, \n",
    "                        lr=0.001, weight_decay= 1, contamination=0.37,\n",
    "                        epoch=100, gpu=0,  \n",
    "                        weight=1, dropout=0.2, verbose=3)\n",
    "    conad_compile = conad_model.fit(pyG_train)\n",
    "\n",
    "    return conad_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\324449283.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 37.6129 |  | Time 2.61\n",
      "Epoch 0001: Loss 37.6106 |  | Time 3.20\n",
      "Epoch 0002: Loss 37.6118 |  | Time 11.55\n",
      "Epoch 0003: Loss 37.6089 |  | Time 11.60\n",
      "Epoch 0004: Loss 37.6086 |  | Time 11.63\n",
      "Epoch 0005: Loss 37.6059 |  | Time 11.55\n",
      "Epoch 0006: Loss 37.6050 |  | Time 11.73\n",
      "Epoch 0007: Loss 37.6045 |  | Time 11.77\n",
      "Epoch 0008: Loss 37.6020 |  | Time 11.60\n",
      "Epoch 0009: Loss 37.6010 |  | Time 11.55\n",
      "Epoch 0010: Loss 37.5995 |  | Time 11.65\n",
      "Epoch 0011: Loss 37.5988 |  | Time 11.59\n",
      "Epoch 0012: Loss 37.5957 |  | Time 11.61\n",
      "Epoch 0013: Loss 37.5949 |  | Time 11.55\n",
      "Epoch 0014: Loss 37.5928 |  | Time 11.60\n",
      "Epoch 0015: Loss 37.5929 |  | Time 11.57\n",
      "Epoch 0016: Loss 37.5918 |  | Time 11.56\n",
      "Epoch 0017: Loss 37.5908 |  | Time 11.67\n",
      "Epoch 0018: Loss 37.5891 |  | Time 11.86\n",
      "Epoch 0019: Loss 37.5871 |  | Time 11.75\n",
      "Epoch 0020: Loss 37.5862 |  | Time 11.59\n",
      "Epoch 0021: Loss 37.5850 |  | Time 12.16\n",
      "Epoch 0022: Loss 37.5862 |  | Time 11.73\n",
      "Epoch 0023: Loss 37.5818 |  | Time 11.73\n",
      "Epoch 0024: Loss 37.5827 |  | Time 11.66\n",
      "Epoch 0025: Loss 37.5814 |  | Time 11.62\n",
      "Epoch 0026: Loss 37.5800 |  | Time 11.63\n",
      "Epoch 0027: Loss 37.5789 |  | Time 11.66\n",
      "Epoch 0028: Loss 37.5772 |  | Time 11.60\n",
      "Epoch 0029: Loss 37.5770 |  | Time 11.55\n",
      "Epoch 0030: Loss 37.5759 |  | Time 11.60\n",
      "Epoch 0031: Loss 37.5760 |  | Time 11.61\n",
      "Epoch 0032: Loss 37.5743 |  | Time 11.69\n",
      "Epoch 0033: Loss 37.5725 |  | Time 11.65\n",
      "Epoch 0034: Loss 37.5707 |  | Time 11.61\n",
      "Epoch 0035: Loss 37.5706 |  | Time 11.59\n",
      "Epoch 0036: Loss 37.5678 |  | Time 11.63\n",
      "Epoch 0037: Loss 37.5684 |  | Time 11.62\n",
      "Epoch 0038: Loss 37.5692 |  | Time 11.61\n",
      "Epoch 0039: Loss 37.5668 |  | Time 11.67\n",
      "Epoch 0040: Loss 37.5664 |  | Time 11.65\n",
      "Epoch 0041: Loss 37.5657 |  | Time 11.68\n",
      "Epoch 0042: Loss 37.5647 |  | Time 11.57\n",
      "Epoch 0043: Loss 37.5641 |  | Time 11.62\n",
      "Epoch 0044: Loss 37.5620 |  | Time 11.68\n",
      "Epoch 0045: Loss 37.5624 |  | Time 11.63\n",
      "Epoch 0046: Loss 37.5613 |  | Time 11.64\n",
      "Epoch 0047: Loss 37.5599 |  | Time 11.79\n",
      "Epoch 0048: Loss 37.5597 |  | Time 11.61\n",
      "Epoch 0049: Loss 37.5590 |  | Time 11.77\n",
      "Epoch 0050: Loss 37.5582 |  | Time 11.66\n",
      "Epoch 0051: Loss 37.5564 |  | Time 11.60\n",
      "Epoch 0052: Loss 37.5569 |  | Time 11.63\n",
      "Epoch 0053: Loss 37.5565 |  | Time 11.74\n",
      "Epoch 0054: Loss 37.5551 |  | Time 11.66\n",
      "Epoch 0055: Loss 37.5550 |  | Time 11.62\n",
      "Epoch 0056: Loss 37.5537 |  | Time 11.58\n",
      "Epoch 0057: Loss 37.5553 |  | Time 11.59\n",
      "Epoch 0058: Loss 37.5525 |  | Time 11.60\n",
      "Epoch 0059: Loss 37.5507 |  | Time 11.58\n",
      "Epoch 0060: Loss 37.5504 |  | Time 11.51\n",
      "Epoch 0061: Loss 37.5504 |  | Time 11.57\n",
      "Epoch 0062: Loss 37.5495 |  | Time 11.58\n",
      "Epoch 0063: Loss 37.5483 |  | Time 11.63\n",
      "Epoch 0064: Loss 37.5481 |  | Time 11.58\n",
      "Epoch 0065: Loss 37.5475 |  | Time 11.67\n",
      "Epoch 0066: Loss 37.5471 |  | Time 11.71\n",
      "Epoch 0067: Loss 37.5449 |  | Time 11.85\n",
      "Epoch 0068: Loss 37.5468 |  | Time 11.72\n",
      "Epoch 0069: Loss 37.5453 |  | Time 11.79\n",
      "Epoch 0070: Loss 37.5444 |  | Time 11.81\n",
      "Epoch 0071: Loss 37.5441 |  | Time 11.66\n",
      "Epoch 0072: Loss 37.5453 |  | Time 11.62\n",
      "Epoch 0073: Loss 37.5432 |  | Time 11.74\n",
      "Epoch 0074: Loss 37.5427 |  | Time 11.70\n",
      "Epoch 0075: Loss 37.5423 |  | Time 11.72\n",
      "Epoch 0076: Loss 37.5419 |  | Time 3.29\n",
      "Epoch 0077: Loss 37.5417 |  | Time 11.58\n",
      "Epoch 0078: Loss 37.5411 |  | Time 11.70\n",
      "Epoch 0079: Loss 37.5408 |  | Time 11.66\n",
      "Epoch 0080: Loss 37.5405 |  | Time 11.66\n",
      "Epoch 0081: Loss 37.5386 |  | Time 11.68\n",
      "Epoch 0082: Loss 37.5391 |  | Time 12.18\n",
      "Epoch 0083: Loss 37.5377 |  | Time 3.51\n",
      "Epoch 0084: Loss 37.5381 |  | Time 3.51\n",
      "Epoch 0085: Loss 37.5380 |  | Time 3.39\n",
      "Epoch 0086: Loss 37.5368 |  | Time 11.74\n",
      "Epoch 0087: Loss 37.5355 |  | Time 11.75\n",
      "Epoch 0088: Loss 37.5351 |  | Time 11.76\n",
      "Epoch 0089: Loss 37.5359 |  | Time 11.61\n",
      "Epoch 0090: Loss 37.5360 |  | Time 11.63\n",
      "Epoch 0091: Loss 37.5344 |  | Time 12.28\n",
      "Epoch 0092: Loss 37.5335 |  | Time 3.67\n",
      "Epoch 0093: Loss 37.5318 |  | Time 11.61\n",
      "Epoch 0094: Loss 37.5322 |  | Time 11.66\n",
      "Epoch 0095: Loss 37.5316 |  | Time 11.61\n",
      "Epoch 0096: Loss 37.5311 |  | Time 3.18\n",
      "Epoch 0097: Loss 37.5311 |  | Time 3.17\n",
      "Epoch 0098: Loss 37.5311 |  | Time 11.67\n",
      "Epoch 0099: Loss 37.5297 |  | Time 11.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\324449283.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0098 | AUC 0.7902 | Recall 0.6653 | Precision 0.6653 | AP 0.7438 | F1 0.6661 | Time 0.76\n",
      "F1 score:  tensor(0.6653)\n",
      "Precision:  tensor(0.6653)\n",
      "Recall:  tensor(0.6653)\n"
     ]
    }
   ],
   "source": [
    "conad_model = make_hconad_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_hconad(label_test, conad_model, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export dominant_model\n",
    "pickle.dump(conad_model, open('ml-model/conad_model_hyper_1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "conad_model_hyper = pickle.load(open('ml-model/conad_model_hyper_1.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_17900\\3201301341.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0098 | AUC 0.7902 | Recall 0.6653 | Precision 0.6653 | AP 0.7438 | F1 0.6661 | Time 0.64\n",
      "F1 score:  tensor(0.6653)\n",
      "Precision:  tensor(0.6653)\n",
      "Recall:  tensor(0.6653)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = predict_gae(label_test, conad_model_hyper, test_graph, test_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

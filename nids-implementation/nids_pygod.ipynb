{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from pygod.detector import DOMINANT, OCGNN, GUIDE, GAE, GAAN, AnomalyDAE, CONAD\n",
    "from pygod.metric import eval_average_precision, eval_roc_auc, eval_f1, eval_precision_at_k, eval_recall_at_k\n",
    "from pygod.generator import gen_contextual_outlier, gen_structural_outlier\n",
    "import pickle\n",
    "import time\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw_labeled_path = \"C:\\\\Users\\\\asus\\\\Documents\\\\nids-pcap-dataset\\\\unsw_parquet_used_dataset\\\\unsw_labeled.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw = pd.read_parquet(unsw_labeled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 125180 entries, 1 to 490022\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count   Dtype   \n",
      "---  ------            --------------   -----   \n",
      " 0   source_ip         125180 non-null  object  \n",
      " 1   destination_ip    125180 non-null  object  \n",
      " 2   source_port       125180 non-null  object  \n",
      " 3   destination_port  125180 non-null  object  \n",
      " 4   info_message      125180 non-null  object  \n",
      " 5   attack_category   15657 non-null   category\n",
      " 6   is_malware        125180 non-null  int64   \n",
      " 7   source_ip_info    125180 non-null  object  \n",
      " 8   source_port_info  125180 non-null  object  \n",
      " 9   dest_ip_info      125180 non-null  object  \n",
      " 10  dest_port_info    125180 non-null  object  \n",
      " 11  count_benign      125180 non-null  int64   \n",
      " 12  count_malware     125180 non-null  int64   \n",
      "dtypes: category(1), int64(3), object(9)\n",
      "memory usage: 12.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_ip</th>\n",
       "      <th>destination_ip</th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>info_message</th>\n",
       "      <th>attack_category</th>\n",
       "      <th>is_malware</th>\n",
       "      <th>source_ip_info</th>\n",
       "      <th>source_port_info</th>\n",
       "      <th>dest_ip_info</th>\n",
       "      <th>dest_port_info</th>\n",
       "      <th>count_benign</th>\n",
       "      <th>count_malware</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175.45.176.1</td>\n",
       "      <td>149.171.126.18</td>\n",
       "      <td>4657</td>\n",
       "      <td>80</td>\n",
       "      <td>GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>175.45.176.1 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>4657 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>149.171.126.18 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>80 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175.45.176.3</td>\n",
       "      <td>149.171.126.18</td>\n",
       "      <td>32473</td>\n",
       "      <td>80</td>\n",
       "      <td>GET /level/15/exec/-/buffers/assigned/dump HTT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>175.45.176.3 GET /level/15/exec/-/buffers/assi...</td>\n",
       "      <td>32473 GET /level/15/exec/-/buffers/assigned/du...</td>\n",
       "      <td>149.171.126.18 GET /level/15/exec/-/buffers/as...</td>\n",
       "      <td>80 GET /level/15/exec/-/buffers/assigned/dump ...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>175.45.176.0</td>\n",
       "      <td>149.171.126.17</td>\n",
       "      <td>49194</td>\n",
       "      <td>80</td>\n",
       "      <td>GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>175.45.176.0 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>49194 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>149.171.126.17 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>80 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source_ip  destination_ip source_port destination_port   \n",
       "index                                                              \n",
       "1      175.45.176.1  149.171.126.18        4657               80  \\\n",
       "2      175.45.176.3  149.171.126.18       32473               80   \n",
       "6      175.45.176.0  149.171.126.17       49194               80   \n",
       "\n",
       "                                            info_message attack_category   \n",
       "index                                                                      \n",
       "1                               GET /oKmwKoVbq HTTP/1.1              NaN  \\\n",
       "2      GET /level/15/exec/-/buffers/assigned/dump HTT...             NaN   \n",
       "6                               GET eLWfxXSPkc HTTP/1.1              NaN   \n",
       "\n",
       "       is_malware                                     source_ip_info   \n",
       "index                                                                  \n",
       "1               0              175.45.176.1 GET /oKmwKoVbq HTTP/1.1   \\\n",
       "2               1  175.45.176.3 GET /level/15/exec/-/buffers/assi...   \n",
       "6               0              175.45.176.0 GET eLWfxXSPkc HTTP/1.1    \n",
       "\n",
       "                                        source_port_info   \n",
       "index                                                      \n",
       "1                          4657 GET /oKmwKoVbq HTTP/1.1   \\\n",
       "2      32473 GET /level/15/exec/-/buffers/assigned/du...   \n",
       "6                         49194 GET eLWfxXSPkc HTTP/1.1    \n",
       "\n",
       "                                            dest_ip_info   \n",
       "index                                                      \n",
       "1                149.171.126.18 GET /oKmwKoVbq HTTP/1.1   \\\n",
       "2      149.171.126.18 GET /level/15/exec/-/buffers/as...   \n",
       "6                149.171.126.17 GET eLWfxXSPkc HTTP/1.1    \n",
       "\n",
       "                                          dest_port_info  count_benign   \n",
       "index                                                                    \n",
       "1                            80 GET /oKmwKoVbq HTTP/1.1              1  \\\n",
       "2      80 GET /level/15/exec/-/buffers/assigned/dump ...             1   \n",
       "6                            80 GET eLWfxXSPkc HTTP/1.1              1   \n",
       "\n",
       "       count_malware  \n",
       "index                 \n",
       "1                  0  \n",
       "2                  7  \n",
       "6                  0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsw.info()\n",
    "unsw.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, test_size=0.3):\n",
    "    train, test = train_test_split(df, test_size=test_size)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_modeling_1(df):\n",
    "    graph = nx.Graph()\n",
    "    node_features = []\n",
    "    labels = []\n",
    "\n",
    "    for source_port_info in df[\"source_port_info\"].unique():\n",
    "        graph.add_node(source_port_info)\n",
    "        info_message = df[df[\"source_port_info\"] == source_port_info][\"info_message\"].iloc[0]\n",
    "        label = df[df[\"source_port_info\"] == source_port_info][\"is_malware\"].iloc[0]\n",
    "        node_features.append([float(len(info_message))])\n",
    "        labels.append(label)\n",
    "        \n",
    "    for (source_ip), group in df.groupby([\"source_ip\"]):\n",
    "        for i in range(len(group) - 1):\n",
    "            from_node = group.iloc[i][\"source_port_info\"]\n",
    "            to_node = group.iloc[i+1][\"source_port_info\"]\n",
    "            if graph.has_edge(from_node, to_node):\n",
    "                graph[from_node][to_node][\"weight\"] += 1\n",
    "            else:\n",
    "                graph.add_edge(from_node, to_node, weight=1)\n",
    "    return graph, node_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_modeling_2(df):\n",
    "    graph = nx.Graph()\n",
    "    node_features = []\n",
    "    labels = []\n",
    "\n",
    "    for info_message in df[\"info_message\"].unique():\n",
    "        graph.add_node(info_message)\n",
    "        node_features.append([float(len(info_message))])\n",
    "    \n",
    "    for (source_ip), group in df.groupby([\"source_ip\"]):\n",
    "        for i in range(len(group) - 1):\n",
    "            from_node = group.iloc[i][\"info_message\"]\n",
    "            to_node = group.iloc[i+1][\"info_message\"]\n",
    "            if graph.has_edge(from_node, to_node):\n",
    "                graph[from_node][to_node][\"weight\"] += 1\n",
    "            else:\n",
    "                graph.add_edge(from_node, to_node, weight=1)\n",
    "    node_features = torch.tensor(node_features)\n",
    "    labels = torch.tensor(labels)\n",
    "    return graph, node_features, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw['source_port'] = unsw.source_port.astype('int32')\n",
    "unsw['destination_port']= unsw.destination_port.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_train_test(unsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    77233\n",
       "1    10393\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.], dtype=float32), array([1653, 5923], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "nD_train_df = train_df.drop_duplicates(subset=['info_message'], keep='first')\n",
    "label_train = nD_train_df['is_malware'].to_numpy()\n",
    "label_train = torch.tensor(label_train, dtype=torch.float)\n",
    "value_counts = np.unique(label_train, return_counts=True)\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "1    5923\n",
       "0    1653\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nD_train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    33041\n",
       "1      863\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.], dtype=float32), array([ 821, 3060], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "nD_test_df = test_df.drop_duplicates(subset=['info_message'], keep='first')\n",
    "nD_test_df.is_malware.value_counts()\n",
    "label_test = nD_test_df['is_malware'].to_numpy()\n",
    "label_test = torch.tensor(label_test, dtype=torch.float)\n",
    "value_counts = np.unique(label_test, return_counts=True)\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph, train_node_features = graph_modeling_2(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7576\n"
     ]
    }
   ],
   "source": [
    "# number of nodes\n",
    "print(train_graph.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph, test_node_features = graph_modeling_2(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOMINANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyG_train = from_networkx(train_graph)\n",
    "pyG_train.x = train_node_features\n",
    "pyG_test = from_networkx(test_graph)\n",
    "pyG_test.x = test_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_model = DOMINANT(gpu=0, weight=0.02, num_layers=64, hid_dim=64, contamination=0.1, lr=0.001, verbose=3, epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 4.8750 | AUC 0.6216 | Recall 0.7873 | Precision 0.7873 | AP 0.8504 | F1 0.7783 | Time 12.57\n",
      "Epoch 0001: Loss 4.8739 | AUC 0.6213 | Recall 0.7874 | Precision 0.7874 | AP 0.8514 | F1 0.7874 | Time 12.25\n",
      "Epoch 0002: Loss 4.8740 | AUC 0.6214 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.20\n",
      "Epoch 0003: Loss 4.8726 | AUC 0.6213 | Recall 0.7874 | Precision 0.7874 | AP 0.8514 | F1 0.7874 | Time 12.20\n",
      "Epoch 0004: Loss 4.8725 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.24\n",
      "Epoch 0005: Loss 4.8718 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.26\n",
      "Epoch 0006: Loss 4.8707 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.07\n",
      "Epoch 0007: Loss 4.8701 | AUC 0.6214 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 11.98\n",
      "Epoch 0008: Loss 4.8685 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.03\n",
      "Epoch 0009: Loss 4.8667 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 11.99\n",
      "Epoch 0010: Loss 4.8638 | AUC 0.6213 | Recall 0.7878 | Precision 0.7878 | AP 0.8514 | F1 0.7878 | Time 12.04\n",
      "Epoch 0011: Loss 4.8576 | AUC 0.6213 | Recall 0.7878 | Precision 0.7878 | AP 0.8514 | F1 0.7878 | Time 12.25\n",
      "Epoch 0012: Loss 4.8428 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.03\n",
      "Epoch 0013: Loss 4.8026 | AUC 0.6212 | Recall 0.7878 | Precision 0.7878 | AP 0.8514 | F1 0.7878 | Time 12.02\n",
      "Epoch 0014: Loss 4.6857 | AUC 0.6213 | Recall 0.7879 | Precision 0.7879 | AP 0.8514 | F1 0.7879 | Time 12.03\n",
      "Epoch 0015: Loss 4.4057 | AUC 0.6289 | Recall 0.7966 | Precision 0.7966 | AP 0.8542 | F1 0.7966 | Time 12.00\n",
      "Epoch 0016: Loss 4.8356 | AUC 0.5488 | Recall 0.7424 | Precision 0.7424 | AP 0.8477 | F1 0.7424 | Time 12.07\n",
      "Epoch 0017: Loss 4.4996 | AUC 0.6608 | Recall 0.8207 | Precision 0.8207 | AP 0.8691 | F1 0.8207 | Time 12.01\n",
      "Epoch 0018: Loss 4.3969 | AUC 0.6336 | Recall 0.7979 | Precision 0.7979 | AP 0.8554 | F1 0.7979 | Time 12.06\n",
      "Epoch 0019: Loss 4.5416 | AUC 0.6215 | Recall 0.7878 | Precision 0.7878 | AP 0.8517 | F1 0.7878 | Time 12.09\n",
      "Epoch 0020: Loss 4.6080 | AUC 0.6213 | Recall 0.7879 | Precision 0.7879 | AP 0.8515 | F1 0.7879 | Time 12.17\n",
      "Epoch 0021: Loss 4.6224 | AUC 0.6213 | Recall 0.7879 | Precision 0.7879 | AP 0.8515 | F1 0.7879 | Time 12.00\n",
      "Epoch 0022: Loss 4.6002 | AUC 0.6214 | Recall 0.7879 | Precision 0.7879 | AP 0.8515 | F1 0.7879 | Time 12.05\n",
      "Epoch 0023: Loss 4.5374 | AUC 0.6216 | Recall 0.7879 | Precision 0.7879 | AP 0.8518 | F1 0.7879 | Time 12.02\n",
      "Epoch 0024: Loss 4.4230 | AUC 0.6257 | Recall 0.7891 | Precision 0.7891 | AP 0.8533 | F1 0.7891 | Time 12.17\n",
      "Epoch 0025: Loss 4.3936 | AUC 0.7189 | Recall 0.8464 | Precision 0.8464 | AP 0.8785 | F1 0.8464 | Time 12.02\n",
      "Epoch 0026: Loss 4.4965 | AUC 0.6617 | Recall 0.8214 | Precision 0.8214 | AP 0.8693 | F1 0.8214 | Time 12.11\n",
      "Epoch 0027: Loss 4.5142 | AUC 0.6507 | Recall 0.8158 | Precision 0.8158 | AP 0.8675 | F1 0.8158 | Time 11.95\n",
      "Epoch 0028: Loss 4.4295 | AUC 0.7014 | Recall 0.8393 | Precision 0.8393 | AP 0.8756 | F1 0.8393 | Time 12.04\n",
      "Epoch 0029: Loss 4.3826 | AUC 0.6710 | Recall 0.8199 | Precision 0.8199 | AP 0.8654 | F1 0.8199 | Time 12.25\n",
      "Epoch 0030: Loss 4.4365 | AUC 0.6252 | Recall 0.7885 | Precision 0.7885 | AP 0.8527 | F1 0.7885 | Time 12.26\n",
      "Epoch 0031: Loss 4.4700 | AUC 0.6220 | Recall 0.7881 | Precision 0.7881 | AP 0.8521 | F1 0.7881 | Time 12.05\n",
      "Epoch 0032: Loss 4.4495 | AUC 0.6225 | Recall 0.7876 | Precision 0.7876 | AP 0.8524 | F1 0.7876 | Time 12.05\n",
      "Epoch 0033: Loss 4.4020 | AUC 0.6326 | Recall 0.7967 | Precision 0.7967 | AP 0.8553 | F1 0.7967 | Time 12.04\n",
      "Epoch 0034: Loss 4.3800 | AUC 0.7122 | Recall 0.8416 | Precision 0.8416 | AP 0.8771 | F1 0.8416 | Time 12.06\n",
      "Epoch 0035: Loss 4.4211 | AUC 0.7054 | Recall 0.8411 | Precision 0.8411 | AP 0.8763 | F1 0.8411 | Time 11.98\n",
      "Epoch 0036: Loss 4.4330 | AUC 0.7001 | Recall 0.8386 | Precision 0.8386 | AP 0.8753 | F1 0.8386 | Time 12.12\n",
      "Epoch 0037: Loss 4.3988 | AUC 0.7175 | Recall 0.8460 | Precision 0.8460 | AP 0.8781 | F1 0.8460 | Time 12.16\n",
      "Epoch 0038: Loss 4.3781 | AUC 0.6786 | Recall 0.8199 | Precision 0.8199 | AP 0.8680 | F1 0.8199 | Time 12.30\n",
      "Epoch 0039: Loss 4.3973 | AUC 0.6321 | Recall 0.7981 | Precision 0.7981 | AP 0.8550 | F1 0.7981 | Time 12.06\n",
      "Epoch 0040: Loss 4.4130 | AUC 0.6267 | Recall 0.7945 | Precision 0.7945 | AP 0.8537 | F1 0.7945 | Time 12.20\n",
      "Epoch 0041: Loss 4.3977 | AUC 0.6318 | Recall 0.7977 | Precision 0.7977 | AP 0.8550 | F1 0.7977 | Time 12.00\n",
      "Epoch 0042: Loss 4.3782 | AUC 0.6657 | Recall 0.8151 | Precision 0.8151 | AP 0.8643 | F1 0.8151 | Time 12.03\n",
      "Epoch 0043: Loss 4.3839 | AUC 0.7172 | Recall 0.8477 | Precision 0.8477 | AP 0.8781 | F1 0.8477 | Time 11.95\n",
      "Epoch 0044: Loss 4.3984 | AUC 0.7163 | Recall 0.8448 | Precision 0.8448 | AP 0.8781 | F1 0.8448 | Time 11.94\n",
      "Epoch 0045: Loss 4.3927 | AUC 0.7181 | Recall 0.8465 | Precision 0.8465 | AP 0.8784 | F1 0.8465 | Time 11.95\n",
      "Epoch 0046: Loss 4.3785 | AUC 0.7079 | Recall 0.8372 | Precision 0.8372 | AP 0.8761 | F1 0.8372 | Time 12.01\n",
      "Epoch 0047: Loss 4.3792 | AUC 0.6583 | Recall 0.8126 | Precision 0.8126 | AP 0.8622 | F1 0.8126 | Time 11.99\n",
      "Epoch 0048: Loss 4.3893 | AUC 0.6379 | Recall 0.7981 | Precision 0.7981 | AP 0.8565 | F1 0.7981 | Time 11.99\n",
      "Epoch 0049: Loss 4.3887 | AUC 0.6384 | Recall 0.7981 | Precision 0.7981 | AP 0.8566 | F1 0.7981 | Time 11.94\n",
      "Epoch 0050: Loss 4.3784 | AUC 0.6593 | Recall 0.8128 | Precision 0.8128 | AP 0.8625 | F1 0.8128 | Time 12.02\n",
      "Epoch 0051: Loss 4.3762 | AUC 0.7021 | Recall 0.8329 | Precision 0.8329 | AP 0.8747 | F1 0.8329 | Time 11.94\n",
      "Epoch 0052: Loss 4.3834 | AUC 0.7176 | Recall 0.8487 | Precision 0.8487 | AP 0.8784 | F1 0.8487 | Time 11.97\n",
      "Epoch 0053: Loss 4.3839 | AUC 0.7183 | Recall 0.8486 | Precision 0.8486 | AP 0.8785 | F1 0.8486 | Time 11.96\n",
      "Epoch 0054: Loss 4.3772 | AUC 0.7092 | Recall 0.8381 | Precision 0.8381 | AP 0.8764 | F1 0.8381 | Time 12.25\n",
      "Epoch 0055: Loss 4.3749 | AUC 0.6772 | Recall 0.8193 | Precision 0.8193 | AP 0.8677 | F1 0.8193 | Time 12.29\n",
      "Epoch 0056: Loss 4.3794 | AUC 0.6532 | Recall 0.8082 | Precision 0.8082 | AP 0.8609 | F1 0.8082 | Time 12.07\n",
      "Epoch 0057: Loss 4.3798 | AUC 0.6506 | Recall 0.8060 | Precision 0.8060 | AP 0.8601 | F1 0.8060 | Time 12.02\n",
      "Epoch 0058: Loss 4.3758 | AUC 0.6650 | Recall 0.8150 | Precision 0.8150 | AP 0.8642 | F1 0.8150 | Time 12.07\n",
      "Epoch 0059: Loss 4.3739 | AUC 0.6967 | Recall 0.8280 | Precision 0.8280 | AP 0.8732 | F1 0.8280 | Time 12.02\n",
      "Epoch 0060: Loss 4.3768 | AUC 0.7119 | Recall 0.8420 | Precision 0.8420 | AP 0.8772 | F1 0.8420 | Time 12.04\n",
      "Epoch 0061: Loss 4.3770 | AUC 0.7131 | Recall 0.8426 | Precision 0.8426 | AP 0.8775 | F1 0.8426 | Time 11.98\n",
      "Epoch 0062: Loss 4.3738 | AUC 0.7026 | Recall 0.8327 | Precision 0.8327 | AP 0.8748 | F1 0.8327 | Time 12.32\n",
      "Epoch 0063: Loss 4.3730 | AUC 0.6794 | Recall 0.8200 | Precision 0.8200 | AP 0.8686 | F1 0.8200 | Time 12.04\n",
      "Epoch 0064: Loss 4.3743 | AUC 0.6642 | Recall 0.8150 | Precision 0.8150 | AP 0.8641 | F1 0.8150 | Time 12.07\n",
      "Epoch 0065: Loss 4.3739 | AUC 0.6648 | Recall 0.8151 | Precision 0.8151 | AP 0.8643 | F1 0.8151 | Time 12.11\n",
      "Epoch 0066: Loss 4.3722 | AUC 0.6809 | Recall 0.8197 | Precision 0.8197 | AP 0.8691 | F1 0.8197 | Time 12.06\n",
      "Epoch 0067: Loss 4.3732 | AUC 0.7001 | Recall 0.8307 | Precision 0.8307 | AP 0.8742 | F1 0.8307 | Time 12.02\n",
      "Epoch 0068: Loss 4.3735 | AUC 0.7080 | Recall 0.8371 | Precision 0.8371 | AP 0.8765 | F1 0.8371 | Time 12.05\n",
      "Epoch 0069: Loss 4.3727 | AUC 0.7058 | Recall 0.8352 | Precision 0.8352 | AP 0.8758 | F1 0.8352 | Time 12.00\n",
      "Epoch 0070: Loss 4.3713 | AUC 0.6933 | Recall 0.8254 | Precision 0.8254 | AP 0.8726 | F1 0.8254 | Time 12.04\n",
      "Epoch 0071: Loss 4.3725 | AUC 0.6767 | Recall 0.8192 | Precision 0.8192 | AP 0.8678 | F1 0.8192 | Time 12.01\n",
      "Epoch 0072: Loss 4.3726 | AUC 0.6702 | Recall 0.8151 | Precision 0.8151 | AP 0.8662 | F1 0.8151 | Time 12.05\n",
      "Epoch 0073: Loss 4.3750 | AUC 0.6722 | Recall 0.8185 | Precision 0.8185 | AP 0.8664 | F1 0.8185 | Time 11.98\n",
      "Epoch 0074: Loss 4.3708 | AUC 0.6852 | Recall 0.8220 | Precision 0.8220 | AP 0.8703 | F1 0.8220 | Time 12.05\n",
      "Epoch 0075: Loss 4.3939 | AUC 0.6965 | Recall 0.8261 | Precision 0.8261 | AP 0.8738 | F1 0.8261 | Time 11.99\n",
      "Epoch 0076: Loss 4.3879 | AUC 0.6785 | Recall 0.8209 | Precision 0.8209 | AP 0.8677 | F1 0.8209 | Time 12.03\n",
      "Epoch 0077: Loss 4.4196 | AUC 0.6687 | Recall 0.8156 | Precision 0.8156 | AP 0.8648 | F1 0.8156 | Time 12.01\n",
      "Epoch 0078: Loss 4.4241 | AUC 0.6782 | Recall 0.8210 | Precision 0.8210 | AP 0.8675 | F1 0.8210 | Time 12.04\n",
      "Epoch 0079: Loss 4.4033 | AUC 0.6964 | Recall 0.8276 | Precision 0.8276 | AP 0.8726 | F1 0.8276 | Time 12.04\n",
      "Epoch 0080: Loss 4.3908 | AUC 0.7070 | Recall 0.8352 | Precision 0.8352 | AP 0.8753 | F1 0.8352 | Time 12.07\n",
      "Epoch 0081: Loss 4.3846 | AUC 0.7063 | Recall 0.8347 | Precision 0.8347 | AP 0.8752 | F1 0.8347 | Time 12.12\n",
      "Epoch 0082: Loss 4.3784 | AUC 0.6966 | Recall 0.8269 | Precision 0.8269 | AP 0.8730 | F1 0.8269 | Time 12.04\n",
      "Epoch 0083: Loss 4.3784 | AUC 0.6802 | Recall 0.8197 | Precision 0.8197 | AP 0.8686 | F1 0.8197 | Time 12.05\n",
      "Epoch 0084: Loss 4.3916 | AUC 0.6720 | Recall 0.8161 | Precision 0.8161 | AP 0.8664 | F1 0.8161 | Time 12.06\n",
      "Epoch 0085: Loss 4.3886 | AUC 0.6723 | Recall 0.8163 | Precision 0.8163 | AP 0.8665 | F1 0.8163 | Time 12.08\n",
      "Epoch 0086: Loss 4.3784 | AUC 0.6803 | Recall 0.8197 | Precision 0.8197 | AP 0.8687 | F1 0.8197 | Time 12.03\n",
      "Epoch 0087: Loss 4.3767 | AUC 0.6938 | Recall 0.8271 | Precision 0.8271 | AP 0.8723 | F1 0.8271 | Time 12.03\n",
      "Epoch 0088: Loss 4.3789 | AUC 0.7025 | Recall 0.8320 | Precision 0.8320 | AP 0.8745 | F1 0.8320 | Time 12.02\n",
      "Epoch 0089: Loss 4.3804 | AUC 0.7026 | Recall 0.8320 | Precision 0.8320 | AP 0.8744 | F1 0.8320 | Time 12.00\n",
      "Epoch 0090: Loss 4.3806 | AUC 0.6949 | Recall 0.8268 | Precision 0.8268 | AP 0.8723 | F1 0.8268 | Time 12.08\n",
      "Epoch 0091: Loss 4.3807 | AUC 0.6855 | Recall 0.8219 | Precision 0.8219 | AP 0.8696 | F1 0.8219 | Time 11.97\n",
      "Epoch 0092: Loss 4.3811 | AUC 0.6759 | Recall 0.8199 | Precision 0.8199 | AP 0.8669 | F1 0.8199 | Time 12.05\n",
      "Epoch 0093: Loss 4.3808 | AUC 0.6771 | Recall 0.8195 | Precision 0.8195 | AP 0.8673 | F1 0.8195 | Time 12.01\n",
      "Epoch 0094: Loss 4.3801 | AUC 0.6857 | Recall 0.8220 | Precision 0.8220 | AP 0.8699 | F1 0.8220 | Time 12.05\n",
      "Epoch 0095: Loss 4.3801 | AUC 0.6931 | Recall 0.8261 | Precision 0.8261 | AP 0.8721 | F1 0.8260 | Time 11.99\n",
      "Epoch 0096: Loss 4.3802 | AUC 0.6980 | Recall 0.8276 | Precision 0.8276 | AP 0.8734 | F1 0.8276 | Time 12.04\n",
      "Epoch 0097: Loss 4.3799 | AUC 0.6950 | Recall 0.8275 | Precision 0.8275 | AP 0.8726 | F1 0.8275 | Time 11.98\n",
      "Epoch 0098: Loss 4.3794 | AUC 0.6890 | Recall 0.8227 | Precision 0.8227 | AP 0.8710 | F1 0.8227 | Time 12.08\n",
      "Epoch 0099: Loss 4.3788 | AUC 0.6818 | Recall 0.8193 | Precision 0.8193 | AP 0.8689 | F1 0.8193 | Time 12.03\n"
     ]
    }
   ],
   "source": [
    "dominant_compile = dominant_model.fit(pyG_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0021 | "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3881, 1582]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res \u001b[39m=\u001b[39m dominant_compile\u001b[39m.\u001b[39;49mpredict(data\u001b[39m=\u001b[39;49mpyG_test, label \u001b[39m=\u001b[39;49m label_test,return_pred\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_prob\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, prob_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m, return_conf\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:605\u001b[0m, in \u001b[0;36mDeepDetector.predict\u001b[1;34m(self, data, label, return_pred, return_score, return_prob, prob_method, return_conf, return_emb)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[39mif\u001b[39;00m return_emb:\n\u001b[0;32m    603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(DeepDetector, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mpredict(data,\n\u001b[0;32m    606\u001b[0m                                            label,\n\u001b[0;32m    607\u001b[0m                                            return_pred,\n\u001b[0;32m    608\u001b[0m                                            return_score,\n\u001b[0;32m    609\u001b[0m                                            return_prob,\n\u001b[0;32m    610\u001b[0m                                            prob_method,\n\u001b[0;32m    611\u001b[0m                                            return_conf)\n\u001b[0;32m    612\u001b[0m \u001b[39mif\u001b[39;00m return_emb:\n\u001b[0;32m    613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(output) \u001b[39m==\u001b[39m \u001b[39mtuple\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:188\u001b[0m, in \u001b[0;36mDetector.predict\u001b[1;34m(self, data, label, return_pred, return_score, return_prob, prob_method, return_conf)\u001b[0m\n\u001b[0;32m    183\u001b[0m     logger(score\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_score_,\n\u001b[0;32m    184\u001b[0m            target\u001b[39m=\u001b[39mlabel,\n\u001b[0;32m    185\u001b[0m            verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    186\u001b[0m            train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(data, label)\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m return_pred:\n\u001b[0;32m    190\u001b[0m     pred \u001b[39m=\u001b[39m (score \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold_)\u001b[39m.\u001b[39mlong()\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:528\u001b[0m, in \u001b[0;36mDeepDetector.decision_function\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb[node_idx[:batch_size]] \u001b[39m=\u001b[39m \\\n\u001b[0;32m    524\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39memb[:batch_size]\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    526\u001b[0m     outlier_score[node_idx[:batch_size]] \u001b[39m=\u001b[39m score\n\u001b[1;32m--> 528\u001b[0m logger(loss\u001b[39m=\u001b[39;49mloss\u001b[39m.\u001b[39;49mitem() \u001b[39m/\u001b[39;49m data\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    529\u001b[0m        score\u001b[39m=\u001b[39;49moutlier_score,\n\u001b[0;32m    530\u001b[0m        target\u001b[39m=\u001b[39;49mlabel,\n\u001b[0;32m    531\u001b[0m        time\u001b[39m=\u001b[39;49mtime\u001b[39m.\u001b[39;49mtime() \u001b[39m-\u001b[39;49m start_time,\n\u001b[0;32m    532\u001b[0m        verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    533\u001b[0m        train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    534\u001b[0m \u001b[39mreturn\u001b[39;00m outlier_score\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\utils\\utility.py:236\u001b[0m, in \u001b[0;36mlogger\u001b[1;34m(epoch, loss, score, target, time, verbose, train, deep)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    235\u001b[0m     \u001b[39mif\u001b[39;00m target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m         auc \u001b[39m=\u001b[39m eval_roc_auc(target, score)\n\u001b[0;32m    237\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAUC \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(auc), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\metric\\metric.py:33\u001b[0m, in \u001b[0;36meval_roc_auc\u001b[1;34m(label, score)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval_roc_auc\u001b[39m(label, score):\n\u001b[0;32m     16\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    ROC-AUC score for binary classification.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m        Average ROC-AUC score across different labels.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     roc_auc \u001b[39m=\u001b[39m roc_auc_score(y_true\u001b[39m=\u001b[39;49mlabel, y_score\u001b[39m=\u001b[39;49mscore)\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m roc_auc\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:572\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    570\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n\u001b[0;32m    571\u001b[0m     y_true \u001b[39m=\u001b[39m label_binarize(y_true, classes\u001b[39m=\u001b[39mlabels)[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    573\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39;49mmax_fpr),\n\u001b[0;32m    574\u001b[0m         y_true,\n\u001b[0;32m    575\u001b[0m         y_score,\n\u001b[0;32m    576\u001b[0m         average,\n\u001b[0;32m    577\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    578\u001b[0m     )\n\u001b[0;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    581\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39mmax_fpr),\n\u001b[0;32m    582\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    586\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m binary_metric(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     78\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:344\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_true)) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not defined in that case.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m     )\n\u001b[1;32m--> 344\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m max_fpr \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m auc(fpr, tpr)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:992\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_curve\u001b[39m(\n\u001b[0;32m    905\u001b[0m     y_true, y_score, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, drop_intermediate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    906\u001b[0m ):\n\u001b[0;32m    907\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \n\u001b[0;32m    909\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[39m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[0;32m    993\u001b[0m         y_true, y_score, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    994\u001b[0m     )\n\u001b[0;32m    996\u001b[0m     \u001b[39m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[39m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     \u001b[39m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     \u001b[39m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m     \u001b[39m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m     \u001b[39mif\u001b[39;00m drop_intermediate \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fps) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:751\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m pos_label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m    749\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[1;32m--> 751\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    752\u001b[0m y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n\u001b[0;32m    753\u001b[0m y_score \u001b[39m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3881, 1582]"
     ]
    }
   ],
   "source": [
    "dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res = dominant_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, return_prob=True, prob_method='linear', return_conf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ip = eval_f1(label_test, dominant_ip_pred_res)\n",
    "print(f1_score_ip)\n",
    "precision = eval_precision_at_k(label_test, dominant_ip_score_res, k=1606)\n",
    "print(precision)\n",
    "recall = eval_recall_at_k(label_test, dominant_ip_score_res, k=1606)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominant(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test):\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    model = DOMINANT(gpu=0, weight=0.02, num_layers=64, hid_dim=64, contamination=0.1, lr=0.001, verbose=3, epoch=100)\n",
    "    dominant_compile = model.fit(pyG_train)\n",
    "    dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res = dominant_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, return_prob=True, prob_method='linear', return_conf=True)\n",
    "    f1_score_ip = eval_f1(label_test, dominant_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, dominant_ip_score_res, k=3881)\n",
    "    recall = eval_recall_at_k(label_test, dominant_ip_score_res, k=3881)\n",
    "    return f1_score_ip, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 4.8750 |  | Time 12.97\n",
      "Epoch 0001: Loss 4.8732 |  | Time 12.00\n",
      "Epoch 0002: Loss 4.8745 |  | Time 12.29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m recall \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     f1_score, precision_score, recall_score \u001b[39m=\u001b[39m dominant(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     f1\u001b[39m.\u001b[39mappend(f1_score)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     precision\u001b[39m.\u001b[39mappend(precision_score)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 23\u001b[0m in \u001b[0;36mdominant\u001b[1;34m(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pyG_test\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m test_node_features\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m DOMINANT(gpu\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weight\u001b[39m=\u001b[39m\u001b[39m0.02\u001b[39m, num_layers\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, hid_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, contamination\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, epoch\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dominant_compile \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res \u001b[39m=\u001b[39m dominant_compile\u001b[39m.\u001b[39mpredict(data\u001b[39m=\u001b[39mpyG_test, label \u001b[39m=\u001b[39m label_test,return_pred\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_prob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prob_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m, return_conf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m f1_score_ip \u001b[39m=\u001b[39m eval_f1(label_test, dominant_ip_pred_res)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:465\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 465\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    466\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\dominant.py:161\u001b[0m, in \u001b[0;36mDOMINANT.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    158\u001b[0m s \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39ms\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    159\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 161\u001b[0m x_, s_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, edge_index)\n\u001b[0;32m    163\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    164\u001b[0m                              x_[:batch_size],\n\u001b[0;32m    165\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    166\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    167\u001b[0m                              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n\u001b[0;32m    169\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\nn\\dominant.py:112\u001b[0m, in \u001b[0;36mDOMINANTBase.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mForward computation.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39m    Reconstructed adjacency matrix.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m# encode feature matrix\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared_encoder(x, edge_index)\n\u001b[0;32m    114\u001b[0m \u001b[39m# reconstruct feature matrix\u001b[39;00m\n\u001b[0;32m    115\u001b[0m x_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattr_decoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb, edge_index)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\nn\\models\\basic_gnn.py:222\u001b[0m, in \u001b[0;36mBasicGNN.forward\u001b[1;34m(self, x, edge_index, edge_weight, edge_attr, num_sampled_nodes_per_hop, num_sampled_edges_per_hop)\u001b[0m\n\u001b[0;32m    219\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs[i](x, edge_index, edge_weight\u001b[39m=\u001b[39medge_weight,\n\u001b[0;32m    220\u001b[0m                       edge_attr\u001b[39m=\u001b[39medge_attr)\n\u001b[0;32m    221\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_edge_weight:\n\u001b[1;32m--> 222\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvs[i](x, edge_index, edge_weight\u001b[39m=\u001b[39;49medge_weight)\n\u001b[0;32m    223\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_edge_attr:\n\u001b[0;32m    224\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs[i](x, edge_index, edge_attr\u001b[39m=\u001b[39medge_attr)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    208\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    211\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[0;32m    212\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow, x\u001b[39m.\u001b[39;49mdtype)\n\u001b[0;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[0;32m    214\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:91\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     88\u001b[0m num_nodes \u001b[39m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 91\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m add_remaining_self_loops(\n\u001b[0;32m     92\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m edge_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     edge_weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), ), dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m     96\u001b[0m                              device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\utils\\loop.py:370\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    366\u001b[0m     loop_attr[edge_index[\u001b[39m0\u001b[39m][inv_mask]] \u001b[39m=\u001b[39m edge_attr[inv_mask]\n\u001b[0;32m    368\u001b[0m     edge_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 370\u001b[0m edge_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_index[:, mask], loop_index], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    371\u001b[0m \u001b[39mreturn\u001b[39;00m edge_index, edge_attr\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f1 = []\n",
    "precision = []\n",
    "recall = []\n",
    "for i in range(3):\n",
    "    f1_score, precision_score, recall_score = dominant(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n",
    "    f1.append(f1_score)\n",
    "    precision.append(precision_score)\n",
    "    recall.append(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_train_test(unsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nD_train_df = train_df.drop_duplicates(subset=['source_port_info'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    10070\n",
       "1     5381\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nD_train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = train_df.is_malware.value_counts()\n",
    "if value_counts[0] > 10398:\n",
    "    benign = train_df[train_df['is_malware'] == 0].sample(n=10398)\n",
    "    malicious = train_df[train_df['is_malware'] == 1].sample(n=5699)\n",
    "    train_df = pd.concat([benign, malicious])\n",
    "    # nD_train_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    10398\n",
       "1     5699\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    33097\n",
       "1     4457\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = test_df.is_malware.value_counts()\n",
    "if value_counts[0] > 5321:\n",
    "    df_to_lower = test_df[test_df['is_malware'] == 0].sample(n=5321)\n",
    "    test_df = pd.concat([test_df[test_df['is_malware'] == 1], df_to_lower])\n",
    "    # nD_test_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    5229\n",
       "1    4296\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nD_test_df = test_df.drop_duplicates(subset=['source_port_info'], keep='first')\n",
    "nD_test_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph, train_node_features, label_train = graph_modeling_1(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph, test_node_features, label_test = graph_modeling_1(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(train_graph, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_graph/train_graph.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(train_node_features, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_graph/train_node_features.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(label_train, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_graph/label_train.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_graph' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(train_graph, open('model_graph/train_graph.pkl', 'wb'))\n",
    "pickle.dump(train_node_features, open('model_graph/train_node_features.pkl', 'wb'))\n",
    "pickle.dump(label_train, open('model_graph/label_train.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_graph, open('model_graph/test_graph.pkl', 'wb'))\n",
    "pickle.dump(test_node_features, open('model_graph/test_node_features.pkl', 'wb'))\n",
    "pickle.dump(label_test, open('model_graph/label_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = pickle.load(open('model_graph/train_graph.pkl', 'rb'))\n",
    "label_train = pickle.load(open('model_graph/label_train.pkl', 'rb'))\n",
    "train_node_features = pickle.load(open('model_graph/train_node_features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph = pickle.load(open('model_graph/test_graph.pkl', 'rb'))\n",
    "label_test = pickle.load(open('model_graph/label_test.pkl', 'rb'))\n",
    "test_node_features = pickle.load(open('model_graph/test_node_features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15451"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1]) tensor([5229, 4296])\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = torch.unique(label_test, return_counts=True)\n",
    "print(unique_values, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9525"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22.],\n",
       "        [34.],\n",
       "        [27.],\n",
       "        ...,\n",
       "        [28.],\n",
       "        [15.],\n",
       "        [28.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch_geometric.nn as pyg_nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOMINANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dominant_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features):\n",
    "    \n",
    "    # train_node_features = torch.tensor(train_node_features)\n",
    "    # label_train = torch.tensor(label_train)\n",
    "    # test_node_features = torch.tensor(test_node_features)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    dominant_model = DOMINANT(gpu=0, weight=0.1, num_layers=8, hid_dim=64, contamination=0.37, lr=0.001, verbose=3, backbone=pyg_nn.EdgeCNN, epoch=100)  \n",
    "    dominant_compile = dominant_model.fit(pyG_train)\n",
    "    return dominant_compile, pyG_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dominant(label_test, dominant_compile, pyG_test):\n",
    "    \n",
    "    dominant_ip_pred_res, dominant_ip_score_res = dominant_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear')\n",
    "    \n",
    "    unique_values, counts = torch.unique(dominant_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "\n",
    "    predictions = dominant_ip_pred_res.numpy()\n",
    "    labels = label_test.numpy()\n",
    "    TP = np.sum((labels == 1) & (predictions == 1))\n",
    "    FN = np.sum((labels == 1) & (predictions == 0))\n",
    "    FP = np.sum((labels == 0) & (predictions == 1))\n",
    "    TN = np.sum((labels == 0) & (predictions == 0))\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    f1_pygod = eval_f1(label_test, dominant_ip_pred_res)\n",
    "    precision_pygod = eval_precision_at_k(label_test, dominant_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, dominant_ip_score_res)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    return precision_pygod, recall_pygod, f1_pygod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 58.7736 |  | Time 0.85\n",
      "Epoch 0001: Loss 54.3093 |  | Time 0.98\n",
      "Epoch 0002: Loss 49.9715 |  | Time 0.97\n",
      "Epoch 0003: Loss 46.1873 |  | Time 0.97\n",
      "Epoch 0004: Loss 42.1571 |  | Time 0.97\n",
      "Epoch 0005: Loss 38.6776 |  | Time 0.97\n",
      "Epoch 0006: Loss 36.1402 |  | Time 0.97\n",
      "Epoch 0007: Loss 32.9313 |  | Time 0.97\n",
      "Epoch 0008: Loss 30.5166 |  | Time 0.97\n",
      "Epoch 0009: Loss 28.3955 |  | Time 1.00\n",
      "Epoch 0010: Loss 26.4588 |  | Time 0.98\n",
      "Epoch 0011: Loss 24.6071 |  | Time 0.98\n",
      "Epoch 0012: Loss 22.7349 |  | Time 0.96\n",
      "Epoch 0013: Loss 21.0353 |  | Time 0.97\n",
      "Epoch 0014: Loss 19.4021 |  | Time 0.96\n",
      "Epoch 0015: Loss 17.8645 |  | Time 0.98\n",
      "Epoch 0016: Loss 16.4100 |  | Time 0.96\n",
      "Epoch 0017: Loss 15.0537 |  | Time 0.97\n",
      "Epoch 0018: Loss 13.8795 |  | Time 0.98\n",
      "Epoch 0019: Loss 12.8639 |  | Time 0.97\n",
      "Epoch 0020: Loss 12.0131 |  | Time 0.98\n",
      "Epoch 0021: Loss 11.3278 |  | Time 0.97\n",
      "Epoch 0022: Loss 10.7669 |  | Time 0.97\n",
      "Epoch 0023: Loss 10.3089 |  | Time 0.98\n",
      "Epoch 0024: Loss 9.9141 |  | Time 0.98\n",
      "Epoch 0025: Loss 9.5678 |  | Time 0.97\n",
      "Epoch 0026: Loss 9.2666 |  | Time 0.97\n",
      "Epoch 0027: Loss 9.0102 |  | Time 0.97\n",
      "Epoch 0028: Loss 8.8044 |  | Time 0.98\n",
      "Epoch 0029: Loss 8.6475 |  | Time 0.97\n",
      "Epoch 0030: Loss 8.5263 |  | Time 0.98\n",
      "Epoch 0031: Loss 8.4205 |  | Time 0.98\n",
      "Epoch 0032: Loss 8.3049 |  | Time 0.98\n",
      "Epoch 0033: Loss 8.1614 |  | Time 0.97\n",
      "Epoch 0034: Loss 7.9661 |  | Time 0.98\n",
      "Epoch 0035: Loss 7.7620 |  | Time 0.97\n",
      "Epoch 0036: Loss 7.5749 |  | Time 0.97\n",
      "Epoch 0037: Loss 7.3103 |  | Time 0.96\n",
      "Epoch 0038: Loss 6.8921 |  | Time 0.98\n",
      "Epoch 0039: Loss 6.4104 |  | Time 0.97\n",
      "Epoch 0040: Loss 6.3536 |  | Time 0.97\n",
      "Epoch 0041: Loss 5.7962 |  | Time 0.97\n",
      "Epoch 0042: Loss 5.3728 |  | Time 0.97\n",
      "Epoch 0043: Loss 4.4521 |  | Time 0.98\n",
      "Epoch 0044: Loss 4.5474 |  | Time 0.97\n",
      "Epoch 0045: Loss 3.9222 |  | Time 0.97\n",
      "Epoch 0046: Loss 3.6690 |  | Time 0.98\n",
      "Epoch 0047: Loss 4.4023 |  | Time 0.97\n",
      "Epoch 0048: Loss 3.5088 |  | Time 0.98\n",
      "Epoch 0049: Loss 3.4656 |  | Time 0.98\n",
      "Epoch 0050: Loss 3.5992 |  | Time 0.96\n",
      "Epoch 0051: Loss 3.8524 |  | Time 0.97\n",
      "Epoch 0052: Loss 3.5183 |  | Time 0.98\n",
      "Epoch 0053: Loss 3.7185 |  | Time 0.98\n",
      "Epoch 0054: Loss 3.6021 |  | Time 0.98\n",
      "Epoch 0055: Loss 3.4217 |  | Time 0.97\n",
      "Epoch 0056: Loss 3.3680 |  | Time 0.97\n",
      "Epoch 0057: Loss 3.2606 |  | Time 0.97\n",
      "Epoch 0058: Loss 3.1455 |  | Time 0.97\n",
      "Epoch 0059: Loss 3.4196 |  | Time 0.98\n",
      "Epoch 0060: Loss 3.4615 |  | Time 0.96\n",
      "Epoch 0061: Loss 2.9976 |  | Time 0.97\n",
      "Epoch 0062: Loss 3.6564 |  | Time 0.97\n",
      "Epoch 0063: Loss 3.4606 |  | Time 0.98\n",
      "Epoch 0064: Loss 3.0589 |  | Time 0.98\n",
      "Epoch 0065: Loss 3.2678 |  | Time 0.97\n",
      "Epoch 0066: Loss 2.9588 |  | Time 0.97\n",
      "Epoch 0067: Loss 3.2905 |  | Time 0.98\n",
      "Epoch 0068: Loss 3.2176 |  | Time 0.98\n",
      "Epoch 0069: Loss 2.9453 |  | Time 0.97\n",
      "Epoch 0070: Loss 3.0302 |  | Time 0.97\n",
      "Epoch 0071: Loss 2.9628 |  | Time 0.98\n",
      "Epoch 0072: Loss 2.8944 |  | Time 0.98\n",
      "Epoch 0073: Loss 3.0596 |  | Time 0.98\n",
      "Epoch 0074: Loss 2.9295 |  | Time 0.98\n",
      "Epoch 0075: Loss 3.0736 |  | Time 0.98\n",
      "Epoch 0076: Loss 3.0537 |  | Time 0.98\n",
      "Epoch 0077: Loss 2.9102 |  | Time 0.98\n",
      "Epoch 0078: Loss 2.9355 |  | Time 0.98\n",
      "Epoch 0079: Loss 2.8640 |  | Time 0.97\n",
      "Epoch 0080: Loss 2.7811 |  | Time 0.98\n",
      "Epoch 0081: Loss 3.0427 |  | Time 0.97\n",
      "Epoch 0082: Loss 2.9670 |  | Time 0.97\n",
      "Epoch 0083: Loss 2.9014 |  | Time 0.97\n",
      "Epoch 0084: Loss 2.8931 |  | Time 0.98\n",
      "Epoch 0085: Loss 2.9030 |  | Time 0.97\n",
      "Epoch 0086: Loss 2.8967 |  | Time 0.97\n",
      "Epoch 0087: Loss 2.8418 |  | Time 0.97\n",
      "Epoch 0088: Loss 2.7676 |  | Time 0.98\n",
      "Epoch 0089: Loss 2.9682 |  | Time 0.98\n",
      "Epoch 0090: Loss 2.8979 |  | Time 0.98\n",
      "Epoch 0091: Loss 2.8814 |  | Time 0.98\n",
      "Epoch 0092: Loss 2.8650 |  | Time 0.99\n",
      "Epoch 0093: Loss 2.8611 |  | Time 0.97\n",
      "Epoch 0094: Loss 2.8555 |  | Time 0.99\n",
      "Epoch 0095: Loss 2.8266 |  | Time 0.98\n",
      "Epoch 0096: Loss 2.7517 |  | Time 0.98\n",
      "Epoch 0097: Loss 2.9378 |  | Time 0.98\n",
      "Epoch 0098: Loss 2.8880 |  | Time 1.02\n",
      "Epoch 0099: Loss 2.8337 |  | Time 0.98\n",
      "Test: Loss 0.0003 | AUC 0.6705 | Recall 0.5426 | Precision 0.5426 | AP 0.6856 | F1 0.5426 | Time 0.36\n",
      "tensor([0, 1]) tensor([6268, 3257])\n",
      "F1 score:  0.6172381835032438\n",
      "Precision:  0.7156892846177464\n",
      "Recall:  0.5425977653631285\n",
      "ini f1_score:  0.6172381835032438\n",
      "ini precision:  tensor(0.5426)\n",
      "ini recall:  tensor(0.5426)\n"
     ]
    }
   ],
   "source": [
    "dominant_model, graph_test = make_dominant_model(train_graph, train_node_features, label_train, test_graph, test_node_features)\n",
    "make_model_runtime = time.time()\n",
    "precision_score, recall_score, f1_score_for = predict_dominant(label_test, dominant_model, graph_test)\n",
    "print(\"ini f1_score: \", f1_score_for)\n",
    "print(\"ini precision: \", precision_score)\n",
    "print(\"ini recall: \", recall_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ini f1:  0.6043874784323392\n",
      "ini precision:  tensor(0.5808)\n",
      "ini recall:  tensor(0.5808)\n"
     ]
    }
   ],
   "source": [
    "print(\"ini f1: \", f1_score_for)\n",
    "print(\"ini precision: \", precision_score)\n",
    "print(\"ini recall: \", recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250.85678052902222, 315.7898392677307, 317.1439673900604]\n",
      "[4.691774845123291, 5.131696939468384, 4.681374549865723]\n"
     ]
    }
   ],
   "source": [
    "print(train_durration)\n",
    "print(predict_durration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ocgnn(label_test, ocgnn_compile, pyG_test):\n",
    "    ocgnn_ip_pred_res, ocgnn_ip_score_res = ocgnn_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "    \n",
    "    unique_values, counts = torch.unique(ocgnn_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "\n",
    "    predictions = ocgnn_ip_pred_res.numpy()\n",
    "    labels = label_test.numpy()\n",
    "    TP = np.sum((labels == 1) & (predictions == 1))\n",
    "    FN = np.sum((labels == 1) & (predictions == 0))\n",
    "    FP = np.sum((labels == 0) & (predictions == 1))\n",
    "    TN = np.sum((labels == 0) & (predictions == 0))\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    f1_pygod = eval_f1(label_test, ocgnn_ip_pred_res)\n",
    "    precision_pygod = eval_precision_at_k(label_test, ocgnn_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, ocgnn_ip_score_res)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    return precision_pygod, recall_pygod, f1_pygod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ocgnn_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features,\n",
    "                        label_test):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = torch.tensor(label_train)\n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train.cpu()\n",
    "    pyG_train.x = train_node_features.cpu()\n",
    "    label_train = label_train.cpu()\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    ocgnn_model = OCGNN(hid_dim=14, num_layers=16, weight_decay=1, \n",
    "                    contamination=0.37, lr=0.004, dropout=0.3, epoch=100, gpu=-1, \n",
    "                    beta=0.1, eps=0.001, verbose=3)\n",
    "    ocgnn_compile = ocgnn_model.fit(pyG_train, label_train)\n",
    "    return ocgnn_compile, pyG_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_15156\\1362066146.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_15156\\1362066146.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_train = torch.tensor(label_train)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_15156\\1362066146.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 90.4539 | AUC 0.8651 | Recall 0.7261 | Precision 0.7261 | AP 0.8188 | F1 0.7261 | Time 0.33\n",
      "Epoch 0001: Loss 18.0573 | AUC 0.2776 | Recall 0.1987 | Precision 0.1987 | AP 0.3595 | F1 0.1987 | Time 0.32\n",
      "Epoch 0002: Loss 5.6296 | AUC 0.3145 | Recall 0.2198 | Precision 0.2198 | AP 0.3985 | F1 0.2198 | Time 0.30\n",
      "Epoch 0003: Loss 2.6356 | AUC 0.1971 | Recall 0.1370 | Precision 0.1370 | AP 0.3060 | F1 0.1370 | Time 0.31\n",
      "Epoch 0004: Loss 2.1936 | AUC 0.1876 | Recall 0.1440 | Precision 0.1440 | AP 0.2693 | F1 0.1440 | Time 0.31\n",
      "Epoch 0005: Loss 1.8038 | AUC 0.2263 | Recall 0.1892 | Precision 0.1892 | AP 0.2676 | F1 0.1892 | Time 0.33\n",
      "Epoch 0006: Loss 1.7659 | AUC 0.2589 | Recall 0.2191 | Precision 0.2191 | AP 0.2709 | F1 0.2191 | Time 0.32\n",
      "Epoch 0007: Loss 1.7562 | AUC 0.2771 | Recall 0.2308 | Precision 0.2308 | AP 0.2722 | F1 0.2308 | Time 0.31\n",
      "Epoch 0008: Loss 1.7572 | AUC 0.3143 | Recall 0.2546 | Precision 0.2546 | AP 0.2811 | F1 0.2544 | Time 0.29\n",
      "Epoch 0009: Loss 1.7543 | AUC 0.3209 | Recall 0.2568 | Precision 0.2568 | AP 0.2776 | F1 0.2568 | Time 0.27\n",
      "Epoch 0010: Loss 1.7572 | AUC 0.3260 | Recall 0.2561 | Precision 0.2561 | AP 0.2790 | F1 0.2561 | Time 0.30\n",
      "Epoch 0011: Loss 1.7543 | AUC 0.3403 | Recall 0.2641 | Precision 0.2641 | AP 0.2823 | F1 0.2641 | Time 0.30\n",
      "Epoch 0012: Loss 1.7572 | AUC 0.3613 | Recall 0.2877 | Precision 0.2877 | AP 0.2939 | F1 0.2877 | Time 0.31\n",
      "Epoch 0013: Loss 1.7647 | AUC 0.3640 | Recall 0.2780 | Precision 0.2780 | AP 0.2871 | F1 0.2780 | Time 0.30\n",
      "Epoch 0014: Loss 1.7543 | AUC 0.3626 | Recall 0.2752 | Precision 0.2752 | AP 0.2878 | F1 0.2752 | Time 0.30\n",
      "Epoch 0015: Loss 1.7543 | AUC 0.3728 | Recall 0.2879 | Precision 0.2879 | AP 0.2945 | F1 0.2879 | Time 0.30\n",
      "Epoch 0016: Loss 1.7543 | AUC 0.3848 | Recall 0.2899 | Precision 0.2899 | AP 0.2979 | F1 0.2899 | Time 0.30\n",
      "Epoch 0017: Loss 1.7543 | AUC 0.3843 | Recall 0.2947 | Precision 0.2947 | AP 0.2916 | F1 0.2948 | Time 0.34\n",
      "Epoch 0018: Loss 1.7543 | AUC 0.4015 | Recall 0.3035 | Precision 0.3035 | AP 0.3004 | F1 0.3035 | Time 0.37\n",
      "Epoch 0019: Loss 1.7547 | AUC 0.3858 | Recall 0.2975 | Precision 0.2975 | AP 0.2925 | F1 0.2975 | Time 0.35\n",
      "Epoch 0020: Loss 1.7543 | AUC 0.4360 | Recall 0.3334 | Precision 0.3334 | AP 0.3178 | F1 0.3334 | Time 0.29\n",
      "Epoch 0021: Loss 1.7543 | AUC 0.4131 | Recall 0.3126 | Precision 0.3126 | AP 0.3040 | F1 0.3126 | Time 0.33\n",
      "Epoch 0022: Loss 1.7543 | AUC 0.4133 | Recall 0.3254 | Precision 0.3254 | AP 0.3107 | F1 0.3254 | Time 0.34\n",
      "Epoch 0023: Loss 1.7543 | AUC 0.4179 | Recall 0.3107 | Precision 0.3107 | AP 0.3090 | F1 0.3107 | Time 0.30\n",
      "Epoch 0024: Loss 1.7543 | AUC 0.4154 | Recall 0.3107 | Precision 0.3107 | AP 0.3100 | F1 0.3108 | Time 0.29\n",
      "Epoch 0025: Loss 1.7543 | AUC 0.4336 | Recall 0.3232 | Precision 0.3232 | AP 0.3208 | F1 0.3230 | Time 0.31\n",
      "Epoch 0026: Loss 1.7543 | AUC 0.4236 | Recall 0.3156 | Precision 0.3156 | AP 0.3131 | F1 0.3156 | Time 0.32\n",
      "Epoch 0027: Loss 1.7543 | AUC 0.4185 | Recall 0.3165 | Precision 0.3165 | AP 0.3077 | F1 0.3162 | Time 0.32\n",
      "Epoch 0028: Loss 1.7543 | AUC 0.4097 | Recall 0.3050 | Precision 0.3050 | AP 0.3037 | F1 0.3050 | Time 0.28\n",
      "Epoch 0029: Loss 1.7543 | AUC 0.4357 | Recall 0.3150 | Precision 0.3150 | AP 0.3148 | F1 0.3150 | Time 0.32\n",
      "Epoch 0030: Loss 1.7543 | AUC 0.4212 | Recall 0.3029 | Precision 0.3029 | AP 0.3152 | F1 0.3025 | Time 0.30\n",
      "Epoch 0031: Loss 1.7543 | AUC 0.4116 | Recall 0.2933 | Precision 0.2933 | AP 0.3132 | F1 0.2933 | Time 0.30\n",
      "Epoch 0032: Loss 1.7543 | AUC 0.4280 | Recall 0.3109 | Precision 0.3109 | AP 0.3261 | F1 0.3108 | Time 0.31\n",
      "Epoch 0033: Loss 1.7543 | AUC 0.4183 | Recall 0.2957 | Precision 0.2957 | AP 0.3154 | F1 0.2955 | Time 0.31\n",
      "Epoch 0034: Loss 1.7543 | AUC 0.4293 | Recall 0.2894 | Precision 0.2894 | AP 0.3178 | F1 0.2895 | Time 0.30\n",
      "Epoch 0035: Loss 1.7543 | AUC 0.4878 | Recall 0.3377 | Precision 0.3377 | AP 0.3480 | F1 0.3374 | Time 0.31\n",
      "Epoch 0036: Loss 1.7543 | AUC 0.4971 | Recall 0.3507 | Precision 0.3507 | AP 0.3503 | F1 0.3461 | Time 0.31\n",
      "Epoch 0037: Loss 1.7543 | AUC 0.5129 | Recall 0.4233 | Precision 0.4233 | AP 0.3627 | F1 0.3345 | Time 0.32\n",
      "Epoch 0038: Loss 1.7543 | AUC 0.5061 | Recall 0.3583 | Precision 0.3583 | AP 0.3578 | F1 0.3235 | Time 0.32\n",
      "Epoch 0039: Loss 1.7543 | AUC 0.4960 | Recall 0.3490 | Precision 0.3490 | AP 0.3416 | F1 0.3489 | Time 0.30\n",
      "Epoch 0040: Loss 1.7543 | AUC 0.5020 | Recall 0.3566 | Precision 0.3566 | AP 0.3569 | F1 0.3561 | Time 0.31\n",
      "Epoch 0041: Loss 1.7543 | AUC 0.5125 | Recall 0.3670 | Precision 0.3670 | AP 0.3608 | F1 0.3667 | Time 0.32\n",
      "Epoch 0042: Loss 1.7543 | AUC 0.5084 | Recall 0.3510 | Precision 0.3510 | AP 0.3558 | F1 0.3512 | Time 0.29\n",
      "Epoch 0043: Loss 1.7543 | AUC 0.5113 | Recall 0.3564 | Precision 0.3564 | AP 0.3564 | F1 0.3564 | Time 0.34\n",
      "Epoch 0044: Loss 1.7543 | AUC 0.4841 | Recall 0.3280 | Precision 0.3280 | AP 0.3451 | F1 0.3268 | Time 0.31\n",
      "Epoch 0045: Loss 1.7543 | AUC 0.5030 | Recall 0.3548 | Precision 0.3548 | AP 0.3497 | F1 0.3238 | Time 0.31\n",
      "Epoch 0046: Loss 1.7543 | AUC 0.5101 | Recall 0.3403 | Precision 0.3403 | AP 0.3591 | F1 0.2908 | Time 0.33\n",
      "Epoch 0047: Loss 1.7543 | AUC 0.5156 | Recall 0.3921 | Precision 0.3921 | AP 0.3667 | F1 0.2772 | Time 0.30\n",
      "Epoch 0048: Loss 1.7543 | AUC 0.5096 | Recall 0.3865 | Precision 0.3865 | AP 0.3629 | F1 0.2425 | Time 0.32\n",
      "Epoch 0049: Loss 1.7543 | AUC 0.5297 | Recall 0.3436 | Precision 0.3436 | AP 0.3709 | F1 0.1413 | Time 0.33\n",
      "Epoch 0050: Loss 1.7543 | AUC 0.4901 | Recall 0.3494 | Precision 0.3494 | AP 0.3438 | F1 0.0863 | Time 0.30\n",
      "Epoch 0051: Loss 1.7543 | AUC 0.4836 | Recall 0.1559 | Precision 0.1559 | AP 0.3410 | F1 0.0832 | Time 0.31\n",
      "Epoch 0052: Loss 1.7543 | AUC 0.5219 | Recall 0.3745 | Precision 0.3745 | AP 0.3660 | F1 0.1958 | Time 0.30\n",
      "Epoch 0053: Loss 1.7543 | AUC 0.5167 | Recall 0.3589 | Precision 0.3589 | AP 0.3672 | F1 0.3483 | Time 0.31\n",
      "Epoch 0054: Loss 1.7543 | AUC 0.4932 | Recall 0.3499 | Precision 0.3499 | AP 0.3628 | F1 0.3429 | Time 0.31\n",
      "Epoch 0055: Loss 1.7543 | AUC 0.5088 | Recall 0.3667 | Precision 0.3667 | AP 0.3807 | F1 0.3596 | Time 0.30\n",
      "Epoch 0056: Loss 1.7543 | AUC 0.4942 | Recall 0.0461 | Precision 0.0461 | AP 0.3537 | F1 0.0845 | Time 0.29\n",
      "Epoch 0057: Loss 1.7543 | AUC 0.4973 | Recall 0.0266 | Precision 0.0266 | AP 0.3548 | F1 0.0499 | Time 0.32\n",
      "Epoch 0058: Loss 1.7543 | AUC 0.4986 | Recall 0.0104 | Precision 0.0104 | AP 0.3522 | F1 0.0206 | Time 0.30\n",
      "Epoch 0059: Loss 1.7543 | AUC 0.4997 | Recall 0.0093 | Precision 0.0093 | AP 0.3521 | F1 0.0184 | Time 0.31\n",
      "Epoch 0060: Loss 1.7543 | AUC 0.4982 | Recall 0.0121 | Precision 0.0121 | AP 0.3527 | F1 0.0239 | Time 0.31\n",
      "Epoch 0061: Loss 1.7543 | AUC 0.5094 | Recall 0.0379 | Precision 0.0379 | AP 0.3681 | F1 0.0718 | Time 0.33\n",
      "Epoch 0062: Loss 1.7543 | AUC 0.5026 | Recall 0.3624 | Precision 0.3624 | AP 0.3760 | F1 0.3554 | Time 0.32\n",
      "Epoch 0063: Loss 1.7543 | AUC 0.4778 | Recall 0.3408 | Precision 0.3408 | AP 0.3549 | F1 0.3374 | Time 0.31\n",
      "Epoch 0064: Loss 1.7543 | AUC 0.4949 | Recall 0.3518 | Precision 0.3518 | AP 0.3792 | F1 0.3450 | Time 0.32\n",
      "Epoch 0065: Loss 1.7543 | AUC 0.5264 | Recall 0.3914 | Precision 0.3914 | AP 0.4241 | F1 0.3487 | Time 0.32\n",
      "Epoch 0066: Loss 1.7543 | AUC 0.5155 | Recall 0.0888 | Precision 0.0888 | AP 0.3863 | F1 0.1543 | Time 0.27\n",
      "Epoch 0067: Loss 1.7543 | AUC 0.5273 | Recall 0.1102 | Precision 0.1102 | AP 0.3859 | F1 0.1830 | Time 0.33\n",
      "Epoch 0068: Loss 1.7543 | AUC 0.5192 | Recall 0.1334 | Precision 0.1334 | AP 0.3837 | F1 0.2096 | Time 0.31\n",
      "Epoch 0069: Loss 1.7543 | AUC 0.4709 | Recall 0.3265 | Precision 0.3265 | AP 0.3517 | F1 0.3208 | Time 0.32\n",
      "Epoch 0070: Loss 1.7543 | AUC 0.4980 | Recall 0.3427 | Precision 0.3427 | AP 0.3602 | F1 0.3425 | Time 0.35\n",
      "Epoch 0071: Loss 1.7543 | AUC 0.4912 | Recall 0.3427 | Precision 0.3427 | AP 0.3647 | F1 0.3379 | Time 0.35\n",
      "Epoch 0072: Loss 1.7543 | AUC 0.4882 | Recall 0.3431 | Precision 0.3431 | AP 0.3633 | F1 0.3413 | Time 0.32\n",
      "Epoch 0073: Loss 1.7543 | AUC 0.4831 | Recall 0.3408 | Precision 0.3408 | AP 0.3576 | F1 0.3344 | Time 0.29\n",
      "Epoch 0074: Loss 1.7543 | AUC 0.4779 | Recall 0.3328 | Precision 0.3328 | AP 0.3507 | F1 0.3315 | Time 0.30\n",
      "Epoch 0075: Loss 1.7543 | AUC 0.4968 | Recall 0.3550 | Precision 0.3550 | AP 0.3657 | F1 0.3511 | Time 0.32\n",
      "Epoch 0076: Loss 1.7543 | AUC 0.4927 | Recall 0.3524 | Precision 0.3524 | AP 0.3616 | F1 0.3518 | Time 0.31\n",
      "Epoch 0077: Loss 1.7543 | AUC 0.4908 | Recall 0.3529 | Precision 0.3529 | AP 0.3585 | F1 0.3490 | Time 0.31\n",
      "Epoch 0078: Loss 1.7543 | AUC 0.4942 | Recall 0.3457 | Precision 0.3457 | AP 0.3624 | F1 0.3453 | Time 0.31\n",
      "Epoch 0079: Loss 1.7543 | AUC 0.5065 | Recall 0.3479 | Precision 0.3479 | AP 0.3777 | F1 0.3474 | Time 0.35\n",
      "Epoch 0080: Loss 1.7543 | AUC 0.4981 | Recall 0.3510 | Precision 0.3510 | AP 0.3733 | F1 0.3472 | Time 0.34\n",
      "Epoch 0081: Loss 1.7543 | AUC 0.4960 | Recall 0.3466 | Precision 0.3466 | AP 0.3651 | F1 0.3436 | Time 0.33\n",
      "Epoch 0082: Loss 1.7543 | AUC 0.5022 | Recall 0.3514 | Precision 0.3514 | AP 0.3702 | F1 0.3492 | Time 0.31\n",
      "Epoch 0083: Loss 1.7543 | AUC 0.4871 | Recall 0.3373 | Precision 0.3373 | AP 0.3626 | F1 0.3331 | Time 0.35\n",
      "Epoch 0084: Loss 1.7543 | AUC 0.4909 | Recall 0.3364 | Precision 0.3364 | AP 0.3525 | F1 0.3343 | Time 0.29\n",
      "Epoch 0085: Loss 1.7543 | AUC 0.5127 | Recall 0.3553 | Precision 0.3553 | AP 0.3720 | F1 0.3515 | Time 0.31\n",
      "Epoch 0086: Loss 1.7543 | AUC 0.5027 | Recall 0.3516 | Precision 0.3516 | AP 0.3627 | F1 0.3493 | Time 0.31\n",
      "Epoch 0087: Loss 1.7543 | AUC 0.5142 | Recall 0.3559 | Precision 0.3559 | AP 0.3713 | F1 0.3529 | Time 0.29\n",
      "Epoch 0088: Loss 1.7543 | AUC 0.4982 | Recall 0.3522 | Precision 0.3522 | AP 0.3636 | F1 0.3521 | Time 0.31\n",
      "Epoch 0089: Loss 1.7543 | AUC 0.5115 | Recall 0.3618 | Precision 0.3618 | AP 0.3708 | F1 0.3587 | Time 0.32\n",
      "Epoch 0090: Loss 1.7543 | AUC 0.4951 | Recall 0.3503 | Precision 0.3503 | AP 0.3587 | F1 0.3446 | Time 0.35\n",
      "Epoch 0091: Loss 1.7543 | AUC 0.5076 | Recall 0.3525 | Precision 0.3525 | AP 0.3714 | F1 0.3480 | Time 0.31\n",
      "Epoch 0092: Loss 1.7543 | AUC 0.5098 | Recall 0.3596 | Precision 0.3596 | AP 0.3752 | F1 0.3482 | Time 0.38\n",
      "Epoch 0093: Loss 1.7543 | AUC 0.5045 | Recall 0.3550 | Precision 0.3550 | AP 0.3768 | F1 0.3479 | Time 0.30\n",
      "Epoch 0094: Loss 1.7543 | AUC 0.5167 | Recall 0.3628 | Precision 0.3628 | AP 0.3805 | F1 0.3558 | Time 0.30\n",
      "Epoch 0095: Loss 1.7543 | AUC 0.5079 | Recall 0.3570 | Precision 0.3570 | AP 0.3678 | F1 0.3562 | Time 0.31\n",
      "Epoch 0096: Loss 1.7543 | AUC 0.5018 | Recall 0.3598 | Precision 0.3598 | AP 0.3615 | F1 0.3434 | Time 0.32\n",
      "Epoch 0097: Loss 1.7543 | AUC 0.5035 | Recall 0.3585 | Precision 0.3585 | AP 0.3696 | F1 0.3576 | Time 0.32\n",
      "Epoch 0098: Loss 1.7543 | AUC 0.5005 | Recall 0.3514 | Precision 0.3514 | AP 0.3604 | F1 0.3396 | Time 0.31\n",
      "Epoch 0099: Loss 1.7543 | AUC 0.4906 | Recall 0.3542 | Precision 0.3542 | AP 0.3611 | F1 0.3358 | Time 0.32\n",
      "Test: Loss 0.0002 | AUC 0.4512 | Recall 0.7886 | Precision 0.7886 | AP 0.4355 | F1 0.0951 | Time 0.08\n",
      "tensor([0]) tensor([9525])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_15156\\1269081826.py:14: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = TP / (TP + FP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  nan\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "ini f1:  0.0\n",
      "ini precision:  tensor(0.7886)\n",
      "ini recall:  tensor(0.7886)\n"
     ]
    }
   ],
   "source": [
    "ocgnn_model, graph_test = make_ocgnn_model(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n",
    "precision_score, recall_score, f1_score= predict_ocgnn(label_test, ocgnn_model, graph_test)\n",
    "print(\"ini f1: \", f1_score)\n",
    "print(\"ini precision: \", precision_score)\n",
    "print(\"ini recall: \", recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18937524353812182, 0.19383259911894274, 0.19747738565422346]\n",
      "[0.21422274463708493, 0.21858562244301577, 0.21812552772305094]\n",
      "[0.1696927374301676, 0.17411545623836128, 0.1804003724394786]\n"
     ]
    }
   ],
   "source": [
    "print(f1_ocgnn)\n",
    "print(precision_ocgnn)\n",
    "print(recall_ocgnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37.998138189315796, 34.89245128631592, 37.14145493507385]\n",
      "[1.1997323036193848, 1.2506287097930908, 0.970118522644043]\n"
     ]
    }
   ],
   "source": [
    "print(train_durration_ocgnn)\n",
    "print(predict_durration_ocgnn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gae_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features,\n",
    "                        label_test):\n",
    "    \n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train.cpu()\n",
    "    pyG_train.x = train_node_features.cpu()\n",
    "    label_train = label_train.cpu()\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    gae_model = GAE(hid_dim=64, num_layers=36, weight_decay=3,\n",
    "                contamination=0.37, lr=0.001, epoch=150, gpu=-1,\n",
    "                num_neigh=-1, verbose=3)\n",
    "    \n",
    "    gae_compile = gae_model.fit(pyG_train, label_train)\n",
    "    return gae_compile, pyG_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gae(label_test, gae_compile, pyG_test):\n",
    "    gae_ip_pred_res, gae_ip_score_res = gae_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "    f1_score_pygod = eval_f1(label_test, gae_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, gae_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, gae_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    unique_values, counts = torch.unique(gae_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 score pygod: \", f1_score_pygod)\n",
    "    return f1_score_pygod, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 115987.7031 | AUC 0.7962 | Recall 0.6861 | Precision 0.6861 | AP 0.6719 | F1 0.6975 | Time 2.19\n",
      "Epoch 0001: Loss 115986.7188 | AUC 0.7797 | Recall 0.6543 | Precision 0.6543 | AP 0.6568 | F1 0.6554 | Time 1.80\n",
      "Epoch 0002: Loss 115985.3281 | AUC 0.7813 | Recall 0.6569 | Precision 0.6569 | AP 0.6581 | F1 0.6580 | Time 2.38\n",
      "Epoch 0003: Loss 115983.9062 | AUC 0.7821 | Recall 0.6584 | Precision 0.6584 | AP 0.6587 | F1 0.6594 | Time 2.54\n",
      "Epoch 0004: Loss 115982.4297 | AUC 0.7826 | Recall 0.6595 | Precision 0.6595 | AP 0.6591 | F1 0.6598 | Time 2.78\n",
      "Epoch 0005: Loss 115980.8750 | AUC 0.7829 | Recall 0.6599 | Precision 0.6599 | AP 0.6593 | F1 0.6607 | Time 1.99\n",
      "Epoch 0006: Loss 115979.2500 | AUC 0.7838 | Recall 0.6612 | Precision 0.6612 | AP 0.6599 | F1 0.6616 | Time 1.82\n",
      "Epoch 0007: Loss 115977.4688 | AUC 0.7839 | Recall 0.6614 | Precision 0.6614 | AP 0.6601 | F1 0.6615 | Time 1.95\n",
      "Epoch 0008: Loss 115975.5781 | AUC 0.7842 | Recall 0.6614 | Precision 0.6614 | AP 0.6603 | F1 0.6614 | Time 2.51\n",
      "Epoch 0009: Loss 115973.5703 | AUC 0.7845 | Recall 0.6614 | Precision 0.6614 | AP 0.6605 | F1 0.6614 | Time 1.89\n",
      "Epoch 0010: Loss 115971.4062 | AUC 0.7850 | Recall 0.6616 | Precision 0.6616 | AP 0.6608 | F1 0.6616 | Time 1.92\n",
      "Epoch 0011: Loss 115969.0547 | AUC 0.7850 | Recall 0.6616 | Precision 0.6616 | AP 0.6609 | F1 0.6617 | Time 1.77\n",
      "Epoch 0012: Loss 115966.4531 | AUC 0.7851 | Recall 0.6616 | Precision 0.6616 | AP 0.6609 | F1 0.6617 | Time 1.99\n",
      "Epoch 0013: Loss 115963.6172 | AUC 0.7852 | Recall 0.6616 | Precision 0.6616 | AP 0.6610 | F1 0.6617 | Time 1.96\n",
      "Epoch 0014: Loss 115960.4766 | AUC 0.7853 | Recall 0.6618 | Precision 0.6618 | AP 0.6611 | F1 0.6618 | Time 2.79\n",
      "Epoch 0015: Loss 115956.9922 | AUC 0.7854 | Recall 0.6618 | Precision 0.6618 | AP 0.6611 | F1 0.6618 | Time 2.39\n",
      "Epoch 0016: Loss 115953.1016 | AUC 0.7856 | Recall 0.6618 | Precision 0.6618 | AP 0.6613 | F1 0.6618 | Time 1.83\n",
      "Epoch 0017: Loss 115948.7578 | AUC 0.7857 | Recall 0.6618 | Precision 0.6618 | AP 0.6614 | F1 0.6618 | Time 1.70\n",
      "Epoch 0018: Loss 115943.8359 | AUC 0.7858 | Recall 0.6618 | Precision 0.6618 | AP 0.6614 | F1 0.6618 | Time 1.88\n",
      "Epoch 0019: Loss 115938.2344 | AUC 0.7859 | Recall 0.6618 | Precision 0.6618 | AP 0.6615 | F1 0.6618 | Time 2.08\n",
      "Epoch 0020: Loss 115931.8125 | AUC 0.7860 | Recall 0.6620 | Precision 0.6620 | AP 0.6615 | F1 0.6620 | Time 2.04\n",
      "Epoch 0021: Loss 115924.3203 | AUC 0.7861 | Recall 0.6620 | Precision 0.6620 | AP 0.6616 | F1 0.6620 | Time 1.72\n",
      "Epoch 0022: Loss 115915.5859 | AUC 0.7861 | Recall 0.6620 | Precision 0.6620 | AP 0.6617 | F1 0.6620 | Time 1.71\n",
      "Epoch 0023: Loss 115905.3047 | AUC 0.7862 | Recall 0.6620 | Precision 0.6620 | AP 0.6617 | F1 0.6620 | Time 2.09\n",
      "Epoch 0024: Loss 115893.1016 | AUC 0.7862 | Recall 0.6621 | Precision 0.6621 | AP 0.6617 | F1 0.6621 | Time 2.00\n",
      "Epoch 0025: Loss 115878.5312 | AUC 0.7862 | Recall 0.6621 | Precision 0.6621 | AP 0.6618 | F1 0.6621 | Time 1.74\n",
      "Epoch 0026: Loss 115860.9141 | AUC 0.7863 | Recall 0.6623 | Precision 0.6623 | AP 0.6618 | F1 0.6623 | Time 1.84\n",
      "Epoch 0027: Loss 115839.3281 | AUC 0.7863 | Recall 0.6625 | Precision 0.6625 | AP 0.6618 | F1 0.6626 | Time 1.96\n",
      "Epoch 0028: Loss 115812.6719 | AUC 0.7863 | Recall 0.6625 | Precision 0.6625 | AP 0.6619 | F1 0.6625 | Time 2.35\n",
      "Epoch 0029: Loss 115779.3828 | AUC 0.7863 | Recall 0.6625 | Precision 0.6625 | AP 0.6619 | F1 0.6626 | Time 1.83\n",
      "Epoch 0030: Loss 115737.2344 | AUC 0.7864 | Recall 0.6625 | Precision 0.6625 | AP 0.6619 | F1 0.6625 | Time 1.82\n",
      "Epoch 0031: Loss 115683.2812 | AUC 0.7863 | Recall 0.6623 | Precision 0.6623 | AP 0.6619 | F1 0.6623 | Time 2.23\n",
      "Epoch 0032: Loss 115613.4609 | AUC 0.7863 | Recall 0.6625 | Precision 0.6625 | AP 0.6619 | F1 0.6625 | Time 2.30\n",
      "Epoch 0033: Loss 115521.8594 | AUC 0.7864 | Recall 0.6625 | Precision 0.6625 | AP 0.6619 | F1 0.6625 | Time 1.97\n",
      "Epoch 0034: Loss 115400.1016 | AUC 0.7865 | Recall 0.6621 | Precision 0.6621 | AP 0.6619 | F1 0.6621 | Time 1.85\n",
      "Epoch 0035: Loss 115236.4844 | AUC 0.7861 | Recall 0.6612 | Precision 0.6612 | AP 0.6616 | F1 0.6612 | Time 2.11\n",
      "Epoch 0036: Loss 115013.9766 | AUC 0.7860 | Recall 0.6610 | Precision 0.6610 | AP 0.6615 | F1 0.6610 | Time 2.17\n",
      "Epoch 0037: Loss 114708.8828 | AUC 0.7857 | Recall 0.6597 | Precision 0.6597 | AP 0.6609 | F1 0.6597 | Time 2.06\n",
      "Epoch 0038: Loss 114286.2109 | AUC 0.7843 | Recall 0.6564 | Precision 0.6564 | AP 0.6596 | F1 0.6564 | Time 1.81\n",
      "Epoch 0039: Loss 113694.2656 | AUC 0.7743 | Recall 0.6538 | Precision 0.6538 | AP 0.6554 | F1 0.6538 | Time 1.99\n",
      "Epoch 0040: Loss 112851.7344 | AUC 0.6132 | Recall 0.5642 | Precision 0.5642 | AP 0.5712 | F1 0.5642 | Time 2.17\n",
      "Epoch 0041: Loss 111635.2344 | AUC 0.5399 | Recall 0.3436 | Precision 0.3436 | AP 0.4739 | F1 0.3436 | Time 1.94\n",
      "Epoch 0042: Loss 109881.6016 | AUC 0.4657 | Recall 0.2828 | Precision 0.2828 | AP 0.4214 | F1 0.2828 | Time 1.96\n",
      "Epoch 0043: Loss 107620.6562 | AUC 0.5033 | Recall 0.3812 | Precision 0.3812 | AP 0.4589 | F1 0.3812 | Time 2.07\n",
      "Epoch 0044: Loss 106858.4062 | AUC 0.6962 | Recall 0.6203 | Precision 0.6203 | AP 0.7153 | F1 0.6203 | Time 2.49\n",
      "Epoch 0045: Loss 107103.6484 | AUC 0.8056 | Recall 0.7421 | Precision 0.7421 | AP 0.8221 | F1 0.7421 | Time 2.11\n",
      "Epoch 0046: Loss 104132.3594 | AUC 0.8032 | Recall 0.7417 | Precision 0.7417 | AP 0.8242 | F1 0.7417 | Time 1.78\n",
      "Epoch 0047: Loss 102136.8203 | AUC 0.7744 | Recall 0.7047 | Precision 0.7047 | AP 0.7905 | F1 0.7047 | Time 2.00\n",
      "Epoch 0048: Loss 101003.9141 | AUC 0.7768 | Recall 0.7149 | Precision 0.7149 | AP 0.7684 | F1 0.7149 | Time 2.31\n",
      "Epoch 0049: Loss 99582.2734 | AUC 0.8094 | Recall 0.7465 | Precision 0.7465 | AP 0.7957 | F1 0.7465 | Time 1.95\n",
      "Epoch 0050: Loss 98423.9609 | AUC 0.8536 | Recall 0.7764 | Precision 0.7764 | AP 0.8474 | F1 0.7764 | Time 1.91\n",
      "Epoch 0051: Loss 98578.9375 | AUC 0.8804 | Recall 0.7993 | Precision 0.7993 | AP 0.8762 | F1 0.7993 | Time 1.90\n",
      "Epoch 0052: Loss 97481.7031 | AUC 0.8585 | Recall 0.7857 | Precision 0.7857 | AP 0.8353 | F1 0.7857 | Time 2.23\n",
      "Epoch 0053: Loss 97776.9609 | AUC 0.8485 | Recall 0.7800 | Precision 0.7800 | AP 0.8061 | F1 0.7800 | Time 2.04\n",
      "Epoch 0054: Loss 97450.1641 | AUC 0.8571 | Recall 0.7854 | Precision 0.7854 | AP 0.8137 | F1 0.7854 | Time 1.74\n",
      "Epoch 0055: Loss 96861.3984 | AUC 0.8755 | Recall 0.8017 | Precision 0.8017 | AP 0.8457 | F1 0.8017 | Time 1.83\n",
      "Epoch 0056: Loss 97297.3906 | AUC 0.8932 | Recall 0.8121 | Precision 0.8121 | AP 0.8726 | F1 0.8121 | Time 2.24\n",
      "Epoch 0057: Loss 96700.7891 | AUC 0.8841 | Recall 0.8091 | Precision 0.8091 | AP 0.8535 | F1 0.8091 | Time 2.10\n",
      "Epoch 0058: Loss 96819.5000 | AUC 0.8774 | Recall 0.7995 | Precision 0.7995 | AP 0.8293 | F1 0.7995 | Time 1.81\n",
      "Epoch 0059: Loss 96892.2656 | AUC 0.8793 | Recall 0.7991 | Precision 0.7991 | AP 0.8256 | F1 0.7991 | Time 1.91\n",
      "Epoch 0060: Loss 96525.1250 | AUC 0.8853 | Recall 0.8067 | Precision 0.8067 | AP 0.8421 | F1 0.8067 | Time 1.87\n",
      "Epoch 0061: Loss 96663.5547 | AUC 0.8948 | Recall 0.8151 | Precision 0.8151 | AP 0.8639 | F1 0.8151 | Time 1.81\n",
      "Epoch 0062: Loss 96594.1016 | AUC 0.8952 | Recall 0.8149 | Precision 0.8149 | AP 0.8625 | F1 0.8149 | Time 2.04\n",
      "Epoch 0063: Loss 96424.4922 | AUC 0.8901 | Recall 0.8093 | Precision 0.8093 | AP 0.8444 | F1 0.8093 | Time 2.14\n",
      "Epoch 0064: Loss 96591.1562 | AUC 0.8896 | Recall 0.8062 | Precision 0.8062 | AP 0.8348 | F1 0.8062 | Time 1.95\n",
      "Epoch 0065: Loss 96449.5312 | AUC 0.8914 | Recall 0.8084 | Precision 0.8084 | AP 0.8406 | F1 0.8084 | Time 1.93\n",
      "Epoch 0066: Loss 96369.5000 | AUC 0.8958 | Recall 0.8155 | Precision 0.8155 | AP 0.8555 | F1 0.8155 | Time 2.02\n",
      "Epoch 0067: Loss 96497.1250 | AUC 0.8993 | Recall 0.8181 | Precision 0.8181 | AP 0.8635 | F1 0.8181 | Time 2.12\n",
      "Epoch 0068: Loss 96319.6406 | AUC 0.8965 | Recall 0.8145 | Precision 0.8145 | AP 0.8533 | F1 0.8145 | Time 2.19\n",
      "Epoch 0069: Loss 96386.6250 | AUC 0.8951 | Recall 0.8108 | Precision 0.8108 | AP 0.8425 | F1 0.8108 | Time 2.28\n",
      "Epoch 0070: Loss 96393.6875 | AUC 0.8957 | Recall 0.8106 | Precision 0.8106 | AP 0.8418 | F1 0.8106 | Time 2.03\n",
      "Epoch 0071: Loss 96281.5781 | AUC 0.8976 | Recall 0.8142 | Precision 0.8142 | AP 0.8511 | F1 0.8142 | Time 1.87\n",
      "Epoch 0072: Loss 96361.8203 | AUC 0.9005 | Recall 0.8179 | Precision 0.8179 | AP 0.8605 | F1 0.8179 | Time 2.04\n",
      "Epoch 0073: Loss 96302.7812 | AUC 0.8999 | Recall 0.8169 | Precision 0.8169 | AP 0.8575 | F1 0.8169 | Time 2.19\n",
      "Epoch 0074: Loss 96276.3359 | AUC 0.8984 | Recall 0.8142 | Precision 0.8142 | AP 0.8482 | F1 0.8142 | Time 1.90\n",
      "Epoch 0075: Loss 96325.4141 | AUC 0.8984 | Recall 0.8127 | Precision 0.8127 | AP 0.8444 | F1 0.8127 | Time 1.79\n",
      "Epoch 0076: Loss 96256.8594 | AUC 0.8994 | Recall 0.8149 | Precision 0.8149 | AP 0.8493 | F1 0.8149 | Time 2.20\n",
      "Epoch 0077: Loss 96270.8203 | AUC 0.9012 | Recall 0.8171 | Precision 0.8171 | AP 0.8574 | F1 0.8171 | Time 2.25\n",
      "Epoch 0078: Loss 96279.1094 | AUC 0.9017 | Recall 0.8175 | Precision 0.8175 | AP 0.8585 | F1 0.8175 | Time 1.98\n",
      "Epoch 0079: Loss 96231.1797 | AUC 0.9007 | Recall 0.8156 | Precision 0.8156 | AP 0.8518 | F1 0.8156 | Time 1.81\n",
      "Epoch 0080: Loss 96265.7891 | AUC 0.9003 | Recall 0.8149 | Precision 0.8149 | AP 0.8472 | F1 0.8149 | Time 1.84\n",
      "Epoch 0081: Loss 96237.4062 | AUC 0.9008 | Recall 0.8156 | Precision 0.8156 | AP 0.8494 | F1 0.8156 | Time 2.38\n",
      "Epoch 0082: Loss 96226.3672 | AUC 0.9022 | Recall 0.8164 | Precision 0.8164 | AP 0.8557 | F1 0.8164 | Time 2.06\n",
      "Epoch 0083: Loss 96244.5000 | AUC 0.9028 | Recall 0.8177 | Precision 0.8177 | AP 0.8580 | F1 0.8177 | Time 1.83\n",
      "Epoch 0084: Loss 96209.3906 | AUC 0.9022 | Recall 0.8156 | Precision 0.8156 | AP 0.8534 | F1 0.8156 | Time 2.02\n",
      "Epoch 0085: Loss 96228.1719 | AUC 0.9018 | Recall 0.8162 | Precision 0.8162 | AP 0.8492 | F1 0.8162 | Time 2.33\n",
      "Epoch 0086: Loss 96214.7812 | AUC 0.9021 | Recall 0.8164 | Precision 0.8164 | AP 0.8503 | F1 0.8164 | Time 2.13\n",
      "Epoch 0087: Loss 96203.1484 | AUC 0.9031 | Recall 0.8162 | Precision 0.8162 | AP 0.8552 | F1 0.8162 | Time 1.86\n",
      "Epoch 0088: Loss 96215.7812 | AUC 0.9037 | Recall 0.8177 | Precision 0.8177 | AP 0.8573 | F1 0.8177 | Time 1.93\n",
      "Epoch 0089: Loss 96193.5000 | AUC 0.9032 | Recall 0.8160 | Precision 0.8160 | AP 0.8539 | F1 0.8160 | Time 2.30\n",
      "Epoch 0090: Loss 96204.1016 | AUC 0.9029 | Recall 0.8166 | Precision 0.8166 | AP 0.8505 | F1 0.8166 | Time 2.18\n",
      "Epoch 0091: Loss 96194.0469 | AUC 0.9032 | Recall 0.8164 | Precision 0.8164 | AP 0.8515 | F1 0.8164 | Time 1.78\n",
      "Epoch 0092: Loss 96188.0859 | AUC 0.9040 | Recall 0.8166 | Precision 0.8166 | AP 0.8553 | F1 0.8166 | Time 1.80\n",
      "Epoch 0093: Loss 96192.5781 | AUC 0.9044 | Recall 0.8169 | Precision 0.8169 | AP 0.8566 | F1 0.8169 | Time 2.27\n",
      "Epoch 0094: Loss 96179.5781 | AUC 0.9040 | Recall 0.8162 | Precision 0.8162 | AP 0.8539 | F1 0.8162 | Time 2.12\n",
      "Epoch 0095: Loss 96185.3906 | AUC 0.9038 | Recall 0.8169 | Precision 0.8169 | AP 0.8515 | F1 0.8169 | Time 1.83\n",
      "Epoch 0096: Loss 96177.0625 | AUC 0.9041 | Recall 0.8168 | Precision 0.8168 | AP 0.8527 | F1 0.8168 | Time 1.82\n",
      "Epoch 0097: Loss 96175.2812 | AUC 0.9048 | Recall 0.8169 | Precision 0.8169 | AP 0.8556 | F1 0.8169 | Time 2.33\n",
      "Epoch 0098: Loss 96175.0312 | AUC 0.9050 | Recall 0.8173 | Precision 0.8173 | AP 0.8561 | F1 0.8173 | Time 2.32\n",
      "Epoch 0099: Loss 96167.7656 | AUC 0.9047 | Recall 0.8166 | Precision 0.8166 | AP 0.8538 | F1 0.8166 | Time 2.09\n",
      "Epoch 0100: Loss 96170.5703 | AUC 0.9047 | Recall 0.8171 | Precision 0.8171 | AP 0.8523 | F1 0.8171 | Time 2.17\n",
      "Epoch 0101: Loss 96163.5000 | AUC 0.9050 | Recall 0.8168 | Precision 0.8168 | AP 0.8537 | F1 0.8168 | Time 2.42\n",
      "Epoch 0102: Loss 96163.8125 | AUC 0.9055 | Recall 0.8175 | Precision 0.8175 | AP 0.8559 | F1 0.8175 | Time 2.51\n",
      "Epoch 0103: Loss 96160.6328 | AUC 0.9056 | Recall 0.8175 | Precision 0.8175 | AP 0.8557 | F1 0.8175 | Time 1.93\n",
      "Epoch 0104: Loss 96157.3516 | AUC 0.9054 | Recall 0.8169 | Precision 0.8169 | AP 0.8537 | F1 0.8169 | Time 2.39\n",
      "Epoch 0105: Loss 96157.4375 | AUC 0.9054 | Recall 0.8175 | Precision 0.8175 | AP 0.8531 | F1 0.8175 | Time 2.46\n",
      "Epoch 0106: Loss 96152.1875 | AUC 0.9058 | Recall 0.8173 | Precision 0.8173 | AP 0.8547 | F1 0.8173 | Time 1.88\n",
      "Epoch 0107: Loss 96152.7812 | AUC 0.9062 | Recall 0.8181 | Precision 0.8181 | AP 0.8560 | F1 0.8181 | Time 1.79\n",
      "Epoch 0108: Loss 96148.4766 | AUC 0.9061 | Recall 0.8175 | Precision 0.8175 | AP 0.8552 | F1 0.8175 | Time 2.07\n",
      "Epoch 0109: Loss 96147.7734 | AUC 0.9060 | Recall 0.8173 | Precision 0.8173 | AP 0.8538 | F1 0.8173 | Time 1.98\n",
      "Epoch 0110: Loss 96145.1406 | AUC 0.9062 | Recall 0.8175 | Precision 0.8175 | AP 0.8540 | F1 0.8175 | Time 1.98\n",
      "Epoch 0111: Loss 96142.6562 | AUC 0.9065 | Recall 0.8177 | Precision 0.8177 | AP 0.8555 | F1 0.8177 | Time 1.92\n",
      "Epoch 0112: Loss 96141.5469 | AUC 0.9067 | Recall 0.8181 | Precision 0.8181 | AP 0.8559 | F1 0.8181 | Time 1.89\n",
      "Epoch 0113: Loss 96138.2500 | AUC 0.9067 | Recall 0.8177 | Precision 0.8177 | AP 0.8548 | F1 0.8177 | Time 2.33\n",
      "Epoch 0114: Loss 96137.6250 | AUC 0.9067 | Recall 0.8179 | Precision 0.8179 | AP 0.8541 | F1 0.8179 | Time 2.05\n",
      "Epoch 0115: Loss 96134.3438 | AUC 0.9069 | Recall 0.8177 | Precision 0.8177 | AP 0.8549 | F1 0.8177 | Time 1.80\n",
      "Epoch 0116: Loss 96133.3984 | AUC 0.9072 | Recall 0.8182 | Precision 0.8182 | AP 0.8560 | F1 0.8182 | Time 1.75\n",
      "Epoch 0117: Loss 96130.6406 | AUC 0.9072 | Recall 0.8181 | Precision 0.8181 | AP 0.8555 | F1 0.8181 | Time 2.28\n",
      "Epoch 0118: Loss 96129.2891 | AUC 0.9072 | Recall 0.8181 | Precision 0.8181 | AP 0.8546 | F1 0.8181 | Time 2.17\n",
      "Epoch 0119: Loss 96126.9141 | AUC 0.9074 | Recall 0.8184 | Precision 0.8184 | AP 0.8549 | F1 0.8184 | Time 1.74\n",
      "Epoch 0120: Loss 96125.2188 | AUC 0.9076 | Recall 0.8186 | Precision 0.8186 | AP 0.8559 | F1 0.8186 | Time 1.81\n",
      "Epoch 0121: Loss 96123.2031 | AUC 0.9077 | Recall 0.8188 | Precision 0.8188 | AP 0.8559 | F1 0.8188 | Time 2.64\n",
      "Epoch 0122: Loss 96121.3203 | AUC 0.9078 | Recall 0.8188 | Precision 0.8188 | AP 0.8550 | F1 0.8188 | Time 2.16\n",
      "Epoch 0123: Loss 96119.5156 | AUC 0.9079 | Recall 0.8190 | Precision 0.8190 | AP 0.8550 | F1 0.8190 | Time 1.83\n",
      "Epoch 0124: Loss 96117.3906 | AUC 0.9081 | Recall 0.8190 | Precision 0.8190 | AP 0.8559 | F1 0.8190 | Time 1.79\n",
      "Epoch 0125: Loss 96115.6875 | AUC 0.9082 | Recall 0.8194 | Precision 0.8194 | AP 0.8561 | F1 0.8194 | Time 2.24\n",
      "Epoch 0126: Loss 96113.5781 | AUC 0.9083 | Recall 0.8190 | Precision 0.8190 | AP 0.8554 | F1 0.8190 | Time 2.21\n",
      "Epoch 0127: Loss 96111.8594 | AUC 0.9084 | Recall 0.8192 | Precision 0.8192 | AP 0.8553 | F1 0.8192 | Time 1.77\n",
      "Epoch 0128: Loss 96109.7500 | AUC 0.9086 | Recall 0.8192 | Precision 0.8192 | AP 0.8560 | F1 0.8192 | Time 1.79\n",
      "Epoch 0129: Loss 96107.9922 | AUC 0.9087 | Recall 0.8196 | Precision 0.8196 | AP 0.8562 | F1 0.8196 | Time 2.16\n",
      "Epoch 0130: Loss 96105.9688 | AUC 0.9088 | Recall 0.8197 | Precision 0.8197 | AP 0.8556 | F1 0.8197 | Time 2.27\n",
      "Epoch 0131: Loss 96104.1250 | AUC 0.9089 | Recall 0.8197 | Precision 0.8197 | AP 0.8556 | F1 0.8197 | Time 1.80\n",
      "Epoch 0132: Loss 96102.1094 | AUC 0.9091 | Recall 0.8201 | Precision 0.8201 | AP 0.8563 | F1 0.8201 | Time 1.80\n",
      "Epoch 0133: Loss 96100.2344 | AUC 0.9092 | Recall 0.8201 | Precision 0.8201 | AP 0.8563 | F1 0.8201 | Time 2.09\n",
      "Epoch 0134: Loss 96098.3203 | AUC 0.9093 | Recall 0.8199 | Precision 0.8199 | AP 0.8558 | F1 0.8199 | Time 2.14\n",
      "Epoch 0135: Loss 96096.1797 | AUC 0.9095 | Recall 0.8201 | Precision 0.8201 | AP 0.8560 | F1 0.8201 | Time 1.83\n",
      "Epoch 0136: Loss 96094.3906 | AUC 0.9096 | Recall 0.8203 | Precision 0.8203 | AP 0.8566 | F1 0.8203 | Time 1.82\n",
      "Epoch 0137: Loss 96092.2109 | AUC 0.9097 | Recall 0.8201 | Precision 0.8201 | AP 0.8563 | F1 0.8201 | Time 2.05\n",
      "Epoch 0138: Loss 96090.4062 | AUC 0.9099 | Recall 0.8201 | Precision 0.8201 | AP 0.8560 | F1 0.8201 | Time 2.23\n",
      "Epoch 0139: Loss 96088.2578 | AUC 0.9100 | Recall 0.8205 | Precision 0.8205 | AP 0.8566 | F1 0.8205 | Time 1.97\n",
      "Epoch 0140: Loss 96086.2656 | AUC 0.9102 | Recall 0.8205 | Precision 0.8205 | AP 0.8566 | F1 0.8205 | Time 1.90\n",
      "Epoch 0141: Loss 96084.2656 | AUC 0.9103 | Recall 0.8205 | Precision 0.8205 | AP 0.8562 | F1 0.8205 | Time 1.92\n",
      "Epoch 0142: Loss 96082.0156 | AUC 0.9104 | Recall 0.8207 | Precision 0.8207 | AP 0.8566 | F1 0.8207 | Time 2.20\n",
      "Epoch 0143: Loss 96080.0312 | AUC 0.9106 | Recall 0.8209 | Precision 0.8209 | AP 0.8569 | F1 0.8209 | Time 2.06\n",
      "Epoch 0144: Loss 96077.8906 | AUC 0.9107 | Recall 0.8212 | Precision 0.8212 | AP 0.8565 | F1 0.8212 | Time 1.78\n",
      "Epoch 0145: Loss 96075.6172 | AUC 0.9109 | Recall 0.8212 | Precision 0.8212 | AP 0.8568 | F1 0.8212 | Time 1.80\n",
      "Epoch 0146: Loss 96073.5156 | AUC 0.9110 | Recall 0.8214 | Precision 0.8214 | AP 0.8571 | F1 0.8214 | Time 2.15\n",
      "Epoch 0147: Loss 96071.2969 | AUC 0.9111 | Recall 0.8214 | Precision 0.8214 | AP 0.8567 | F1 0.8214 | Time 2.36\n",
      "Epoch 0148: Loss 96068.9219 | AUC 0.9113 | Recall 0.8218 | Precision 0.8218 | AP 0.8571 | F1 0.8218 | Time 1.94\n",
      "Epoch 0149: Loss 96066.6250 | AUC 0.9115 | Recall 0.8218 | Precision 0.8218 | AP 0.8571 | F1 0.8218 | Time 1.94\n",
      "Test: Loss 14.7749 | AUC 0.9112 | Recall 0.8520 | Precision 0.8520 | AP 0.8973 | F1 0.8520 | Time 0.51\n",
      "tensor([0, 1]) tensor([5169, 4356])\n",
      "F1 score:  tensor(0.8520)\n",
      "Precision:  tensor(0.8520)\n",
      "Recall:  tensor(0.8520)\n",
      "F1 score pygod:  0.8518261673601479\n"
     ]
    }
   ],
   "source": [
    "gae_model, graph_test = make_gae_model(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n",
    "f1_score_pygod, precision_score, recall_score, f1_score = predict_gae(label_test, gae_model, graph_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conad_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features,\n",
    "                        label_test):\n",
    "    \n",
    "    # node_features = torch.tensor(node_features)\n",
    "    # labels = torch.tensor(labels)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train.cpu()\n",
    "    pyG_train.x = train_node_features.cpu()\n",
    "    label_train = label_train.cpu()\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    conad_model = CONAD(hid_dim=10, num_layers=16, \n",
    "                        lr=0.001, weight_decay= 1, contamination=0.37,\n",
    "                        epoch=100, gpu=-1,  \n",
    "                        weight=1, dropout=0.2, verbose=3)\n",
    "    conad_compile = conad_model.fit(pyG_train, label_train)\n",
    "\n",
    "    return conad_compile, pyG_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_conad(label_test, conda_compile, pyG_test):\n",
    "    conad_ip_pred_res, conad_ip_score_res = conda_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear', )\n",
    "    f1_score_pygod = eval_f1(label_test, conad_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, conad_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, conad_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    unique_values, counts = torch.unique(conad_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 score pygod: \", f1_score_pygod)\n",
    "    return f1_score_pygod, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 37.5968 | AUC 0.7805 | Recall 0.6532 | Precision 0.6532 | AP 0.6571 | F1 0.6532 | Time 8.91\n",
      "Epoch 0001: Loss 37.5945 | AUC 0.7805 | Recall 0.6521 | Precision 0.6521 | AP 0.6570 | F1 0.6521 | Time 8.62\n",
      "Epoch 0002: Loss 37.5923 | AUC 0.7808 | Recall 0.6532 | Precision 0.6532 | AP 0.6574 | F1 0.6532 | Time 6.91\n",
      "Epoch 0003: Loss 37.5907 | AUC 0.7807 | Recall 0.6540 | Precision 0.6540 | AP 0.6573 | F1 0.6540 | Time 7.60\n",
      "Epoch 0004: Loss 37.5878 | AUC 0.7810 | Recall 0.6538 | Precision 0.6538 | AP 0.6574 | F1 0.6538 | Time 6.55\n",
      "Epoch 0005: Loss 37.5842 | AUC 0.7817 | Recall 0.6549 | Precision 0.6549 | AP 0.6580 | F1 0.6549 | Time 7.04\n",
      "Epoch 0006: Loss 37.5851 | AUC 0.7819 | Recall 0.6555 | Precision 0.6555 | AP 0.6583 | F1 0.6555 | Time 5.68\n",
      "Epoch 0007: Loss 37.5827 | AUC 0.7820 | Recall 0.6543 | Precision 0.6543 | AP 0.6582 | F1 0.6543 | Time 6.49\n",
      "Epoch 0008: Loss 37.5815 | AUC 0.7821 | Recall 0.6555 | Precision 0.6555 | AP 0.6583 | F1 0.6555 | Time 6.45\n",
      "Epoch 0009: Loss 37.5794 | AUC 0.7824 | Recall 0.6551 | Precision 0.6551 | AP 0.6585 | F1 0.6551 | Time 6.64\n",
      "Epoch 0010: Loss 37.5778 | AUC 0.7827 | Recall 0.6560 | Precision 0.6560 | AP 0.6587 | F1 0.6560 | Time 6.87\n",
      "Epoch 0011: Loss 37.5760 | AUC 0.7834 | Recall 0.6573 | Precision 0.6573 | AP 0.6593 | F1 0.6573 | Time 5.97\n",
      "Epoch 0012: Loss 37.5722 | AUC 0.7832 | Recall 0.6577 | Precision 0.6577 | AP 0.6592 | F1 0.6577 | Time 5.86\n",
      "Epoch 0013: Loss 37.5707 | AUC 0.7832 | Recall 0.6558 | Precision 0.6558 | AP 0.6591 | F1 0.6558 | Time 5.59\n",
      "Epoch 0014: Loss 37.5694 | AUC 0.7829 | Recall 0.6566 | Precision 0.6566 | AP 0.6589 | F1 0.6566 | Time 6.50\n",
      "Epoch 0015: Loss 37.5677 | AUC 0.7845 | Recall 0.6579 | Precision 0.6579 | AP 0.6602 | F1 0.6579 | Time 5.42\n",
      "Epoch 0016: Loss 37.5657 | AUC 0.7843 | Recall 0.6579 | Precision 0.6579 | AP 0.6599 | F1 0.6579 | Time 5.80\n",
      "Epoch 0017: Loss 37.5632 | AUC 0.7833 | Recall 0.6560 | Precision 0.6560 | AP 0.6592 | F1 0.6560 | Time 5.80\n",
      "Epoch 0018: Loss 37.5619 | AUC 0.7842 | Recall 0.6573 | Precision 0.6573 | AP 0.6599 | F1 0.6573 | Time 6.36\n",
      "Epoch 0019: Loss 37.5612 | AUC 0.7848 | Recall 0.6586 | Precision 0.6586 | AP 0.6604 | F1 0.6586 | Time 7.39\n",
      "Epoch 0020: Loss 37.5590 | AUC 0.7847 | Recall 0.6581 | Precision 0.6581 | AP 0.6602 | F1 0.6581 | Time 6.72\n",
      "Epoch 0021: Loss 37.5566 | AUC 0.7841 | Recall 0.6579 | Precision 0.6579 | AP 0.6599 | F1 0.6579 | Time 5.39\n",
      "Epoch 0022: Loss 37.5541 | AUC 0.7841 | Recall 0.6566 | Precision 0.6566 | AP 0.6598 | F1 0.6566 | Time 4.95\n",
      "Epoch 0023: Loss 37.5526 | AUC 0.7856 | Recall 0.6584 | Precision 0.6584 | AP 0.6608 | F1 0.6584 | Time 5.70\n",
      "Epoch 0024: Loss 37.5505 | AUC 0.7844 | Recall 0.6562 | Precision 0.6562 | AP 0.6598 | F1 0.6563 | Time 5.96\n",
      "Epoch 0025: Loss 37.5473 | AUC 0.7853 | Recall 0.6581 | Precision 0.6581 | AP 0.6607 | F1 0.6581 | Time 5.17\n",
      "Epoch 0026: Loss 37.5463 | AUC 0.7856 | Recall 0.6590 | Precision 0.6590 | AP 0.6609 | F1 0.6590 | Time 4.82\n",
      "Epoch 0027: Loss 37.5432 | AUC 0.7852 | Recall 0.6584 | Precision 0.6584 | AP 0.6607 | F1 0.6584 | Time 6.22\n",
      "Epoch 0028: Loss 37.5420 | AUC 0.7861 | Recall 0.6581 | Precision 0.6581 | AP 0.6612 | F1 0.6581 | Time 5.19\n",
      "Epoch 0029: Loss 37.5414 | AUC 0.7858 | Recall 0.6597 | Precision 0.6597 | AP 0.6611 | F1 0.6597 | Time 4.78\n",
      "Epoch 0030: Loss 37.5410 | AUC 0.7861 | Recall 0.6592 | Precision 0.6592 | AP 0.6612 | F1 0.6592 | Time 5.76\n",
      "Epoch 0031: Loss 37.5378 | AUC 0.7862 | Recall 0.6595 | Precision 0.6595 | AP 0.6613 | F1 0.6595 | Time 5.63\n",
      "Epoch 0032: Loss 37.5334 | AUC 0.7856 | Recall 0.6584 | Precision 0.6584 | AP 0.6608 | F1 0.6584 | Time 4.87\n",
      "Epoch 0033: Loss 37.5344 | AUC 0.7858 | Recall 0.6597 | Precision 0.6597 | AP 0.6610 | F1 0.6597 | Time 4.78\n",
      "Epoch 0034: Loss 37.5311 | AUC 0.7863 | Recall 0.6610 | Precision 0.6610 | AP 0.6615 | F1 0.6610 | Time 4.77\n",
      "Epoch 0035: Loss 37.5303 | AUC 0.7858 | Recall 0.6594 | Precision 0.6594 | AP 0.6611 | F1 0.6594 | Time 4.75\n",
      "Epoch 0036: Loss 37.5296 | AUC 0.7861 | Recall 0.6586 | Precision 0.6586 | AP 0.6612 | F1 0.6586 | Time 4.81\n",
      "Epoch 0037: Loss 37.5258 | AUC 0.7859 | Recall 0.6592 | Precision 0.6592 | AP 0.6611 | F1 0.6592 | Time 4.80\n",
      "Epoch 0038: Loss 37.5264 | AUC 0.7859 | Recall 0.6584 | Precision 0.6584 | AP 0.6610 | F1 0.6584 | Time 4.94\n",
      "Epoch 0039: Loss 37.5256 | AUC 0.7866 | Recall 0.6608 | Precision 0.6608 | AP 0.6616 | F1 0.6608 | Time 6.43\n",
      "Epoch 0040: Loss 37.5216 | AUC 0.7862 | Recall 0.6597 | Precision 0.6597 | AP 0.6614 | F1 0.6597 | Time 6.72\n",
      "Epoch 0041: Loss 37.5211 | AUC 0.7857 | Recall 0.6586 | Precision 0.6586 | AP 0.6610 | F1 0.6586 | Time 5.26\n",
      "Epoch 0042: Loss 37.5186 | AUC 0.7862 | Recall 0.6595 | Precision 0.6595 | AP 0.6615 | F1 0.6596 | Time 5.71\n",
      "Epoch 0043: Loss 37.5173 | AUC 0.7861 | Recall 0.6582 | Precision 0.6582 | AP 0.6612 | F1 0.6583 | Time 5.89\n",
      "Epoch 0044: Loss 37.5149 | AUC 0.7865 | Recall 0.6594 | Precision 0.6594 | AP 0.6615 | F1 0.6594 | Time 5.38\n",
      "Epoch 0045: Loss 37.5159 | AUC 0.7860 | Recall 0.6590 | Precision 0.6590 | AP 0.6610 | F1 0.6590 | Time 6.32\n",
      "Epoch 0046: Loss 37.5124 | AUC 0.7866 | Recall 0.6618 | Precision 0.6618 | AP 0.6617 | F1 0.6618 | Time 5.83\n",
      "Epoch 0047: Loss 37.5111 | AUC 0.7866 | Recall 0.6605 | Precision 0.6605 | AP 0.6617 | F1 0.6605 | Time 5.93\n",
      "Epoch 0048: Loss 37.5107 | AUC 0.7871 | Recall 0.6612 | Precision 0.6612 | AP 0.6621 | F1 0.6612 | Time 6.41\n",
      "Epoch 0049: Loss 37.5097 | AUC 0.7864 | Recall 0.6603 | Precision 0.6603 | AP 0.6616 | F1 0.6603 | Time 5.76\n",
      "Epoch 0050: Loss 37.5073 | AUC 0.7862 | Recall 0.6577 | Precision 0.6577 | AP 0.6611 | F1 0.6577 | Time 5.97\n",
      "Epoch 0051: Loss 37.5071 | AUC 0.7863 | Recall 0.6601 | Precision 0.6601 | AP 0.6614 | F1 0.6601 | Time 6.37\n",
      "Epoch 0052: Loss 37.5041 | AUC 0.7859 | Recall 0.6581 | Precision 0.6581 | AP 0.6610 | F1 0.6581 | Time 5.61\n",
      "Epoch 0053: Loss 37.5041 | AUC 0.7863 | Recall 0.6594 | Precision 0.6594 | AP 0.6613 | F1 0.6594 | Time 5.93\n",
      "Epoch 0054: Loss 37.5031 | AUC 0.7866 | Recall 0.6614 | Precision 0.6614 | AP 0.6617 | F1 0.6614 | Time 5.13\n",
      "Epoch 0055: Loss 37.5011 | AUC 0.7871 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 4.85\n",
      "Epoch 0056: Loss 37.4999 | AUC 0.7869 | Recall 0.6614 | Precision 0.6614 | AP 0.6619 | F1 0.6614 | Time 6.89\n",
      "Epoch 0057: Loss 37.4990 | AUC 0.7862 | Recall 0.6599 | Precision 0.6599 | AP 0.6613 | F1 0.6599 | Time 5.65\n",
      "Epoch 0058: Loss 37.4974 | AUC 0.7864 | Recall 0.6594 | Precision 0.6594 | AP 0.6615 | F1 0.6594 | Time 6.51\n",
      "Epoch 0059: Loss 37.4974 | AUC 0.7864 | Recall 0.6597 | Precision 0.6597 | AP 0.6614 | F1 0.6597 | Time 5.51\n",
      "Epoch 0060: Loss 37.4953 | AUC 0.7863 | Recall 0.6601 | Precision 0.6601 | AP 0.6614 | F1 0.6601 | Time 5.55\n",
      "Epoch 0061: Loss 37.4937 | AUC 0.7861 | Recall 0.6579 | Precision 0.6579 | AP 0.6611 | F1 0.6579 | Time 4.85\n",
      "Epoch 0062: Loss 37.4926 | AUC 0.7866 | Recall 0.6599 | Precision 0.6599 | AP 0.6617 | F1 0.6599 | Time 6.03\n",
      "Epoch 0063: Loss 37.4919 | AUC 0.7864 | Recall 0.6605 | Precision 0.6605 | AP 0.6613 | F1 0.6605 | Time 5.85\n",
      "Epoch 0064: Loss 37.4909 | AUC 0.7867 | Recall 0.6597 | Precision 0.6597 | AP 0.6617 | F1 0.6597 | Time 5.82\n",
      "Epoch 0065: Loss 37.4901 | AUC 0.7863 | Recall 0.6584 | Precision 0.6584 | AP 0.6612 | F1 0.6584 | Time 5.95\n",
      "Epoch 0066: Loss 37.4902 | AUC 0.7863 | Recall 0.6607 | Precision 0.6607 | AP 0.6614 | F1 0.6607 | Time 6.23\n",
      "Epoch 0067: Loss 37.4883 | AUC 0.7877 | Recall 0.6634 | Precision 0.6634 | AP 0.6627 | F1 0.6634 | Time 5.52\n",
      "Epoch 0068: Loss 37.4894 | AUC 0.7860 | Recall 0.6581 | Precision 0.6581 | AP 0.6610 | F1 0.6581 | Time 6.21\n",
      "Epoch 0069: Loss 37.4878 | AUC 0.7873 | Recall 0.6618 | Precision 0.6618 | AP 0.6622 | F1 0.6618 | Time 6.60\n",
      "Epoch 0070: Loss 37.4870 | AUC 0.7865 | Recall 0.6605 | Precision 0.6605 | AP 0.6616 | F1 0.6605 | Time 5.39\n",
      "Epoch 0071: Loss 37.4852 | AUC 0.7864 | Recall 0.6592 | Precision 0.6592 | AP 0.6614 | F1 0.6592 | Time 5.99\n",
      "Epoch 0072: Loss 37.4838 | AUC 0.7865 | Recall 0.6584 | Precision 0.6584 | AP 0.6614 | F1 0.6584 | Time 6.08\n",
      "Epoch 0073: Loss 37.4825 | AUC 0.7867 | Recall 0.6590 | Precision 0.6590 | AP 0.6616 | F1 0.6590 | Time 5.27\n",
      "Epoch 0074: Loss 37.4842 | AUC 0.7866 | Recall 0.6601 | Precision 0.6601 | AP 0.6616 | F1 0.6601 | Time 6.17\n",
      "Epoch 0075: Loss 37.4810 | AUC 0.7868 | Recall 0.6599 | Precision 0.6599 | AP 0.6617 | F1 0.6599 | Time 5.79\n",
      "Epoch 0076: Loss 37.4807 | AUC 0.7866 | Recall 0.6608 | Precision 0.6608 | AP 0.6617 | F1 0.6608 | Time 5.46\n",
      "Epoch 0077: Loss 37.4819 | AUC 0.7870 | Recall 0.6586 | Precision 0.6586 | AP 0.6618 | F1 0.6586 | Time 6.38\n",
      "Epoch 0078: Loss 37.4813 | AUC 0.7859 | Recall 0.6579 | Precision 0.6579 | AP 0.6609 | F1 0.6579 | Time 5.72\n",
      "Epoch 0079: Loss 37.4800 | AUC 0.7864 | Recall 0.6582 | Precision 0.6582 | AP 0.6615 | F1 0.6582 | Time 6.01\n",
      "Epoch 0080: Loss 37.4784 | AUC 0.7871 | Recall 0.6607 | Precision 0.6607 | AP 0.6620 | F1 0.6607 | Time 6.28\n",
      "Epoch 0081: Loss 37.4788 | AUC 0.7861 | Recall 0.6582 | Precision 0.6582 | AP 0.6611 | F1 0.6582 | Time 5.40\n",
      "Epoch 0082: Loss 37.4771 | AUC 0.7860 | Recall 0.6584 | Precision 0.6584 | AP 0.6610 | F1 0.6584 | Time 6.25\n",
      "Epoch 0083: Loss 37.4775 | AUC 0.7862 | Recall 0.6569 | Precision 0.6569 | AP 0.6612 | F1 0.6569 | Time 5.78\n",
      "Epoch 0084: Loss 37.4771 | AUC 0.7870 | Recall 0.6605 | Precision 0.6605 | AP 0.6619 | F1 0.6605 | Time 5.56\n",
      "Epoch 0085: Loss 37.4750 | AUC 0.7872 | Recall 0.6614 | Precision 0.6614 | AP 0.6622 | F1 0.6614 | Time 6.05\n",
      "Epoch 0086: Loss 37.4750 | AUC 0.7866 | Recall 0.6594 | Precision 0.6594 | AP 0.6616 | F1 0.6594 | Time 6.77\n",
      "Epoch 0087: Loss 37.4743 | AUC 0.7873 | Recall 0.6612 | Precision 0.6612 | AP 0.6622 | F1 0.6612 | Time 6.06\n",
      "Epoch 0088: Loss 37.4734 | AUC 0.7864 | Recall 0.6601 | Precision 0.6601 | AP 0.6615 | F1 0.6601 | Time 6.80\n",
      "Epoch 0089: Loss 37.4741 | AUC 0.7866 | Recall 0.6594 | Precision 0.6594 | AP 0.6616 | F1 0.6594 | Time 5.71\n",
      "Epoch 0090: Loss 37.4731 | AUC 0.7867 | Recall 0.6592 | Precision 0.6592 | AP 0.6616 | F1 0.6592 | Time 5.73\n",
      "Epoch 0091: Loss 37.4747 | AUC 0.7863 | Recall 0.6582 | Precision 0.6582 | AP 0.6613 | F1 0.6582 | Time 5.85\n",
      "Epoch 0092: Loss 37.4719 | AUC 0.7861 | Recall 0.6584 | Precision 0.6584 | AP 0.6613 | F1 0.6585 | Time 5.61\n",
      "Epoch 0093: Loss 37.4732 | AUC 0.7867 | Recall 0.6597 | Precision 0.6597 | AP 0.6616 | F1 0.6597 | Time 6.58\n",
      "Epoch 0094: Loss 37.4723 | AUC 0.7862 | Recall 0.6584 | Precision 0.6584 | AP 0.6612 | F1 0.6584 | Time 6.88\n",
      "Epoch 0095: Loss 37.4717 | AUC 0.7872 | Recall 0.6618 | Precision 0.6618 | AP 0.6621 | F1 0.6618 | Time 6.03\n",
      "Epoch 0096: Loss 37.4710 | AUC 0.7868 | Recall 0.6608 | Precision 0.6608 | AP 0.6618 | F1 0.6608 | Time 5.66\n",
      "Epoch 0097: Loss 37.4703 | AUC 0.7863 | Recall 0.6584 | Precision 0.6584 | AP 0.6613 | F1 0.6584 | Time 5.77\n",
      "Epoch 0098: Loss 37.4709 | AUC 0.7866 | Recall 0.6599 | Precision 0.6599 | AP 0.6616 | F1 0.6599 | Time 5.47\n",
      "Epoch 0099: Loss 37.4719 | AUC 0.7865 | Recall 0.6590 | Precision 0.6590 | AP 0.6614 | F1 0.6590 | Time 6.32\n",
      "Test: Loss 0.0098 | AUC 0.7915 | Recall 0.6688 | Precision 0.6688 | AP 0.7448 | F1 0.6731 | Time 0.66\n",
      "tensor([0, 1]) tensor([6329, 3196])\n",
      "F1 score:  tensor(0.6688)\n",
      "Precision:  tensor(0.6688)\n",
      "Recall:  tensor(0.6688)\n",
      "F1 score pygod:  0.7549386011745863\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conad_model, graph_test = make_conad_model(train_graph, train_node_features, label_train,\n",
    "                                            test_graph, test_node_features, label_test)\n",
    "precision_score, recall_score, f1_score = predict_conad(label_test, conad_model, graph_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnomalyDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_anomalydae_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features,\n",
    "                        label_test):\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "    label_train = label_train\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    anomalydae_model = AnomalyDAE(hid_dim=12, emb_dim=4, \n",
    "                        lr=0.001, contamination=0.37,\n",
    "                        epoch=100, gpu=0,\n",
    "                        weight=1, verbose=3)\n",
    "    anomalydae_compile = anomalydae_model.fit(pyG_train, label_train)\n",
    "\n",
    "    return anomalydae_compile, pyG_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_anomalydae(label_test, anomalydae_compile, pyG_test):\n",
    "    anomalydae_ip_pred_res, anomalydae_ip_score_res = anomalydae_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear', )\n",
    "    f1_score_pygod = eval_f1(label_test, anomalydae_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, anomalydae_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, anomalydae_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    unique_values, counts = torch.unique(anomalydae_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 score pygod: \", f1_score_pygod)\n",
    "    return f1_score_pygod, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 6808.8037 | AUC 0.8459 | Recall 0.6921 | Precision 0.6921 | AP 0.7182 | F1 0.6921 | Time 0.97\n",
      "Epoch 0001: Loss 146.2048 | AUC 0.8468 | Recall 0.6967 | Precision 0.6967 | AP 0.7175 | F1 0.6967 | Time 0.68\n",
      "Epoch 0002: Loss 146.1078 | AUC 0.8468 | Recall 0.6967 | Precision 0.6967 | AP 0.7175 | F1 0.6967 | Time 8.99\n",
      "Epoch 0003: Loss 145.8769 | AUC 0.8468 | Recall 0.6967 | Precision 0.6967 | AP 0.7175 | F1 0.6967 | Time 8.63\n",
      "Epoch 0004: Loss 145.5796 | AUC 0.8468 | Recall 0.6967 | Precision 0.6967 | AP 0.7176 | F1 0.6967 | Time 8.81\n",
      "Epoch 0005: Loss 145.2428 | AUC 0.8469 | Recall 0.6967 | Precision 0.6967 | AP 0.7177 | F1 0.6967 | Time 8.70\n",
      "Epoch 0006: Loss 144.8798 | AUC 0.8469 | Recall 0.6967 | Precision 0.6967 | AP 0.7178 | F1 0.6967 | Time 8.75\n",
      "Epoch 0007: Loss 144.4981 | AUC 0.8467 | Recall 0.6967 | Precision 0.6967 | AP 0.7178 | F1 0.6967 | Time 8.73\n",
      "Epoch 0008: Loss 144.1026 | AUC 0.8467 | Recall 0.6965 | Precision 0.6965 | AP 0.7179 | F1 0.6965 | Time 8.77\n",
      "Epoch 0009: Loss 143.6964 | AUC 0.8466 | Recall 0.6965 | Precision 0.6965 | AP 0.7180 | F1 0.6965 | Time 8.54\n",
      "Epoch 0010: Loss 143.2817 | AUC 0.8466 | Recall 0.6963 | Precision 0.6963 | AP 0.7181 | F1 0.6963 | Time 8.64\n",
      "Epoch 0011: Loss 142.8601 | AUC 0.8466 | Recall 0.6962 | Precision 0.6962 | AP 0.7182 | F1 0.6962 | Time 8.45\n",
      "Epoch 0012: Loss 142.4328 | AUC 0.8466 | Recall 0.6962 | Precision 0.6962 | AP 0.7183 | F1 0.6962 | Time 7.97\n",
      "Epoch 0013: Loss 142.0007 | AUC 0.8466 | Recall 0.6962 | Precision 0.6962 | AP 0.7185 | F1 0.6962 | Time 8.71\n",
      "Epoch 0014: Loss 141.5647 | AUC 0.8467 | Recall 0.6962 | Precision 0.6962 | AP 0.7187 | F1 0.6962 | Time 8.66\n",
      "Epoch 0015: Loss 141.1252 | AUC 0.8466 | Recall 0.6962 | Precision 0.6962 | AP 0.7187 | F1 0.6962 | Time 8.64\n",
      "Epoch 0016: Loss 140.6828 | AUC 0.8466 | Recall 0.6962 | Precision 0.6962 | AP 0.7188 | F1 0.6962 | Time 8.74\n",
      "Epoch 0017: Loss 140.2378 | AUC 0.8465 | Recall 0.6962 | Precision 0.6962 | AP 0.7188 | F1 0.6962 | Time 8.75\n",
      "Epoch 0018: Loss 139.7905 | AUC 0.8463 | Recall 0.6962 | Precision 0.6962 | AP 0.7188 | F1 0.6962 | Time 8.69\n",
      "Epoch 0019: Loss 139.3413 | AUC 0.8463 | Recall 0.6960 | Precision 0.6960 | AP 0.7189 | F1 0.6960 | Time 8.64\n",
      "Epoch 0020: Loss 138.8904 | AUC 0.8460 | Recall 0.6960 | Precision 0.6960 | AP 0.7188 | F1 0.6960 | Time 8.81\n",
      "Epoch 0021: Loss 138.4380 | AUC 0.8459 | Recall 0.6950 | Precision 0.6950 | AP 0.7187 | F1 0.6950 | Time 8.61\n",
      "Epoch 0022: Loss 137.9842 | AUC 0.8459 | Recall 0.6949 | Precision 0.6949 | AP 0.7188 | F1 0.6949 | Time 8.63\n",
      "Epoch 0023: Loss 137.5292 | AUC 0.8458 | Recall 0.6949 | Precision 0.6949 | AP 0.7189 | F1 0.6949 | Time 8.69\n",
      "Epoch 0024: Loss 137.0732 | AUC 0.8458 | Recall 0.6947 | Precision 0.6947 | AP 0.7190 | F1 0.6947 | Time 8.69\n",
      "Epoch 0025: Loss 136.6162 | AUC 0.8456 | Recall 0.6947 | Precision 0.6947 | AP 0.7189 | F1 0.6947 | Time 8.71\n",
      "Epoch 0026: Loss 136.1584 | AUC 0.8454 | Recall 0.6945 | Precision 0.6945 | AP 0.7189 | F1 0.6945 | Time 8.55\n",
      "Epoch 0027: Loss 135.6998 | AUC 0.8453 | Recall 0.6945 | Precision 0.6945 | AP 0.7189 | F1 0.6944 | Time 8.73\n",
      "Epoch 0028: Loss 135.2407 | AUC 0.8451 | Recall 0.6945 | Precision 0.6945 | AP 0.7189 | F1 0.6945 | Time 8.67\n",
      "Epoch 0029: Loss 134.7809 | AUC 0.8450 | Recall 0.6945 | Precision 0.6945 | AP 0.7190 | F1 0.6945 | Time 8.64\n",
      "Epoch 0030: Loss 134.3206 | AUC 0.8450 | Recall 0.6945 | Precision 0.6945 | AP 0.7191 | F1 0.6945 | Time 8.75\n",
      "Epoch 0031: Loss 133.8599 | AUC 0.8451 | Recall 0.6945 | Precision 0.6945 | AP 0.7194 | F1 0.6945 | Time 8.69\n",
      "Epoch 0032: Loss 133.3988 | AUC 0.8452 | Recall 0.6945 | Precision 0.6945 | AP 0.7196 | F1 0.6945 | Time 8.78\n",
      "Epoch 0033: Loss 132.9373 | AUC 0.8452 | Recall 0.6945 | Precision 0.6945 | AP 0.7198 | F1 0.6945 | Time 8.59\n",
      "Epoch 0034: Loss 132.4755 | AUC 0.8452 | Recall 0.6941 | Precision 0.6941 | AP 0.7199 | F1 0.6941 | Time 8.67\n",
      "Epoch 0035: Loss 132.0135 | AUC 0.8452 | Recall 0.6937 | Precision 0.6937 | AP 0.7201 | F1 0.6937 | Time 8.72\n",
      "Epoch 0036: Loss 131.5513 | AUC 0.8452 | Recall 0.6936 | Precision 0.6936 | AP 0.7203 | F1 0.6936 | Time 8.65\n",
      "Epoch 0037: Loss 131.0889 | AUC 0.8452 | Recall 0.6936 | Precision 0.6936 | AP 0.7204 | F1 0.6936 | Time 8.73\n",
      "Epoch 0038: Loss 130.6263 | AUC 0.8451 | Recall 0.6932 | Precision 0.6932 | AP 0.7205 | F1 0.6932 | Time 8.70\n",
      "Epoch 0039: Loss 130.1636 | AUC 0.8451 | Recall 0.6932 | Precision 0.6932 | AP 0.7206 | F1 0.6932 | Time 8.62\n",
      "Epoch 0040: Loss 129.7008 | AUC 0.8451 | Recall 0.6932 | Precision 0.6932 | AP 0.7207 | F1 0.6932 | Time 8.63\n",
      "Epoch 0041: Loss 129.2379 | AUC 0.8451 | Recall 0.6932 | Precision 0.6932 | AP 0.7208 | F1 0.6932 | Time 8.73\n",
      "Epoch 0042: Loss 128.7750 | AUC 0.8451 | Recall 0.6930 | Precision 0.6930 | AP 0.7210 | F1 0.6930 | Time 8.61\n",
      "Epoch 0043: Loss 128.3120 | AUC 0.8451 | Recall 0.6930 | Precision 0.6930 | AP 0.7211 | F1 0.6930 | Time 8.73\n",
      "Epoch 0044: Loss 127.8490 | AUC 0.8451 | Recall 0.6928 | Precision 0.6928 | AP 0.7212 | F1 0.6927 | Time 8.72\n",
      "Epoch 0045: Loss 127.3861 | AUC 0.8449 | Recall 0.6924 | Precision 0.6924 | AP 0.7212 | F1 0.6924 | Time 8.63\n",
      "Epoch 0046: Loss 126.9231 | AUC 0.8448 | Recall 0.6919 | Precision 0.6919 | AP 0.7213 | F1 0.6919 | Time 8.59\n",
      "Epoch 0047: Loss 126.4602 | AUC 0.8447 | Recall 0.6915 | Precision 0.6915 | AP 0.7214 | F1 0.6915 | Time 8.72\n",
      "Epoch 0048: Loss 125.9974 | AUC 0.8448 | Recall 0.6915 | Precision 0.6915 | AP 0.7215 | F1 0.6915 | Time 8.65\n",
      "Epoch 0049: Loss 125.5346 | AUC 0.8447 | Recall 0.6915 | Precision 0.6915 | AP 0.7217 | F1 0.6914 | Time 8.50\n",
      "Epoch 0050: Loss 125.0719 | AUC 0.8447 | Recall 0.6913 | Precision 0.6913 | AP 0.7219 | F1 0.6913 | Time 8.82\n",
      "Epoch 0051: Loss 124.6092 | AUC 0.8446 | Recall 0.6911 | Precision 0.6911 | AP 0.7220 | F1 0.6911 | Time 8.58\n",
      "Epoch 0052: Loss 124.1467 | AUC 0.8443 | Recall 0.6911 | Precision 0.6911 | AP 0.7222 | F1 0.6911 | Time 8.67\n",
      "Epoch 0053: Loss 123.6842 | AUC 0.8441 | Recall 0.6911 | Precision 0.6911 | AP 0.7223 | F1 0.6910 | Time 8.61\n",
      "Epoch 0054: Loss 123.2219 | AUC 0.8441 | Recall 0.6909 | Precision 0.6909 | AP 0.7225 | F1 0.6909 | Time 8.66\n",
      "Epoch 0055: Loss 122.7597 | AUC 0.8440 | Recall 0.6906 | Precision 0.6906 | AP 0.7226 | F1 0.6906 | Time 8.71\n",
      "Epoch 0056: Loss 122.2977 | AUC 0.8439 | Recall 0.6898 | Precision 0.6898 | AP 0.7228 | F1 0.6898 | Time 8.63\n",
      "Epoch 0057: Loss 121.8357 | AUC 0.8437 | Recall 0.6893 | Precision 0.6893 | AP 0.7228 | F1 0.6893 | Time 8.84\n",
      "Epoch 0058: Loss 121.3739 | AUC 0.8436 | Recall 0.6885 | Precision 0.6885 | AP 0.7228 | F1 0.6885 | Time 8.49\n",
      "Epoch 0059: Loss 120.9122 | AUC 0.8432 | Recall 0.6883 | Precision 0.6883 | AP 0.7226 | F1 0.6883 | Time 8.43\n",
      "Epoch 0060: Loss 120.4507 | AUC 0.8428 | Recall 0.6882 | Precision 0.6882 | AP 0.7224 | F1 0.6882 | Time 8.72\n",
      "Epoch 0061: Loss 119.9893 | AUC 0.8424 | Recall 0.6882 | Precision 0.6882 | AP 0.7221 | F1 0.6882 | Time 8.72\n",
      "Epoch 0062: Loss 119.5281 | AUC 0.8414 | Recall 0.6882 | Precision 0.6882 | AP 0.7215 | F1 0.6882 | Time 8.63\n",
      "Epoch 0063: Loss 119.0670 | AUC 0.8409 | Recall 0.6880 | Precision 0.6880 | AP 0.7213 | F1 0.6880 | Time 8.69\n",
      "Epoch 0064: Loss 118.6061 | AUC 0.8405 | Recall 0.6878 | Precision 0.6878 | AP 0.7211 | F1 0.6878 | Time 8.69\n",
      "Epoch 0065: Loss 118.1453 | AUC 0.8404 | Recall 0.6874 | Precision 0.6874 | AP 0.7212 | F1 0.6874 | Time 8.63\n",
      "Epoch 0066: Loss 117.6848 | AUC 0.8404 | Recall 0.6869 | Precision 0.6869 | AP 0.7214 | F1 0.6869 | Time 8.59\n",
      "Epoch 0067: Loss 117.2243 | AUC 0.8404 | Recall 0.6865 | Precision 0.6865 | AP 0.7216 | F1 0.6865 | Time 8.57\n",
      "Epoch 0068: Loss 116.7640 | AUC 0.8399 | Recall 0.6863 | Precision 0.6863 | AP 0.7215 | F1 0.6863 | Time 8.64\n",
      "Epoch 0069: Loss 116.3039 | AUC 0.8397 | Recall 0.6863 | Precision 0.6863 | AP 0.7217 | F1 0.6862 | Time 8.32\n",
      "Epoch 0070: Loss 115.8440 | AUC 0.8397 | Recall 0.6863 | Precision 0.6863 | AP 0.7221 | F1 0.6863 | Time 8.72\n",
      "Epoch 0071: Loss 115.3842 | AUC 0.8396 | Recall 0.6863 | Precision 0.6863 | AP 0.7225 | F1 0.6863 | Time 8.57\n",
      "Epoch 0072: Loss 114.9246 | AUC 0.8397 | Recall 0.6863 | Precision 0.6863 | AP 0.7231 | F1 0.6862 | Time 8.68\n",
      "Epoch 0073: Loss 114.4651 | AUC 0.8393 | Recall 0.6861 | Precision 0.6861 | AP 0.7233 | F1 0.6861 | Time 8.61\n",
      "Epoch 0074: Loss 114.0058 | AUC 0.8392 | Recall 0.6859 | Precision 0.6859 | AP 0.7237 | F1 0.6859 | Time 8.72\n",
      "Epoch 0075: Loss 113.5467 | AUC 0.8389 | Recall 0.6857 | Precision 0.6857 | AP 0.7238 | F1 0.6857 | Time 8.73\n",
      "Epoch 0076: Loss 113.0877 | AUC 0.8385 | Recall 0.6857 | Precision 0.6857 | AP 0.7238 | F1 0.6856 | Time 8.57\n",
      "Epoch 0077: Loss 112.6289 | AUC 0.8381 | Recall 0.6856 | Precision 0.6856 | AP 0.7239 | F1 0.6856 | Time 8.69\n",
      "Epoch 0078: Loss 112.1702 | AUC 0.8378 | Recall 0.6854 | Precision 0.6854 | AP 0.7239 | F1 0.6853 | Time 8.69\n",
      "Epoch 0079: Loss 111.7117 | AUC 0.8378 | Recall 0.6876 | Precision 0.6876 | AP 0.7241 | F1 0.6877 | Time 8.68\n",
      "Epoch 0080: Loss 111.2533 | AUC 0.8377 | Recall 0.6895 | Precision 0.6895 | AP 0.7243 | F1 0.6893 | Time 8.58\n",
      "Epoch 0081: Loss 110.7950 | AUC 0.8378 | Recall 0.6936 | Precision 0.6936 | AP 0.7246 | F1 0.6936 | Time 8.74\n",
      "Epoch 0082: Loss 110.3369 | AUC 0.8373 | Recall 0.7038 | Precision 0.7038 | AP 0.7246 | F1 0.7053 | Time 8.66\n",
      "Epoch 0083: Loss 109.8790 | AUC 0.8369 | Recall 0.7103 | Precision 0.7103 | AP 0.7246 | F1 0.7102 | Time 8.56\n",
      "Epoch 0084: Loss 109.4212 | AUC 0.8362 | Recall 0.7255 | Precision 0.7255 | AP 0.7243 | F1 0.7267 | Time 8.70\n",
      "Epoch 0085: Loss 108.9635 | AUC 0.8358 | Recall 0.7382 | Precision 0.7382 | AP 0.7243 | F1 0.7380 | Time 8.70\n",
      "Epoch 0086: Loss 108.5059 | AUC 0.8355 | Recall 0.7447 | Precision 0.7447 | AP 0.7242 | F1 0.7447 | Time 8.58\n",
      "Epoch 0087: Loss 108.0485 | AUC 0.8347 | Recall 0.7569 | Precision 0.7569 | AP 0.7239 | F1 0.7568 | Time 8.76\n",
      "Epoch 0088: Loss 107.5912 | AUC 0.8343 | Recall 0.7645 | Precision 0.7645 | AP 0.7240 | F1 0.7647 | Time 8.47\n",
      "Epoch 0089: Loss 107.1340 | AUC 0.8342 | Recall 0.7673 | Precision 0.7673 | AP 0.7240 | F1 0.7673 | Time 8.60\n",
      "Epoch 0090: Loss 106.6769 | AUC 0.8334 | Recall 0.7671 | Precision 0.7671 | AP 0.7230 | F1 0.7671 | Time 8.68\n",
      "Epoch 0091: Loss 106.2199 | AUC 0.8327 | Recall 0.7668 | Precision 0.7668 | AP 0.7218 | F1 0.7668 | Time 8.70\n",
      "Epoch 0092: Loss 105.7631 | AUC 0.8325 | Recall 0.7670 | Precision 0.7670 | AP 0.7208 | F1 0.7670 | Time 8.72\n",
      "Epoch 0093: Loss 105.3063 | AUC 0.8316 | Recall 0.7666 | Precision 0.7666 | AP 0.7191 | F1 0.7665 | Time 8.62\n",
      "Epoch 0094: Loss 104.8496 | AUC 0.8311 | Recall 0.7666 | Precision 0.7666 | AP 0.7175 | F1 0.7666 | Time 8.69\n",
      "Epoch 0095: Loss 104.3930 | AUC 0.8294 | Recall 0.7658 | Precision 0.7658 | AP 0.7152 | F1 0.7658 | Time 8.67\n",
      "Epoch 0096: Loss 103.9365 | AUC 0.8261 | Recall 0.7651 | Precision 0.7651 | AP 0.7082 | F1 0.7651 | Time 8.75\n",
      "Epoch 0097: Loss 103.4801 | AUC 0.8230 | Recall 0.7644 | Precision 0.7644 | AP 0.6997 | F1 0.7646 | Time 8.49\n",
      "Epoch 0098: Loss 103.0237 | AUC 0.8220 | Recall 0.7588 | Precision 0.7588 | AP 0.6976 | F1 0.7608 | Time 8.68\n",
      "Epoch 0099: Loss 102.5674 | AUC 0.8196 | Recall 0.7510 | Precision 0.7510 | AP 0.6940 | F1 0.7560 | Time 8.64\n",
      "Test: Loss 1.7728 | AUC 0.8477 | Recall 0.7945 | Precision 0.7945 | AP 0.7929 | F1 0.7945 | Time 8.18\n",
      "tensor([1]) tensor([9525])\n",
      "F1 score:  tensor(0.7945)\n",
      "Precision:  tensor(0.7945)\n",
      "Recall:  tensor(0.7945)\n",
      "F1 score pygod:  0.6216626872151074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "anomalydae_model, graph_test = make_anomalydae_model(train_graph, train_node_features, label_train,\n",
    "                                            test_graph, test_node_features, label_test)\n",
    "precision_score, recall_score, f1_score = predict_anomalydae(label_test, anomalydae_model, graph_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_guide_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features,\n",
    "                        label_test):\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "    label_train = label_train\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    guide_model = GUIDE(hid_a=64, hid_s=4, num_layers=4,  \n",
    "                             weight_decay=1,alpha=0.5, contamination=0.1, lr=0.001, epoch=100, gpu=0, \n",
    "                             graphlet_size=16, selected_motif=False,\n",
    "                             verbose=3)\n",
    "    guide_compile = guide_model.fit(pyG_train, label_train)\n",
    "\n",
    "    return guide_compile, pyG_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_guide(label_test, guide_compile, pyG_test):\n",
    "    guide_ip_pred_res, guide_ip_score_res = guide_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear', )\n",
    "    f1_score_pygod = eval_f1(label_test, guide_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, guide_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, guide_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    unique_values, counts = torch.unique(guide_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 score pygod: \", f1_score_pygod)\n",
    "    return f1_score_pygod, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "guide_model, graph_test = make_guide_model(train_graph, train_node_features, label_train,\n",
    "                                            test_graph, test_node_features, label_test)\n",
    "precision_score, recall_score, f1_score = predict_guide(label_test, guide_model, graph_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5595794392523366, 0.5607181851267742, 0.5414656558695153]\n",
      "[tensor(0.4510), tensor(0.4510), tensor(0.4510)]\n",
      "[tensor(1.), tensor(1.), tensor(1.)]\n",
      "[tensor(0.6217), tensor(0.6217), tensor(0.6217)]\n"
     ]
    }
   ],
   "source": [
    "print(f1_conad)\n",
    "print(precision_conad)\n",
    "print(recall_conad)\n",
    "print(f1_conad_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1306.3826067447662, 1317.8039546012878, 1331.694186449051]\n",
      "[3.382333993911743, 3.0480153560638428, 3.529951810836792]\n"
     ]
    }
   ],
   "source": [
    "print(train_time_conad)\n",
    "print(test_time_conad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

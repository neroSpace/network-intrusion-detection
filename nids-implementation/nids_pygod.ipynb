{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from pygod.detector import DOMINANT, OCGNN, GUIDE, GAE, GAAN, AnomalyDAE, CONAD\n",
    "from pygod.metric import eval_average_precision, eval_roc_auc, eval_f1, eval_precision_at_k, eval_recall_at_k\n",
    "from pygod.generator import gen_contextual_outlier, gen_structural_outlier\n",
    "import pickle\n",
    "import time\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw_labeled_path = \"C:\\\\Users\\\\asus\\\\Documents\\\\nids-pcap-dataset\\\\unsw_parquet_used_dataset\\\\unsw_labeled.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw = pd.read_parquet(unsw_labeled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 125180 entries, 1 to 490022\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count   Dtype   \n",
      "---  ------            --------------   -----   \n",
      " 0   source_ip         125180 non-null  object  \n",
      " 1   destination_ip    125180 non-null  object  \n",
      " 2   source_port       125180 non-null  object  \n",
      " 3   destination_port  125180 non-null  object  \n",
      " 4   info_message      125180 non-null  object  \n",
      " 5   attack_category   15657 non-null   category\n",
      " 6   is_malware        125180 non-null  int64   \n",
      " 7   source_ip_info    125180 non-null  object  \n",
      " 8   source_port_info  125180 non-null  object  \n",
      " 9   dest_ip_info      125180 non-null  object  \n",
      " 10  dest_port_info    125180 non-null  object  \n",
      " 11  count_benign      125180 non-null  int64   \n",
      " 12  count_malware     125180 non-null  int64   \n",
      "dtypes: category(1), int64(3), object(9)\n",
      "memory usage: 12.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_ip</th>\n",
       "      <th>destination_ip</th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>info_message</th>\n",
       "      <th>attack_category</th>\n",
       "      <th>is_malware</th>\n",
       "      <th>source_ip_info</th>\n",
       "      <th>source_port_info</th>\n",
       "      <th>dest_ip_info</th>\n",
       "      <th>dest_port_info</th>\n",
       "      <th>count_benign</th>\n",
       "      <th>count_malware</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175.45.176.1</td>\n",
       "      <td>149.171.126.18</td>\n",
       "      <td>4657</td>\n",
       "      <td>80</td>\n",
       "      <td>GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>175.45.176.1 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>4657 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>149.171.126.18 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>80 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175.45.176.3</td>\n",
       "      <td>149.171.126.18</td>\n",
       "      <td>32473</td>\n",
       "      <td>80</td>\n",
       "      <td>GET /level/15/exec/-/buffers/assigned/dump HTT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>175.45.176.3 GET /level/15/exec/-/buffers/assi...</td>\n",
       "      <td>32473 GET /level/15/exec/-/buffers/assigned/du...</td>\n",
       "      <td>149.171.126.18 GET /level/15/exec/-/buffers/as...</td>\n",
       "      <td>80 GET /level/15/exec/-/buffers/assigned/dump ...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>175.45.176.0</td>\n",
       "      <td>149.171.126.17</td>\n",
       "      <td>49194</td>\n",
       "      <td>80</td>\n",
       "      <td>GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>175.45.176.0 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>49194 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>149.171.126.17 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>80 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source_ip  destination_ip source_port destination_port   \n",
       "index                                                              \n",
       "1      175.45.176.1  149.171.126.18        4657               80  \\\n",
       "2      175.45.176.3  149.171.126.18       32473               80   \n",
       "6      175.45.176.0  149.171.126.17       49194               80   \n",
       "\n",
       "                                            info_message attack_category   \n",
       "index                                                                      \n",
       "1                               GET /oKmwKoVbq HTTP/1.1              NaN  \\\n",
       "2      GET /level/15/exec/-/buffers/assigned/dump HTT...             NaN   \n",
       "6                               GET eLWfxXSPkc HTTP/1.1              NaN   \n",
       "\n",
       "       is_malware                                     source_ip_info   \n",
       "index                                                                  \n",
       "1               0              175.45.176.1 GET /oKmwKoVbq HTTP/1.1   \\\n",
       "2               1  175.45.176.3 GET /level/15/exec/-/buffers/assi...   \n",
       "6               0              175.45.176.0 GET eLWfxXSPkc HTTP/1.1    \n",
       "\n",
       "                                        source_port_info   \n",
       "index                                                      \n",
       "1                          4657 GET /oKmwKoVbq HTTP/1.1   \\\n",
       "2      32473 GET /level/15/exec/-/buffers/assigned/du...   \n",
       "6                         49194 GET eLWfxXSPkc HTTP/1.1    \n",
       "\n",
       "                                            dest_ip_info   \n",
       "index                                                      \n",
       "1                149.171.126.18 GET /oKmwKoVbq HTTP/1.1   \\\n",
       "2      149.171.126.18 GET /level/15/exec/-/buffers/as...   \n",
       "6                149.171.126.17 GET eLWfxXSPkc HTTP/1.1    \n",
       "\n",
       "                                          dest_port_info  count_benign   \n",
       "index                                                                    \n",
       "1                            80 GET /oKmwKoVbq HTTP/1.1              1  \\\n",
       "2      80 GET /level/15/exec/-/buffers/assigned/dump ...             1   \n",
       "6                            80 GET eLWfxXSPkc HTTP/1.1              1   \n",
       "\n",
       "       count_malware  \n",
       "index                 \n",
       "1                  0  \n",
       "2                  7  \n",
       "6                  0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsw.info()\n",
    "unsw.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, test_size=0.3):\n",
    "    train, test = train_test_split(df, test_size=test_size)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_modeling_1(df):\n",
    "    graph = nx.Graph()\n",
    "    node_features = []\n",
    "    labels = []\n",
    "\n",
    "    for source_port_info in df[\"source_port_info\"].unique():\n",
    "        graph.add_node(source_port_info)\n",
    "        info_message = df[df[\"source_port_info\"] == source_port_info][\"info_message\"].iloc[0]\n",
    "        label = df[df[\"source_port_info\"] == source_port_info][\"is_malware\"].iloc[0]\n",
    "        node_features.append([float(len(info_message))])\n",
    "        labels.append(label)\n",
    "        \n",
    "    for (source_ip), group in df.groupby([\"source_ip\"]):\n",
    "        for i in range(len(group) - 1):\n",
    "            from_node = group.iloc[i][\"source_port_info\"]\n",
    "            to_node = group.iloc[i+1][\"source_port_info\"]\n",
    "            if graph.has_edge(from_node, to_node):\n",
    "                graph[from_node][to_node][\"weight\"] += 1\n",
    "            else:\n",
    "                graph.add_edge(from_node, to_node, weight=1)\n",
    "    return graph, node_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_modeling_2(df):\n",
    "    graph = nx.Graph()\n",
    "    node_features = []\n",
    "    labels = []\n",
    "\n",
    "    for info_message in df[\"info_message\"].unique():\n",
    "        graph.add_node(info_message)\n",
    "        node_features.append([float(len(info_message))])\n",
    "    \n",
    "    for (source_ip), group in df.groupby([\"source_ip\"]):\n",
    "        for i in range(len(group) - 1):\n",
    "            from_node = group.iloc[i][\"info_message\"]\n",
    "            to_node = group.iloc[i+1][\"info_message\"]\n",
    "            if graph.has_edge(from_node, to_node):\n",
    "                graph[from_node][to_node][\"weight\"] += 1\n",
    "            else:\n",
    "                graph.add_edge(from_node, to_node, weight=1)\n",
    "    node_features = torch.tensor(node_features)\n",
    "    labels = torch.tensor(labels)\n",
    "    return graph, node_features, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw['source_port'] = unsw.source_port.astype('int32')\n",
    "unsw['destination_port']= unsw.destination_port.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_train_test(unsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    77233\n",
       "1    10393\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.], dtype=float32), array([1653, 5923], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "nD_train_df = train_df.drop_duplicates(subset=['info_message'], keep='first')\n",
    "label_train = nD_train_df['is_malware'].to_numpy()\n",
    "label_train = torch.tensor(label_train, dtype=torch.float)\n",
    "value_counts = np.unique(label_train, return_counts=True)\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "1    5923\n",
       "0    1653\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nD_train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    33041\n",
       "1      863\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.], dtype=float32), array([ 821, 3060], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "nD_test_df = test_df.drop_duplicates(subset=['info_message'], keep='first')\n",
    "nD_test_df.is_malware.value_counts()\n",
    "label_test = nD_test_df['is_malware'].to_numpy()\n",
    "label_test = torch.tensor(label_test, dtype=torch.float)\n",
    "value_counts = np.unique(label_test, return_counts=True)\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph, train_node_features = graph_modeling_2(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7576\n"
     ]
    }
   ],
   "source": [
    "# number of nodes\n",
    "print(train_graph.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph, test_node_features = graph_modeling_2(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOMINANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyG_train = from_networkx(train_graph)\n",
    "pyG_train.x = train_node_features\n",
    "pyG_test = from_networkx(test_graph)\n",
    "pyG_test.x = test_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_model = DOMINANT(gpu=0, weight=0.02, num_layers=64, hid_dim=64, contamination=0.1, lr=0.001, verbose=3, epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 4.8750 | AUC 0.6216 | Recall 0.7873 | Precision 0.7873 | AP 0.8504 | F1 0.7783 | Time 12.57\n",
      "Epoch 0001: Loss 4.8739 | AUC 0.6213 | Recall 0.7874 | Precision 0.7874 | AP 0.8514 | F1 0.7874 | Time 12.25\n",
      "Epoch 0002: Loss 4.8740 | AUC 0.6214 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.20\n",
      "Epoch 0003: Loss 4.8726 | AUC 0.6213 | Recall 0.7874 | Precision 0.7874 | AP 0.8514 | F1 0.7874 | Time 12.20\n",
      "Epoch 0004: Loss 4.8725 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.24\n",
      "Epoch 0005: Loss 4.8718 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.26\n",
      "Epoch 0006: Loss 4.8707 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.07\n",
      "Epoch 0007: Loss 4.8701 | AUC 0.6214 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 11.98\n",
      "Epoch 0008: Loss 4.8685 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.03\n",
      "Epoch 0009: Loss 4.8667 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 11.99\n",
      "Epoch 0010: Loss 4.8638 | AUC 0.6213 | Recall 0.7878 | Precision 0.7878 | AP 0.8514 | F1 0.7878 | Time 12.04\n",
      "Epoch 0011: Loss 4.8576 | AUC 0.6213 | Recall 0.7878 | Precision 0.7878 | AP 0.8514 | F1 0.7878 | Time 12.25\n",
      "Epoch 0012: Loss 4.8428 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.03\n",
      "Epoch 0013: Loss 4.8026 | AUC 0.6212 | Recall 0.7878 | Precision 0.7878 | AP 0.8514 | F1 0.7878 | Time 12.02\n",
      "Epoch 0014: Loss 4.6857 | AUC 0.6213 | Recall 0.7879 | Precision 0.7879 | AP 0.8514 | F1 0.7879 | Time 12.03\n",
      "Epoch 0015: Loss 4.4057 | AUC 0.6289 | Recall 0.7966 | Precision 0.7966 | AP 0.8542 | F1 0.7966 | Time 12.00\n",
      "Epoch 0016: Loss 4.8356 | AUC 0.5488 | Recall 0.7424 | Precision 0.7424 | AP 0.8477 | F1 0.7424 | Time 12.07\n",
      "Epoch 0017: Loss 4.4996 | AUC 0.6608 | Recall 0.8207 | Precision 0.8207 | AP 0.8691 | F1 0.8207 | Time 12.01\n",
      "Epoch 0018: Loss 4.3969 | AUC 0.6336 | Recall 0.7979 | Precision 0.7979 | AP 0.8554 | F1 0.7979 | Time 12.06\n",
      "Epoch 0019: Loss 4.5416 | AUC 0.6215 | Recall 0.7878 | Precision 0.7878 | AP 0.8517 | F1 0.7878 | Time 12.09\n",
      "Epoch 0020: Loss 4.6080 | AUC 0.6213 | Recall 0.7879 | Precision 0.7879 | AP 0.8515 | F1 0.7879 | Time 12.17\n",
      "Epoch 0021: Loss 4.6224 | AUC 0.6213 | Recall 0.7879 | Precision 0.7879 | AP 0.8515 | F1 0.7879 | Time 12.00\n",
      "Epoch 0022: Loss 4.6002 | AUC 0.6214 | Recall 0.7879 | Precision 0.7879 | AP 0.8515 | F1 0.7879 | Time 12.05\n",
      "Epoch 0023: Loss 4.5374 | AUC 0.6216 | Recall 0.7879 | Precision 0.7879 | AP 0.8518 | F1 0.7879 | Time 12.02\n",
      "Epoch 0024: Loss 4.4230 | AUC 0.6257 | Recall 0.7891 | Precision 0.7891 | AP 0.8533 | F1 0.7891 | Time 12.17\n",
      "Epoch 0025: Loss 4.3936 | AUC 0.7189 | Recall 0.8464 | Precision 0.8464 | AP 0.8785 | F1 0.8464 | Time 12.02\n",
      "Epoch 0026: Loss 4.4965 | AUC 0.6617 | Recall 0.8214 | Precision 0.8214 | AP 0.8693 | F1 0.8214 | Time 12.11\n",
      "Epoch 0027: Loss 4.5142 | AUC 0.6507 | Recall 0.8158 | Precision 0.8158 | AP 0.8675 | F1 0.8158 | Time 11.95\n",
      "Epoch 0028: Loss 4.4295 | AUC 0.7014 | Recall 0.8393 | Precision 0.8393 | AP 0.8756 | F1 0.8393 | Time 12.04\n",
      "Epoch 0029: Loss 4.3826 | AUC 0.6710 | Recall 0.8199 | Precision 0.8199 | AP 0.8654 | F1 0.8199 | Time 12.25\n",
      "Epoch 0030: Loss 4.4365 | AUC 0.6252 | Recall 0.7885 | Precision 0.7885 | AP 0.8527 | F1 0.7885 | Time 12.26\n",
      "Epoch 0031: Loss 4.4700 | AUC 0.6220 | Recall 0.7881 | Precision 0.7881 | AP 0.8521 | F1 0.7881 | Time 12.05\n",
      "Epoch 0032: Loss 4.4495 | AUC 0.6225 | Recall 0.7876 | Precision 0.7876 | AP 0.8524 | F1 0.7876 | Time 12.05\n",
      "Epoch 0033: Loss 4.4020 | AUC 0.6326 | Recall 0.7967 | Precision 0.7967 | AP 0.8553 | F1 0.7967 | Time 12.04\n",
      "Epoch 0034: Loss 4.3800 | AUC 0.7122 | Recall 0.8416 | Precision 0.8416 | AP 0.8771 | F1 0.8416 | Time 12.06\n",
      "Epoch 0035: Loss 4.4211 | AUC 0.7054 | Recall 0.8411 | Precision 0.8411 | AP 0.8763 | F1 0.8411 | Time 11.98\n",
      "Epoch 0036: Loss 4.4330 | AUC 0.7001 | Recall 0.8386 | Precision 0.8386 | AP 0.8753 | F1 0.8386 | Time 12.12\n",
      "Epoch 0037: Loss 4.3988 | AUC 0.7175 | Recall 0.8460 | Precision 0.8460 | AP 0.8781 | F1 0.8460 | Time 12.16\n",
      "Epoch 0038: Loss 4.3781 | AUC 0.6786 | Recall 0.8199 | Precision 0.8199 | AP 0.8680 | F1 0.8199 | Time 12.30\n",
      "Epoch 0039: Loss 4.3973 | AUC 0.6321 | Recall 0.7981 | Precision 0.7981 | AP 0.8550 | F1 0.7981 | Time 12.06\n",
      "Epoch 0040: Loss 4.4130 | AUC 0.6267 | Recall 0.7945 | Precision 0.7945 | AP 0.8537 | F1 0.7945 | Time 12.20\n",
      "Epoch 0041: Loss 4.3977 | AUC 0.6318 | Recall 0.7977 | Precision 0.7977 | AP 0.8550 | F1 0.7977 | Time 12.00\n",
      "Epoch 0042: Loss 4.3782 | AUC 0.6657 | Recall 0.8151 | Precision 0.8151 | AP 0.8643 | F1 0.8151 | Time 12.03\n",
      "Epoch 0043: Loss 4.3839 | AUC 0.7172 | Recall 0.8477 | Precision 0.8477 | AP 0.8781 | F1 0.8477 | Time 11.95\n",
      "Epoch 0044: Loss 4.3984 | AUC 0.7163 | Recall 0.8448 | Precision 0.8448 | AP 0.8781 | F1 0.8448 | Time 11.94\n",
      "Epoch 0045: Loss 4.3927 | AUC 0.7181 | Recall 0.8465 | Precision 0.8465 | AP 0.8784 | F1 0.8465 | Time 11.95\n",
      "Epoch 0046: Loss 4.3785 | AUC 0.7079 | Recall 0.8372 | Precision 0.8372 | AP 0.8761 | F1 0.8372 | Time 12.01\n",
      "Epoch 0047: Loss 4.3792 | AUC 0.6583 | Recall 0.8126 | Precision 0.8126 | AP 0.8622 | F1 0.8126 | Time 11.99\n",
      "Epoch 0048: Loss 4.3893 | AUC 0.6379 | Recall 0.7981 | Precision 0.7981 | AP 0.8565 | F1 0.7981 | Time 11.99\n",
      "Epoch 0049: Loss 4.3887 | AUC 0.6384 | Recall 0.7981 | Precision 0.7981 | AP 0.8566 | F1 0.7981 | Time 11.94\n",
      "Epoch 0050: Loss 4.3784 | AUC 0.6593 | Recall 0.8128 | Precision 0.8128 | AP 0.8625 | F1 0.8128 | Time 12.02\n",
      "Epoch 0051: Loss 4.3762 | AUC 0.7021 | Recall 0.8329 | Precision 0.8329 | AP 0.8747 | F1 0.8329 | Time 11.94\n",
      "Epoch 0052: Loss 4.3834 | AUC 0.7176 | Recall 0.8487 | Precision 0.8487 | AP 0.8784 | F1 0.8487 | Time 11.97\n",
      "Epoch 0053: Loss 4.3839 | AUC 0.7183 | Recall 0.8486 | Precision 0.8486 | AP 0.8785 | F1 0.8486 | Time 11.96\n",
      "Epoch 0054: Loss 4.3772 | AUC 0.7092 | Recall 0.8381 | Precision 0.8381 | AP 0.8764 | F1 0.8381 | Time 12.25\n",
      "Epoch 0055: Loss 4.3749 | AUC 0.6772 | Recall 0.8193 | Precision 0.8193 | AP 0.8677 | F1 0.8193 | Time 12.29\n",
      "Epoch 0056: Loss 4.3794 | AUC 0.6532 | Recall 0.8082 | Precision 0.8082 | AP 0.8609 | F1 0.8082 | Time 12.07\n",
      "Epoch 0057: Loss 4.3798 | AUC 0.6506 | Recall 0.8060 | Precision 0.8060 | AP 0.8601 | F1 0.8060 | Time 12.02\n",
      "Epoch 0058: Loss 4.3758 | AUC 0.6650 | Recall 0.8150 | Precision 0.8150 | AP 0.8642 | F1 0.8150 | Time 12.07\n",
      "Epoch 0059: Loss 4.3739 | AUC 0.6967 | Recall 0.8280 | Precision 0.8280 | AP 0.8732 | F1 0.8280 | Time 12.02\n",
      "Epoch 0060: Loss 4.3768 | AUC 0.7119 | Recall 0.8420 | Precision 0.8420 | AP 0.8772 | F1 0.8420 | Time 12.04\n",
      "Epoch 0061: Loss 4.3770 | AUC 0.7131 | Recall 0.8426 | Precision 0.8426 | AP 0.8775 | F1 0.8426 | Time 11.98\n",
      "Epoch 0062: Loss 4.3738 | AUC 0.7026 | Recall 0.8327 | Precision 0.8327 | AP 0.8748 | F1 0.8327 | Time 12.32\n",
      "Epoch 0063: Loss 4.3730 | AUC 0.6794 | Recall 0.8200 | Precision 0.8200 | AP 0.8686 | F1 0.8200 | Time 12.04\n",
      "Epoch 0064: Loss 4.3743 | AUC 0.6642 | Recall 0.8150 | Precision 0.8150 | AP 0.8641 | F1 0.8150 | Time 12.07\n",
      "Epoch 0065: Loss 4.3739 | AUC 0.6648 | Recall 0.8151 | Precision 0.8151 | AP 0.8643 | F1 0.8151 | Time 12.11\n",
      "Epoch 0066: Loss 4.3722 | AUC 0.6809 | Recall 0.8197 | Precision 0.8197 | AP 0.8691 | F1 0.8197 | Time 12.06\n",
      "Epoch 0067: Loss 4.3732 | AUC 0.7001 | Recall 0.8307 | Precision 0.8307 | AP 0.8742 | F1 0.8307 | Time 12.02\n",
      "Epoch 0068: Loss 4.3735 | AUC 0.7080 | Recall 0.8371 | Precision 0.8371 | AP 0.8765 | F1 0.8371 | Time 12.05\n",
      "Epoch 0069: Loss 4.3727 | AUC 0.7058 | Recall 0.8352 | Precision 0.8352 | AP 0.8758 | F1 0.8352 | Time 12.00\n",
      "Epoch 0070: Loss 4.3713 | AUC 0.6933 | Recall 0.8254 | Precision 0.8254 | AP 0.8726 | F1 0.8254 | Time 12.04\n",
      "Epoch 0071: Loss 4.3725 | AUC 0.6767 | Recall 0.8192 | Precision 0.8192 | AP 0.8678 | F1 0.8192 | Time 12.01\n",
      "Epoch 0072: Loss 4.3726 | AUC 0.6702 | Recall 0.8151 | Precision 0.8151 | AP 0.8662 | F1 0.8151 | Time 12.05\n",
      "Epoch 0073: Loss 4.3750 | AUC 0.6722 | Recall 0.8185 | Precision 0.8185 | AP 0.8664 | F1 0.8185 | Time 11.98\n",
      "Epoch 0074: Loss 4.3708 | AUC 0.6852 | Recall 0.8220 | Precision 0.8220 | AP 0.8703 | F1 0.8220 | Time 12.05\n",
      "Epoch 0075: Loss 4.3939 | AUC 0.6965 | Recall 0.8261 | Precision 0.8261 | AP 0.8738 | F1 0.8261 | Time 11.99\n",
      "Epoch 0076: Loss 4.3879 | AUC 0.6785 | Recall 0.8209 | Precision 0.8209 | AP 0.8677 | F1 0.8209 | Time 12.03\n",
      "Epoch 0077: Loss 4.4196 | AUC 0.6687 | Recall 0.8156 | Precision 0.8156 | AP 0.8648 | F1 0.8156 | Time 12.01\n",
      "Epoch 0078: Loss 4.4241 | AUC 0.6782 | Recall 0.8210 | Precision 0.8210 | AP 0.8675 | F1 0.8210 | Time 12.04\n",
      "Epoch 0079: Loss 4.4033 | AUC 0.6964 | Recall 0.8276 | Precision 0.8276 | AP 0.8726 | F1 0.8276 | Time 12.04\n",
      "Epoch 0080: Loss 4.3908 | AUC 0.7070 | Recall 0.8352 | Precision 0.8352 | AP 0.8753 | F1 0.8352 | Time 12.07\n",
      "Epoch 0081: Loss 4.3846 | AUC 0.7063 | Recall 0.8347 | Precision 0.8347 | AP 0.8752 | F1 0.8347 | Time 12.12\n",
      "Epoch 0082: Loss 4.3784 | AUC 0.6966 | Recall 0.8269 | Precision 0.8269 | AP 0.8730 | F1 0.8269 | Time 12.04\n",
      "Epoch 0083: Loss 4.3784 | AUC 0.6802 | Recall 0.8197 | Precision 0.8197 | AP 0.8686 | F1 0.8197 | Time 12.05\n",
      "Epoch 0084: Loss 4.3916 | AUC 0.6720 | Recall 0.8161 | Precision 0.8161 | AP 0.8664 | F1 0.8161 | Time 12.06\n",
      "Epoch 0085: Loss 4.3886 | AUC 0.6723 | Recall 0.8163 | Precision 0.8163 | AP 0.8665 | F1 0.8163 | Time 12.08\n",
      "Epoch 0086: Loss 4.3784 | AUC 0.6803 | Recall 0.8197 | Precision 0.8197 | AP 0.8687 | F1 0.8197 | Time 12.03\n",
      "Epoch 0087: Loss 4.3767 | AUC 0.6938 | Recall 0.8271 | Precision 0.8271 | AP 0.8723 | F1 0.8271 | Time 12.03\n",
      "Epoch 0088: Loss 4.3789 | AUC 0.7025 | Recall 0.8320 | Precision 0.8320 | AP 0.8745 | F1 0.8320 | Time 12.02\n",
      "Epoch 0089: Loss 4.3804 | AUC 0.7026 | Recall 0.8320 | Precision 0.8320 | AP 0.8744 | F1 0.8320 | Time 12.00\n",
      "Epoch 0090: Loss 4.3806 | AUC 0.6949 | Recall 0.8268 | Precision 0.8268 | AP 0.8723 | F1 0.8268 | Time 12.08\n",
      "Epoch 0091: Loss 4.3807 | AUC 0.6855 | Recall 0.8219 | Precision 0.8219 | AP 0.8696 | F1 0.8219 | Time 11.97\n",
      "Epoch 0092: Loss 4.3811 | AUC 0.6759 | Recall 0.8199 | Precision 0.8199 | AP 0.8669 | F1 0.8199 | Time 12.05\n",
      "Epoch 0093: Loss 4.3808 | AUC 0.6771 | Recall 0.8195 | Precision 0.8195 | AP 0.8673 | F1 0.8195 | Time 12.01\n",
      "Epoch 0094: Loss 4.3801 | AUC 0.6857 | Recall 0.8220 | Precision 0.8220 | AP 0.8699 | F1 0.8220 | Time 12.05\n",
      "Epoch 0095: Loss 4.3801 | AUC 0.6931 | Recall 0.8261 | Precision 0.8261 | AP 0.8721 | F1 0.8260 | Time 11.99\n",
      "Epoch 0096: Loss 4.3802 | AUC 0.6980 | Recall 0.8276 | Precision 0.8276 | AP 0.8734 | F1 0.8276 | Time 12.04\n",
      "Epoch 0097: Loss 4.3799 | AUC 0.6950 | Recall 0.8275 | Precision 0.8275 | AP 0.8726 | F1 0.8275 | Time 11.98\n",
      "Epoch 0098: Loss 4.3794 | AUC 0.6890 | Recall 0.8227 | Precision 0.8227 | AP 0.8710 | F1 0.8227 | Time 12.08\n",
      "Epoch 0099: Loss 4.3788 | AUC 0.6818 | Recall 0.8193 | Precision 0.8193 | AP 0.8689 | F1 0.8193 | Time 12.03\n"
     ]
    }
   ],
   "source": [
    "dominant_compile = dominant_model.fit(pyG_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0021 | "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3881, 1582]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res \u001b[39m=\u001b[39m dominant_compile\u001b[39m.\u001b[39;49mpredict(data\u001b[39m=\u001b[39;49mpyG_test, label \u001b[39m=\u001b[39;49m label_test,return_pred\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_prob\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, prob_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m, return_conf\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:605\u001b[0m, in \u001b[0;36mDeepDetector.predict\u001b[1;34m(self, data, label, return_pred, return_score, return_prob, prob_method, return_conf, return_emb)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[39mif\u001b[39;00m return_emb:\n\u001b[0;32m    603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(DeepDetector, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mpredict(data,\n\u001b[0;32m    606\u001b[0m                                            label,\n\u001b[0;32m    607\u001b[0m                                            return_pred,\n\u001b[0;32m    608\u001b[0m                                            return_score,\n\u001b[0;32m    609\u001b[0m                                            return_prob,\n\u001b[0;32m    610\u001b[0m                                            prob_method,\n\u001b[0;32m    611\u001b[0m                                            return_conf)\n\u001b[0;32m    612\u001b[0m \u001b[39mif\u001b[39;00m return_emb:\n\u001b[0;32m    613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(output) \u001b[39m==\u001b[39m \u001b[39mtuple\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:188\u001b[0m, in \u001b[0;36mDetector.predict\u001b[1;34m(self, data, label, return_pred, return_score, return_prob, prob_method, return_conf)\u001b[0m\n\u001b[0;32m    183\u001b[0m     logger(score\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_score_,\n\u001b[0;32m    184\u001b[0m            target\u001b[39m=\u001b[39mlabel,\n\u001b[0;32m    185\u001b[0m            verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    186\u001b[0m            train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(data, label)\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m return_pred:\n\u001b[0;32m    190\u001b[0m     pred \u001b[39m=\u001b[39m (score \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold_)\u001b[39m.\u001b[39mlong()\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:528\u001b[0m, in \u001b[0;36mDeepDetector.decision_function\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb[node_idx[:batch_size]] \u001b[39m=\u001b[39m \\\n\u001b[0;32m    524\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39memb[:batch_size]\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    526\u001b[0m     outlier_score[node_idx[:batch_size]] \u001b[39m=\u001b[39m score\n\u001b[1;32m--> 528\u001b[0m logger(loss\u001b[39m=\u001b[39;49mloss\u001b[39m.\u001b[39;49mitem() \u001b[39m/\u001b[39;49m data\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    529\u001b[0m        score\u001b[39m=\u001b[39;49moutlier_score,\n\u001b[0;32m    530\u001b[0m        target\u001b[39m=\u001b[39;49mlabel,\n\u001b[0;32m    531\u001b[0m        time\u001b[39m=\u001b[39;49mtime\u001b[39m.\u001b[39;49mtime() \u001b[39m-\u001b[39;49m start_time,\n\u001b[0;32m    532\u001b[0m        verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    533\u001b[0m        train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    534\u001b[0m \u001b[39mreturn\u001b[39;00m outlier_score\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\utils\\utility.py:236\u001b[0m, in \u001b[0;36mlogger\u001b[1;34m(epoch, loss, score, target, time, verbose, train, deep)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    235\u001b[0m     \u001b[39mif\u001b[39;00m target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m         auc \u001b[39m=\u001b[39m eval_roc_auc(target, score)\n\u001b[0;32m    237\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAUC \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(auc), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\metric\\metric.py:33\u001b[0m, in \u001b[0;36meval_roc_auc\u001b[1;34m(label, score)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval_roc_auc\u001b[39m(label, score):\n\u001b[0;32m     16\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    ROC-AUC score for binary classification.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m        Average ROC-AUC score across different labels.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     roc_auc \u001b[39m=\u001b[39m roc_auc_score(y_true\u001b[39m=\u001b[39;49mlabel, y_score\u001b[39m=\u001b[39;49mscore)\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m roc_auc\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:572\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    570\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n\u001b[0;32m    571\u001b[0m     y_true \u001b[39m=\u001b[39m label_binarize(y_true, classes\u001b[39m=\u001b[39mlabels)[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    573\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39;49mmax_fpr),\n\u001b[0;32m    574\u001b[0m         y_true,\n\u001b[0;32m    575\u001b[0m         y_score,\n\u001b[0;32m    576\u001b[0m         average,\n\u001b[0;32m    577\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    578\u001b[0m     )\n\u001b[0;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    581\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39mmax_fpr),\n\u001b[0;32m    582\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    586\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m binary_metric(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     78\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:344\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_true)) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not defined in that case.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m     )\n\u001b[1;32m--> 344\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m max_fpr \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m auc(fpr, tpr)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:992\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_curve\u001b[39m(\n\u001b[0;32m    905\u001b[0m     y_true, y_score, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, drop_intermediate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    906\u001b[0m ):\n\u001b[0;32m    907\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \n\u001b[0;32m    909\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[39m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[0;32m    993\u001b[0m         y_true, y_score, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    994\u001b[0m     )\n\u001b[0;32m    996\u001b[0m     \u001b[39m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[39m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     \u001b[39m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     \u001b[39m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m     \u001b[39m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m     \u001b[39mif\u001b[39;00m drop_intermediate \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fps) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:751\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m pos_label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m    749\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[1;32m--> 751\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    752\u001b[0m y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n\u001b[0;32m    753\u001b[0m y_score \u001b[39m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3881, 1582]"
     ]
    }
   ],
   "source": [
    "dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res = dominant_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, return_prob=True, prob_method='linear', return_conf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ip = eval_f1(label_test, dominant_ip_pred_res)\n",
    "print(f1_score_ip)\n",
    "precision = eval_precision_at_k(label_test, dominant_ip_score_res, k=1606)\n",
    "print(precision)\n",
    "recall = eval_recall_at_k(label_test, dominant_ip_score_res, k=1606)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominant(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test):\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    model = DOMINANT(gpu=0, weight=0.02, num_layers=64, hid_dim=64, contamination=0.1, lr=0.001, verbose=3, epoch=100)\n",
    "    dominant_compile = model.fit(pyG_train)\n",
    "    dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res = dominant_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, return_prob=True, prob_method='linear', return_conf=True)\n",
    "    f1_score_ip = eval_f1(label_test, dominant_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, dominant_ip_score_res, k=3881)\n",
    "    recall = eval_recall_at_k(label_test, dominant_ip_score_res, k=3881)\n",
    "    return f1_score_ip, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 4.8750 |  | Time 12.97\n",
      "Epoch 0001: Loss 4.8732 |  | Time 12.00\n",
      "Epoch 0002: Loss 4.8745 |  | Time 12.29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m recall \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     f1_score, precision_score, recall_score \u001b[39m=\u001b[39m dominant(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     f1\u001b[39m.\u001b[39mappend(f1_score)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     precision\u001b[39m.\u001b[39mappend(precision_score)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 23\u001b[0m in \u001b[0;36mdominant\u001b[1;34m(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pyG_test\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m test_node_features\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m DOMINANT(gpu\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weight\u001b[39m=\u001b[39m\u001b[39m0.02\u001b[39m, num_layers\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, hid_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, contamination\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, epoch\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dominant_compile \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res \u001b[39m=\u001b[39m dominant_compile\u001b[39m.\u001b[39mpredict(data\u001b[39m=\u001b[39mpyG_test, label \u001b[39m=\u001b[39m label_test,return_pred\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_prob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prob_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m, return_conf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m f1_score_ip \u001b[39m=\u001b[39m eval_f1(label_test, dominant_ip_pred_res)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:465\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 465\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    466\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\dominant.py:161\u001b[0m, in \u001b[0;36mDOMINANT.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    158\u001b[0m s \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39ms\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    159\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 161\u001b[0m x_, s_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, edge_index)\n\u001b[0;32m    163\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    164\u001b[0m                              x_[:batch_size],\n\u001b[0;32m    165\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    166\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    167\u001b[0m                              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n\u001b[0;32m    169\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\nn\\dominant.py:112\u001b[0m, in \u001b[0;36mDOMINANTBase.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mForward computation.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39m    Reconstructed adjacency matrix.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m# encode feature matrix\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared_encoder(x, edge_index)\n\u001b[0;32m    114\u001b[0m \u001b[39m# reconstruct feature matrix\u001b[39;00m\n\u001b[0;32m    115\u001b[0m x_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattr_decoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb, edge_index)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\nn\\models\\basic_gnn.py:222\u001b[0m, in \u001b[0;36mBasicGNN.forward\u001b[1;34m(self, x, edge_index, edge_weight, edge_attr, num_sampled_nodes_per_hop, num_sampled_edges_per_hop)\u001b[0m\n\u001b[0;32m    219\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs[i](x, edge_index, edge_weight\u001b[39m=\u001b[39medge_weight,\n\u001b[0;32m    220\u001b[0m                       edge_attr\u001b[39m=\u001b[39medge_attr)\n\u001b[0;32m    221\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_edge_weight:\n\u001b[1;32m--> 222\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvs[i](x, edge_index, edge_weight\u001b[39m=\u001b[39;49medge_weight)\n\u001b[0;32m    223\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_edge_attr:\n\u001b[0;32m    224\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs[i](x, edge_index, edge_attr\u001b[39m=\u001b[39medge_attr)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    208\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    211\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[0;32m    212\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow, x\u001b[39m.\u001b[39;49mdtype)\n\u001b[0;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[0;32m    214\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:91\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     88\u001b[0m num_nodes \u001b[39m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 91\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m add_remaining_self_loops(\n\u001b[0;32m     92\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m edge_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     edge_weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), ), dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m     96\u001b[0m                              device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\utils\\loop.py:370\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    366\u001b[0m     loop_attr[edge_index[\u001b[39m0\u001b[39m][inv_mask]] \u001b[39m=\u001b[39m edge_attr[inv_mask]\n\u001b[0;32m    368\u001b[0m     edge_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 370\u001b[0m edge_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_index[:, mask], loop_index], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    371\u001b[0m \u001b[39mreturn\u001b[39;00m edge_index, edge_attr\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f1 = []\n",
    "precision = []\n",
    "recall = []\n",
    "for i in range(3):\n",
    "    f1_score, precision_score, recall_score = dominant(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n",
    "    f1.append(f1_score)\n",
    "    precision.append(precision_score)\n",
    "    recall.append(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_train_test(unsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nD_train_df = train_df.drop_duplicates(subset=['source_port_info'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    10070\n",
       "1     5381\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nD_train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = train_df.is_malware.value_counts()\n",
    "if value_counts[0] > 10398:\n",
    "    benign = train_df[train_df['is_malware'] == 0].sample(n=10398)\n",
    "    malicious = train_df[train_df['is_malware'] == 1].sample(n=5699)\n",
    "    train_df = pd.concat([benign, malicious])\n",
    "    # nD_train_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    10398\n",
       "1     5699\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    33097\n",
       "1     4457\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = test_df.is_malware.value_counts()\n",
    "if value_counts[0] > 5321:\n",
    "    df_to_lower = test_df[test_df['is_malware'] == 0].sample(n=5321)\n",
    "    test_df = pd.concat([test_df[test_df['is_malware'] == 1], df_to_lower])\n",
    "    # nD_test_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    5229\n",
       "1    4296\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nD_test_df = test_df.drop_duplicates(subset=['source_port_info'], keep='first')\n",
    "nD_test_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph, train_node_features, label_train = graph_modeling_1(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph, test_node_features, label_test = graph_modeling_1(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(train_graph, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_graph/train_graph.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(train_node_features, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_graph/train_node_features.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(label_train, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_graph/label_train.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_graph' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(train_graph, open('model_graph/train_graph.pkl', 'wb'))\n",
    "pickle.dump(train_node_features, open('model_graph/train_node_features.pkl', 'wb'))\n",
    "pickle.dump(label_train, open('model_graph/label_train.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_graph, open('model_graph/test_graph.pkl', 'wb'))\n",
    "pickle.dump(test_node_features, open('model_graph/test_node_features.pkl', 'wb'))\n",
    "pickle.dump(label_test, open('model_graph/label_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = pickle.load(open('model_graph/train_graph.pkl', 'rb'))\n",
    "label_train = pickle.load(open('model_graph/label_train.pkl', 'rb'))\n",
    "train_node_features = pickle.load(open('model_graph/train_node_features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph = pickle.load(open('model_graph/test_graph.pkl', 'rb'))\n",
    "label_test = pickle.load(open('model_graph/label_test.pkl', 'rb'))\n",
    "test_node_features = pickle.load(open('model_graph/test_node_features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15451"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1]) tensor([5229, 4296])\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = torch.unique(label_test, return_counts=True)\n",
    "print(unique_values, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9525"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22.],\n",
       "        [34.],\n",
       "        [27.],\n",
       "        ...,\n",
       "        [28.],\n",
       "        [15.],\n",
       "        [28.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_node_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOMINANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch_geometric.nn as pyg_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dominant_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features):\n",
    "    \n",
    "    # train_node_features = torch.tensor(train_node_features)\n",
    "    # label_train = torch.tensor(label_train)\n",
    "    # test_node_features = torch.tensor(test_node_features)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    dominant_model = DOMINANT(gpu=0, weight=0.01, num_layers=64, hid_dim=64, backbone=pyg_nn.EdgeCNN, contamination=0.37, lr=0.001, verbose=3, epoch=25)  \n",
    "    dominant_compile = dominant_model.fit(pyG_train, label_train)\n",
    "    return dominant_compile, pyG_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dominant(label_test, dominant_compile, pyG_test):\n",
    "    \n",
    "    dominant_ip_pred_res, dominant_ip_score_res = dominant_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear')\n",
    "    \n",
    "    unique_values, counts = torch.unique(dominant_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "\n",
    "    predictions = dominant_ip_pred_res.numpy()\n",
    "    labels = label_test.numpy()\n",
    "    TP = np.sum((labels == 1) & (predictions == 1))\n",
    "    FN = np.sum((labels == 1) & (predictions == 0))\n",
    "    FP = np.sum((labels == 0) & (predictions == 1))\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score_ip, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 43.0706 | AUC 0.7866 | Recall 0.6608 | Precision 0.6608 | AP 0.6602 | F1 0.7074 | Time 4.50\n",
      "Epoch 0001: Loss 40.1312 | AUC 0.7865 | Recall 0.6527 | Precision 0.6527 | AP 0.6587 | F1 0.7198 | Time 6.31\n",
      "Epoch 0002: Loss 37.4453 | AUC 0.7862 | Recall 0.6534 | Precision 0.6534 | AP 0.6581 | F1 0.7203 | Time 13.44\n",
      "Epoch 0003: Loss 34.9284 | AUC 0.7861 | Recall 0.6610 | Precision 0.6610 | AP 0.6594 | F1 0.7203 | Time 6.36\n",
      "Epoch 0004: Loss 32.5632 | AUC 0.7861 | Recall 0.6534 | Precision 0.6534 | AP 0.6581 | F1 0.7203 | Time 13.37\n",
      "Epoch 0005: Loss 30.3307 | AUC 0.7859 | Recall 0.6612 | Precision 0.6612 | AP 0.6589 | F1 0.7195 | Time 6.38\n",
      "Epoch 0006: Loss 28.2153 | AUC 0.7851 | Recall 0.6595 | Precision 0.6595 | AP 0.6585 | F1 0.7204 | Time 13.34\n",
      "Epoch 0007: Loss 26.2050 | AUC 0.7860 | Recall 0.6543 | Precision 0.6543 | AP 0.6574 | F1 0.7210 | Time 6.30\n",
      "Epoch 0008: Loss 24.2915 | AUC 0.7860 | Recall 0.6543 | Precision 0.6543 | AP 0.6575 | F1 0.7210 | Time 13.45\n",
      "Epoch 0009: Loss 22.4676 | AUC 0.7860 | Recall 0.6543 | Precision 0.6543 | AP 0.6575 | F1 0.7210 | Time 6.33\n",
      "Epoch 0010: Loss 20.7280 | AUC 0.7862 | Recall 0.6555 | Precision 0.6555 | AP 0.6574 | F1 0.7211 | Time 13.41\n",
      "Epoch 0011: Loss 19.0716 | AUC 0.7862 | Recall 0.6555 | Precision 0.6555 | AP 0.6574 | F1 0.7217 | Time 6.32\n",
      "Epoch 0012: Loss 17.4970 | AUC 0.7858 | Recall 0.6636 | Precision 0.6636 | AP 0.6579 | F1 0.7222 | Time 13.39\n",
      "Epoch 0013: Loss 16.0002 | AUC 0.7860 | Recall 0.6655 | Precision 0.6655 | AP 0.6572 | F1 0.7221 | Time 6.27\n",
      "Epoch 0014: Loss 14.5638 | AUC 0.7859 | Recall 0.6694 | Precision 0.6694 | AP 0.6580 | F1 0.7161 | Time 13.35\n",
      "Epoch 0015: Loss 13.2006 | AUC 0.7848 | Recall 0.6625 | Precision 0.6625 | AP 0.6558 | F1 0.7225 | Time 6.20\n",
      "Epoch 0016: Loss 11.9152 | AUC 0.7844 | Recall 0.6636 | Precision 0.6636 | AP 0.6553 | F1 0.6751 | Time 13.36\n",
      "Epoch 0017: Loss 10.7132 | AUC 0.7840 | Recall 0.6588 | Precision 0.6588 | AP 0.6543 | F1 0.7230 | Time 6.23\n",
      "Epoch 0018: Loss 9.5909 | AUC 0.7844 | Recall 0.6595 | Precision 0.6595 | AP 0.6551 | F1 0.7208 | Time 13.38\n",
      "Epoch 0019: Loss 8.5680 | AUC 0.7822 | Recall 0.6590 | Precision 0.6590 | AP 0.6540 | F1 0.7230 | Time 6.25\n",
      "Epoch 0020: Loss 7.6352 | AUC 0.7812 | Recall 0.6590 | Precision 0.6590 | AP 0.6518 | F1 0.7236 | Time 13.41\n",
      "Epoch 0021: Loss 6.7927 | AUC 0.7810 | Recall 0.6592 | Precision 0.6592 | AP 0.6518 | F1 0.7125 | Time 6.21\n",
      "Epoch 0022: Loss 6.0374 | AUC 0.7791 | Recall 0.6588 | Precision 0.6588 | AP 0.6486 | F1 0.7125 | Time 13.34\n",
      "Epoch 0023: Loss 5.3780 | AUC 0.7751 | Recall 0.6592 | Precision 0.6592 | AP 0.6433 | F1 0.7124 | Time 6.25\n",
      "Epoch 0024: Loss 4.8018 | AUC 0.7769 | Recall 0.6608 | Precision 0.6608 | AP 0.6417 | F1 0.7124 | Time 13.50\n",
      "Test: Loss 0.0004 | AUC 0.7871 | Recall 0.6743 | Precision 0.6743 | AP 0.7366 | F1 0.7015 | Time 4.39\n",
      "tensor([0, 1]) tensor([8710,  815])\n",
      "Epoch 0000: Loss 47.5583 | AUC 0.7862 | Recall 0.6588 | Precision 0.6588 | AP 0.6599 | F1 0.7198 | Time 13.94\n",
      "Epoch 0001: Loss 43.1333 | AUC 0.7865 | Recall 0.6527 | Precision 0.6527 | AP 0.6587 | F1 0.7198 | Time 9.55\n",
      "Epoch 0002: Loss 39.0887 | AUC 0.7866 | Recall 0.6530 | Precision 0.6530 | AP 0.6589 | F1 0.7198 | Time 14.54\n",
      "Epoch 0003: Loss 35.3893 | AUC 0.7862 | Recall 0.6534 | Precision 0.6534 | AP 0.6580 | F1 0.7199 | Time 9.56\n",
      "Epoch 0004: Loss 32.0071 | AUC 0.7862 | Recall 0.6534 | Precision 0.6534 | AP 0.6581 | F1 0.7203 | Time 14.56\n",
      "Epoch 0005: Loss 28.9041 | AUC 0.7866 | Recall 0.6543 | Precision 0.6543 | AP 0.6587 | F1 0.7183 | Time 9.56\n",
      "Epoch 0006: Loss 26.0511 | AUC 0.7848 | Recall 0.6566 | Precision 0.6566 | AP 0.6581 | F1 0.6945 | Time 14.54\n",
      "Epoch 0007: Loss 23.4220 | AUC 0.7869 | Recall 0.6636 | Precision 0.6636 | AP 0.6593 | F1 0.7210 | Time 9.78\n",
      "Epoch 0008: Loss 21.0049 | AUC 0.7862 | Recall 0.6555 | Precision 0.6555 | AP 0.6574 | F1 0.7217 | Time 14.39\n",
      "Epoch 0009: Loss 18.7880 | AUC 0.7861 | Recall 0.6620 | Precision 0.6620 | AP 0.6585 | F1 0.6633 | Time 9.56\n",
      "Epoch 0010: Loss 16.7620 | AUC 0.7860 | Recall 0.6657 | Precision 0.6657 | AP 0.6569 | F1 0.7217 | Time 14.55\n",
      "Epoch 0011: Loss 14.9165 | AUC 0.7857 | Recall 0.6640 | Precision 0.6640 | AP 0.6578 | F1 0.7146 | Time 9.62\n",
      "Epoch 0012: Loss 13.2458 | AUC 0.7846 | Recall 0.6646 | Precision 0.6646 | AP 0.6558 | F1 0.6853 | Time 14.53\n",
      "Epoch 0013: Loss 11.7444 | AUC 0.7841 | Recall 0.6597 | Precision 0.6597 | AP 0.6545 | F1 0.6990 | Time 9.55\n",
      "Epoch 0014: Loss 10.3985 | AUC 0.7839 | Recall 0.6581 | Precision 0.6581 | AP 0.6537 | F1 0.7230 | Time 14.68\n",
      "Epoch 0015: Loss 9.2131 | AUC 0.7831 | Recall 0.6582 | Precision 0.6582 | AP 0.6533 | F1 0.7227 | Time 9.66\n",
      "Epoch 0016: Loss 8.1802 | AUC 0.7815 | Recall 0.6584 | Precision 0.6584 | AP 0.6523 | F1 0.7231 | Time 14.69\n",
      "Epoch 0017: Loss 7.2978 | AUC 0.7805 | Recall 0.6644 | Precision 0.6644 | AP 0.6520 | F1 0.6756 | Time 9.59\n",
      "Epoch 0018: Loss 6.5521 | AUC 0.7794 | Recall 0.6590 | Precision 0.6590 | AP 0.6492 | F1 0.7125 | Time 14.47\n",
      "Epoch 0019: Loss 5.9269 | AUC 0.7791 | Recall 0.6623 | Precision 0.6623 | AP 0.6486 | F1 0.6904 | Time 9.95\n",
      "Epoch 0020: Loss 5.3935 | AUC 0.7770 | Recall 0.6655 | Precision 0.6655 | AP 0.6440 | F1 0.7124 | Time 14.88\n",
      "Epoch 0021: Loss 4.9263 | AUC 0.7761 | Recall 0.6614 | Precision 0.6614 | AP 0.6430 | F1 0.6931 | Time 9.51\n",
      "Epoch 0022: Loss 4.5035 | AUC 0.7759 | Recall 0.6634 | Precision 0.6634 | AP 0.6409 | F1 0.7098 | Time 14.48\n",
      "Epoch 0023: Loss 4.1134 | AUC 0.7754 | Recall 0.6631 | Precision 0.6631 | AP 0.6408 | F1 0.6796 | Time 9.69\n",
      "Epoch 0024: Loss 3.7576 | AUC 0.7753 | Recall 0.6666 | Precision 0.6666 | AP 0.6394 | F1 0.7121 | Time 14.57\n",
      "Test: Loss 0.0003 | AUC 0.7882 | Recall 0.6790 | Precision 0.6790 | AP 0.7360 | F1 0.7490 | Time 4.41\n",
      "tensor([0, 1]) tensor([8524, 1001])\n",
      "Epoch 0000: Loss 41.2426 | AUC 0.7862 | Recall 0.6575 | Precision 0.6575 | AP 0.6595 | F1 0.7198 | Time 13.91\n",
      "Epoch 0001: Loss 37.9606 | AUC 0.7864 | Recall 0.6542 | Precision 0.6542 | AP 0.6586 | F1 0.7203 | Time 9.80\n",
      "Epoch 0002: Loss 35.0215 | AUC 0.7875 | Recall 0.6603 | Precision 0.6603 | AP 0.6600 | F1 0.7203 | Time 14.97\n",
      "Epoch 0003: Loss 32.3996 | AUC 0.7859 | Recall 0.6536 | Precision 0.6536 | AP 0.6587 | F1 0.7203 | Time 9.69\n",
      "Epoch 0004: Loss 30.0070 | AUC 0.7859 | Recall 0.6551 | Precision 0.6551 | AP 0.6585 | F1 0.6816 | Time 14.70\n",
      "Epoch 0005: Loss 27.8078 | AUC 0.7860 | Recall 0.6543 | Precision 0.6543 | AP 0.6574 | F1 0.7210 | Time 9.84\n",
      "Epoch 0006: Loss 25.8414 | AUC 0.7860 | Recall 0.6543 | Precision 0.6543 | AP 0.6574 | F1 0.7210 | Time 14.59\n",
      "Epoch 0007: Loss 24.0157 | AUC 0.7861 | Recall 0.6553 | Precision 0.6553 | AP 0.6581 | F1 0.7210 | Time 9.56\n",
      "Epoch 0008: Loss 22.3174 | AUC 0.7860 | Recall 0.6543 | Precision 0.6543 | AP 0.6575 | F1 0.7210 | Time 14.45\n",
      "Epoch 0009: Loss 20.7204 | AUC 0.7862 | Recall 0.6555 | Precision 0.6555 | AP 0.6574 | F1 0.7217 | Time 9.62\n",
      "Epoch 0010: Loss 19.1950 | AUC 0.7862 | Recall 0.6555 | Precision 0.6555 | AP 0.6574 | F1 0.7217 | Time 14.55\n",
      "Epoch 0011: Loss 17.7347 | AUC 0.7860 | Recall 0.6649 | Precision 0.6649 | AP 0.6569 | F1 0.7222 | Time 9.60\n",
      "Epoch 0012: Loss 16.3253 | AUC 0.7860 | Recall 0.6655 | Precision 0.6655 | AP 0.6569 | F1 0.7221 | Time 14.61\n",
      "Epoch 0013: Loss 14.9740 | AUC 0.7848 | Recall 0.6688 | Precision 0.6688 | AP 0.6568 | F1 0.7225 | Time 9.59\n",
      "Epoch 0014: Loss 13.6856 | AUC 0.7861 | Recall 0.6618 | Precision 0.6618 | AP 0.6580 | F1 0.7225 | Time 14.69\n",
      "Epoch 0015: Loss 12.4687 | AUC 0.7840 | Recall 0.6638 | Precision 0.6638 | AP 0.6555 | F1 0.7220 | Time 9.54\n",
      "Epoch 0016: Loss 11.3274 | AUC 0.7841 | Recall 0.6608 | Precision 0.6608 | AP 0.6548 | F1 0.6827 | Time 15.07\n",
      "Epoch 0017: Loss 10.2721 | AUC 0.7839 | Recall 0.6581 | Precision 0.6581 | AP 0.6537 | F1 0.7230 | Time 9.73\n",
      "Epoch 0018: Loss 9.3031 | AUC 0.7832 | Recall 0.6582 | Precision 0.6582 | AP 0.6533 | F1 0.7230 | Time 14.50\n",
      "Epoch 0019: Loss 8.4288 | AUC 0.7819 | Recall 0.6586 | Precision 0.6586 | AP 0.6527 | F1 0.7231 | Time 9.49\n",
      "Epoch 0020: Loss 7.6429 | AUC 0.7812 | Recall 0.6590 | Precision 0.6590 | AP 0.6518 | F1 0.7236 | Time 14.48\n",
      "Epoch 0021: Loss 6.9373 | AUC 0.7796 | Recall 0.6675 | Precision 0.6675 | AP 0.6513 | F1 0.7125 | Time 9.48\n",
      "Epoch 0022: Loss 6.2933 | AUC 0.7796 | Recall 0.6653 | Precision 0.6653 | AP 0.6499 | F1 0.7008 | Time 14.67\n",
      "Epoch 0023: Loss 5.6988 | AUC 0.7783 | Recall 0.6673 | Precision 0.6673 | AP 0.6476 | F1 0.7124 | Time 9.60\n",
      "Epoch 0024: Loss 5.1473 | AUC 0.7767 | Recall 0.6618 | Precision 0.6618 | AP 0.6424 | F1 0.7032 | Time 15.07\n",
      "Test: Loss 0.0004 | AUC 0.7890 | Recall 0.6830 | Precision 0.6830 | AP 0.7369 | F1 0.7490 | Time 4.38\n",
      "tensor([0, 1]) tensor([8736,  789])\n"
     ]
    }
   ],
   "source": [
    "f1 = []\n",
    "precision = []\n",
    "recall = []\n",
    "train_durration = []\n",
    "predict_durration = []\n",
    "f1_score_unv = []\n",
    "for i in range(3):\n",
    "    start_time = time.time()\n",
    "    dominant_model, graph_test = make_dominant_model(train_graph, train_node_features, label_train, test_graph, test_node_features)\n",
    "    make_model_runtime = time.time() - start_time\n",
    "    train_durration.append(make_model_runtime)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    f1_score, precision_score, recall_score, f1_score_for = predict_dominant(label_test, dominant_model, graph_test)\n",
    "    predict_runtime = time.time() - start_time\n",
    "    predict_durration.append(predict_runtime)\n",
    "\n",
    "    f1.append(f1_score)\n",
    "    precision.append(precision_score)\n",
    "    recall.append(recall_score)\n",
    "    f1_score_unv.append(f1_score_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ini f1:  [0.5414656558695153, 0.5414656558695153, 0.5414656558695153]\n",
      "ini precision:  [0.6355828220858896, 0.7032967032967034, 0.623574144486692]\n",
      "ini recall:  [0.12057728119180633, 0.16387337057728119, 0.11452513966480447]\n",
      "ini f1_score_unv:  [0.2027000586969282, 0.2658108363224467, 0.1935103244837758]\n"
     ]
    }
   ],
   "source": [
    "print(\"ini f1: \", f1)\n",
    "print(\"ini precision: \", precision)\n",
    "print(\"ini recall: \", recall)\n",
    "print(\"ini f1_score_unv: \", f1_score_unv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250.85678052902222, 315.7898392677307, 317.1439673900604]\n",
      "[4.691774845123291, 5.131696939468384, 4.681374549865723]\n"
     ]
    }
   ],
   "source": [
    "print(train_durration)\n",
    "print(predict_durration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ocgnn(label_test, ocgnn_compile, pyG_test):\n",
    "    ocgnn_ip_pred_res, ocgnn_ip_score_res, ocgnn_ip_prob_res, ocgnn_ip_conf_res = ocgnn_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, return_prob=True, prob_method='linear', return_conf=True)\n",
    "    precision = eval_precision_at_k(label_test, ocgnn_ip_score_res, k=9525)\n",
    "    recall = eval_recall_at_k(label_test, ocgnn_ip_score_res, k=9525)\n",
    "    f1_score_ip = 2*(precision*recall)/(precision+recall)\n",
    "    return f1_score_ip, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ocgnn_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features,\n",
    "                        label_test):\n",
    "    \n",
    "    node_features = torch.tensor(node_features)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train.cpu()\n",
    "    pyG_train.x = train_node_features.cpu()\n",
    "    label_train = label_train.cpu()\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    ocgnn_model = OCGNN(hid_dim=64, num_layers=64, weight_decay=1, \n",
    "                    contamination=0.37, lr=0.004, dropout=0.3, epoch=100, gpu=-1, \n",
    "                    beta=0.1, eps=0.001, verbose=3)\n",
    "    ocgnn_compile = ocgnn_model.fit(pyG_train, label_train)\n",
    "    return ocgnn_compile, pyG_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 0.0000 | AUC 0.9421 | Recall 0.8220 | Precision 0.8220 | AP 0.9072 | F1 0.8220 | Time 2.92\n",
      "Epoch 0001: Loss 0.0189 | AUC 0.4884 | Recall 0.3457 | Precision 0.3457 | AP 0.3505 | F1 0.3457 | Time 2.89\n",
      "Epoch 0002: Loss 0.0020 | AUC 0.5033 | Recall 0.3563 | Precision 0.3563 | AP 0.3543 | F1 0.3563 | Time 2.88\n",
      "Epoch 0003: Loss 0.0245 | AUC 0.4923 | Recall 0.3507 | Precision 0.3507 | AP 0.3489 | F1 0.3507 | Time 2.88\n",
      "Epoch 0004: Loss 0.0026 | AUC 0.5019 | Recall 0.3548 | Precision 0.3548 | AP 0.3587 | F1 0.3548 | Time 2.84\n",
      "Epoch 0005: Loss 0.0020 | AUC 0.5028 | Recall 0.3484 | Precision 0.3484 | AP 0.3507 | F1 0.3484 | Time 2.92\n",
      "Epoch 0006: Loss 0.0020 | AUC 0.5056 | Recall 0.3460 | Precision 0.3460 | AP 0.3549 | F1 0.3460 | Time 2.94\n",
      "Epoch 0007: Loss 0.0020 | AUC 0.4776 | Recall 0.3299 | Precision 0.3299 | AP 0.3370 | F1 0.3299 | Time 2.94\n",
      "Epoch 0008: Loss 0.0020 | AUC 0.4735 | Recall 0.3241 | Precision 0.3241 | AP 0.3387 | F1 0.3241 | Time 2.88\n",
      "Epoch 0009: Loss 0.0020 | AUC 0.4765 | Recall 0.3234 | Precision 0.3234 | AP 0.3392 | F1 0.3234 | Time 3.01\n",
      "Epoch 0010: Loss 0.0020 | AUC 0.4849 | Recall 0.3332 | Precision 0.3332 | AP 0.3455 | F1 0.3332 | Time 2.94\n",
      "Epoch 0011: Loss 0.0020 | AUC 0.4899 | Recall 0.3338 | Precision 0.3338 | AP 0.3435 | F1 0.3338 | Time 2.95\n",
      "Epoch 0012: Loss 0.0020 | AUC 0.4848 | Recall 0.3237 | Precision 0.3237 | AP 0.3463 | F1 0.3237 | Time 2.99\n",
      "Epoch 0013: Loss 0.0020 | AUC 0.4897 | Recall 0.3269 | Precision 0.3269 | AP 0.3459 | F1 0.3269 | Time 3.17\n",
      "Epoch 0014: Loss 0.0020 | AUC 0.4978 | Recall 0.3457 | Precision 0.3457 | AP 0.3558 | F1 0.3457 | Time 3.14\n",
      "Epoch 0015: Loss 0.0020 | AUC 0.4705 | Recall 0.3243 | Precision 0.3243 | AP 0.3332 | F1 0.3243 | Time 3.11\n",
      "Epoch 0016: Loss 0.0020 | AUC 0.4850 | Recall 0.3306 | Precision 0.3306 | AP 0.3475 | F1 0.3306 | Time 2.94\n",
      "Epoch 0017: Loss 0.0020 | AUC 0.4847 | Recall 0.3360 | Precision 0.3360 | AP 0.3409 | F1 0.3360 | Time 3.10\n",
      "Epoch 0018: Loss 0.0020 | AUC 0.5038 | Recall 0.3533 | Precision 0.3533 | AP 0.3583 | F1 0.3533 | Time 3.70\n",
      "Epoch 0019: Loss 0.0020 | AUC 0.4971 | Recall 0.3460 | Precision 0.3460 | AP 0.3492 | F1 0.3460 | Time 3.39\n",
      "Epoch 0020: Loss 0.0020 | AUC 0.4909 | Recall 0.3395 | Precision 0.3395 | AP 0.3459 | F1 0.3395 | Time 3.38\n",
      "Epoch 0021: Loss 0.0020 | AUC 0.4930 | Recall 0.3497 | Precision 0.3497 | AP 0.3481 | F1 0.3497 | Time 3.51\n",
      "Epoch 0022: Loss 0.0020 | AUC 0.5085 | Recall 0.3553 | Precision 0.3553 | AP 0.3589 | F1 0.3553 | Time 4.30\n",
      "Epoch 0023: Loss 0.0020 | AUC 0.5161 | Recall 0.3576 | Precision 0.3576 | AP 0.3575 | F1 0.3576 | Time 4.05\n",
      "Epoch 0024: Loss 0.0020 | AUC 0.5001 | Recall 0.3492 | Precision 0.3492 | AP 0.3514 | F1 0.3492 | Time 4.05\n",
      "Epoch 0025: Loss 0.0020 | AUC 0.4984 | Recall 0.3527 | Precision 0.3527 | AP 0.3488 | F1 0.3527 | Time 3.83\n",
      "Epoch 0026: Loss 0.0020 | AUC 0.4903 | Recall 0.3405 | Precision 0.3405 | AP 0.3423 | F1 0.3405 | Time 3.71\n",
      "Epoch 0027: Loss 0.0020 | AUC 0.5000 | Recall 0.3453 | Precision 0.3453 | AP 0.3483 | F1 0.3453 | Time 3.79\n",
      "Epoch 0028: Loss 0.0020 | AUC 0.5097 | Recall 0.3624 | Precision 0.3624 | AP 0.3599 | F1 0.3624 | Time 4.10\n",
      "Epoch 0029: Loss 0.0020 | AUC 0.4973 | Recall 0.3406 | Precision 0.3406 | AP 0.3492 | F1 0.3406 | Time 4.15\n",
      "Epoch 0030: Loss 0.0020 | AUC 0.4873 | Recall 0.3388 | Precision 0.3388 | AP 0.3454 | F1 0.3388 | Time 3.65\n",
      "Epoch 0031: Loss 0.0020 | AUC 0.5071 | Recall 0.3507 | Precision 0.3507 | AP 0.3499 | F1 0.3507 | Time 4.61\n",
      "Epoch 0032: Loss 0.0020 | AUC 0.5082 | Recall 0.3555 | Precision 0.3555 | AP 0.3544 | F1 0.3555 | Time 4.02\n",
      "Epoch 0033: Loss 0.0020 | AUC 0.5099 | Recall 0.3622 | Precision 0.3622 | AP 0.3565 | F1 0.3622 | Time 3.10\n",
      "Epoch 0034: Loss 0.0020 | AUC 0.4880 | Recall 0.3401 | Precision 0.3401 | AP 0.3436 | F1 0.3401 | Time 2.93\n",
      "Epoch 0035: Loss 0.0020 | AUC 0.5040 | Recall 0.3460 | Precision 0.3460 | AP 0.3500 | F1 0.3460 | Time 3.22\n",
      "Epoch 0036: Loss 0.0020 | AUC 0.5129 | Recall 0.3609 | Precision 0.3609 | AP 0.3576 | F1 0.3609 | Time 3.02\n",
      "Epoch 0037: Loss 0.0020 | AUC 0.5126 | Recall 0.3587 | Precision 0.3587 | AP 0.3560 | F1 0.3587 | Time 3.34\n",
      "Epoch 0038: Loss 0.0020 | AUC 0.5024 | Recall 0.3512 | Precision 0.3512 | AP 0.3463 | F1 0.3512 | Time 3.20\n",
      "Epoch 0039: Loss 0.0020 | AUC 0.5097 | Recall 0.3512 | Precision 0.3512 | AP 0.3541 | F1 0.3512 | Time 2.93\n",
      "Epoch 0040: Loss 0.0020 | AUC 0.5115 | Recall 0.3594 | Precision 0.3594 | AP 0.3547 | F1 0.3594 | Time 3.05\n",
      "Epoch 0041: Loss 0.0020 | AUC 0.5078 | Recall 0.3602 | Precision 0.3602 | AP 0.3561 | F1 0.3602 | Time 3.58\n",
      "Epoch 0042: Loss 0.0020 | AUC 0.4898 | Recall 0.3366 | Precision 0.3366 | AP 0.3405 | F1 0.3366 | Time 2.88\n",
      "Epoch 0043: Loss 0.0020 | AUC 0.5024 | Recall 0.3568 | Precision 0.3568 | AP 0.3501 | F1 0.3568 | Time 2.80\n",
      "Epoch 0044: Loss 0.0020 | AUC 0.5032 | Recall 0.3468 | Precision 0.3468 | AP 0.3526 | F1 0.3468 | Time 2.91\n",
      "Epoch 0045: Loss 0.0020 | AUC 0.5006 | Recall 0.3514 | Precision 0.3514 | AP 0.3571 | F1 0.3514 | Time 2.89\n",
      "Epoch 0046: Loss 0.0020 | AUC 0.5006 | Recall 0.3458 | Precision 0.3458 | AP 0.3495 | F1 0.3458 | Time 2.97\n",
      "Epoch 0047: Loss 0.0020 | AUC 0.5030 | Recall 0.3514 | Precision 0.3514 | AP 0.3590 | F1 0.3514 | Time 3.17\n",
      "Epoch 0048: Loss 0.0020 | AUC 0.4976 | Recall 0.3460 | Precision 0.3460 | AP 0.3507 | F1 0.3460 | Time 3.18\n",
      "Epoch 0049: Loss 0.0020 | AUC 0.5020 | Recall 0.3477 | Precision 0.3477 | AP 0.3564 | F1 0.3477 | Time 2.85\n",
      "Epoch 0050: Loss 0.0020 | AUC 0.4984 | Recall 0.3468 | Precision 0.3468 | AP 0.3500 | F1 0.3468 | Time 3.04\n",
      "Epoch 0051: Loss 0.0020 | AUC 0.4893 | Recall 0.3369 | Precision 0.3369 | AP 0.3426 | F1 0.3369 | Time 3.05\n",
      "Epoch 0052: Loss 0.0020 | AUC 0.4894 | Recall 0.3406 | Precision 0.3406 | AP 0.3404 | F1 0.3406 | Time 3.14\n",
      "Epoch 0053: Loss 0.0020 | AUC 0.4995 | Recall 0.3421 | Precision 0.3421 | AP 0.3517 | F1 0.3421 | Time 3.06\n",
      "Epoch 0054: Loss 0.0020 | AUC 0.4866 | Recall 0.3334 | Precision 0.3334 | AP 0.3409 | F1 0.3334 | Time 3.10\n",
      "Epoch 0055: Loss 0.0020 | AUC 0.4936 | Recall 0.3429 | Precision 0.3429 | AP 0.3499 | F1 0.3429 | Time 3.71\n",
      "Epoch 0056: Loss 0.0020 | AUC 0.4890 | Recall 0.3340 | Precision 0.3340 | AP 0.3465 | F1 0.3340 | Time 4.05\n",
      "Epoch 0057: Loss 0.0020 | AUC 0.4864 | Recall 0.3366 | Precision 0.3366 | AP 0.3405 | F1 0.3366 | Time 3.20\n",
      "Epoch 0058: Loss 0.0020 | AUC 0.4992 | Recall 0.3538 | Precision 0.3538 | AP 0.3553 | F1 0.3539 | Time 3.25\n",
      "Epoch 0059: Loss 0.0020 | AUC 0.4959 | Recall 0.3447 | Precision 0.3447 | AP 0.3523 | F1 0.3447 | Time 3.42\n",
      "Epoch 0060: Loss 0.0020 | AUC 0.5040 | Recall 0.3525 | Precision 0.3525 | AP 0.3560 | F1 0.3525 | Time 3.23\n",
      "Epoch 0061: Loss 0.0020 | AUC 0.4907 | Recall 0.3366 | Precision 0.3366 | AP 0.3471 | F1 0.3366 | Time 3.10\n",
      "Epoch 0062: Loss 0.0020 | AUC 0.4860 | Recall 0.3380 | Precision 0.3380 | AP 0.3405 | F1 0.3380 | Time 3.01\n",
      "Epoch 0063: Loss 0.0020 | AUC 0.4978 | Recall 0.3406 | Precision 0.3406 | AP 0.3511 | F1 0.3405 | Time 3.02\n",
      "Epoch 0064: Loss 0.0020 | AUC 0.4900 | Recall 0.3405 | Precision 0.3405 | AP 0.3506 | F1 0.3405 | Time 3.23\n",
      "Epoch 0065: Loss 0.0020 | AUC 0.4890 | Recall 0.3379 | Precision 0.3379 | AP 0.3447 | F1 0.3379 | Time 3.00\n",
      "Epoch 0066: Loss 0.0020 | AUC 0.4882 | Recall 0.3358 | Precision 0.3358 | AP 0.3413 | F1 0.3358 | Time 3.01\n",
      "Epoch 0067: Loss 0.0020 | AUC 0.5122 | Recall 0.3572 | Precision 0.3572 | AP 0.3611 | F1 0.3572 | Time 3.02\n",
      "Epoch 0068: Loss 0.0020 | AUC 0.4707 | Recall 0.3202 | Precision 0.3202 | AP 0.3338 | F1 0.3201 | Time 3.06\n",
      "Epoch 0069: Loss 0.0020 | AUC 0.4958 | Recall 0.3425 | Precision 0.3425 | AP 0.3492 | F1 0.3425 | Time 3.08\n",
      "Epoch 0070: Loss 0.0020 | AUC 0.4970 | Recall 0.3408 | Precision 0.3408 | AP 0.3468 | F1 0.3408 | Time 3.06\n",
      "Epoch 0071: Loss 0.0020 | AUC 0.4915 | Recall 0.3377 | Precision 0.3377 | AP 0.3460 | F1 0.3377 | Time 3.02\n",
      "Epoch 0072: Loss 0.0020 | AUC 0.4897 | Recall 0.3347 | Precision 0.3347 | AP 0.3459 | F1 0.3347 | Time 3.14\n",
      "Epoch 0073: Loss 0.0020 | AUC 0.4901 | Recall 0.3406 | Precision 0.3406 | AP 0.3485 | F1 0.3406 | Time 3.03\n",
      "Epoch 0074: Loss 0.0020 | AUC 0.4951 | Recall 0.3486 | Precision 0.3486 | AP 0.3460 | F1 0.3487 | Time 2.94\n",
      "Epoch 0075: Loss 0.0020 | AUC 0.4925 | Recall 0.3432 | Precision 0.3432 | AP 0.3473 | F1 0.3432 | Time 2.96\n",
      "Epoch 0076: Loss 0.0020 | AUC 0.4938 | Recall 0.3373 | Precision 0.3373 | AP 0.3461 | F1 0.3374 | Time 2.96\n",
      "Epoch 0077: Loss 0.0020 | AUC 0.5003 | Recall 0.3527 | Precision 0.3527 | AP 0.3536 | F1 0.3526 | Time 2.94\n",
      "Epoch 0078: Loss 0.0020 | AUC 0.4959 | Recall 0.3390 | Precision 0.3390 | AP 0.3485 | F1 0.3390 | Time 3.16\n",
      "Epoch 0079: Loss 0.0020 | AUC 0.5033 | Recall 0.3458 | Precision 0.3458 | AP 0.3553 | F1 0.3457 | Time 3.09\n",
      "Epoch 0080: Loss 0.0020 | AUC 0.4895 | Recall 0.3418 | Precision 0.3418 | AP 0.3445 | F1 0.3418 | Time 3.11\n",
      "Epoch 0081: Loss 0.0020 | AUC 0.4952 | Recall 0.3418 | Precision 0.3418 | AP 0.3456 | F1 0.3418 | Time 2.98\n",
      "Epoch 0082: Loss 0.0020 | AUC 0.5047 | Recall 0.3453 | Precision 0.3453 | AP 0.3530 | F1 0.3453 | Time 2.99\n",
      "Epoch 0083: Loss 0.0020 | AUC 0.4987 | Recall 0.3453 | Precision 0.3453 | AP 0.3470 | F1 0.3453 | Time 3.00\n",
      "Epoch 0084: Loss 0.0020 | AUC 0.4848 | Recall 0.3364 | Precision 0.3364 | AP 0.3403 | F1 0.3363 | Time 3.01\n",
      "Epoch 0085: Loss 0.0020 | AUC 0.4958 | Recall 0.3434 | Precision 0.3434 | AP 0.3432 | F1 0.3432 | Time 2.97\n",
      "Epoch 0086: Loss 0.0020 | AUC 0.4988 | Recall 0.3429 | Precision 0.3429 | AP 0.3436 | F1 0.3428 | Time 2.96\n",
      "Epoch 0087: Loss 0.0020 | AUC 0.5183 | Recall 0.3646 | Precision 0.3646 | AP 0.3623 | F1 0.3647 | Time 3.06\n",
      "Epoch 0088: Loss 0.0020 | AUC 0.5111 | Recall 0.3641 | Precision 0.3641 | AP 0.3604 | F1 0.3635 | Time 2.97\n",
      "Epoch 0089: Loss 0.0020 | AUC 0.4936 | Recall 0.3405 | Precision 0.3405 | AP 0.3434 | F1 0.3403 | Time 3.00\n",
      "Epoch 0090: Loss 0.0020 | AUC 0.4985 | Recall 0.3466 | Precision 0.3466 | AP 0.3451 | F1 0.3467 | Time 2.98\n",
      "Epoch 0091: Loss 0.0020 | AUC 0.5012 | Recall 0.3520 | Precision 0.3520 | AP 0.3499 | F1 0.3517 | Time 2.96\n",
      "Epoch 0092: Loss 0.0020 | AUC 0.5059 | Recall 0.3546 | Precision 0.3546 | AP 0.3513 | F1 0.3543 | Time 3.00\n",
      "Epoch 0093: Loss 0.0020 | AUC 0.4972 | Recall 0.3486 | Precision 0.3486 | AP 0.3470 | F1 0.3485 | Time 2.96\n",
      "Epoch 0094: Loss 0.0020 | AUC 0.4973 | Recall 0.3429 | Precision 0.3429 | AP 0.3414 | F1 0.3429 | Time 2.95\n",
      "Epoch 0095: Loss 0.0020 | AUC 0.4975 | Recall 0.3479 | Precision 0.3479 | AP 0.3452 | F1 0.3478 | Time 2.96\n",
      "Epoch 0096: Loss 0.0020 | AUC 0.5012 | Recall 0.3510 | Precision 0.3510 | AP 0.3479 | F1 0.3508 | Time 3.18\n",
      "Epoch 0097: Loss 0.0020 | AUC 0.5029 | Recall 0.3544 | Precision 0.3544 | AP 0.3524 | F1 0.3541 | Time 2.98\n",
      "Epoch 0098: Loss 0.0020 | AUC 0.5215 | Recall 0.3676 | Precision 0.3676 | AP 0.3668 | F1 0.3669 | Time 2.99\n",
      "Epoch 0099: Loss 0.0020 | AUC 0.5199 | Recall 0.3644 | Precision 0.3644 | AP 0.3642 | F1 0.3637 | Time 2.97\n",
      "Test: Loss 0.0000 | AUC 0.5296 | Recall 0.9132 | Precision 0.9132 | AP 0.4959 | F1 0.2159 | Time 0.78\n",
      "Epoch 0000: Loss 0.0000 | AUC 0.9327 | Recall 0.8069 | Precision 0.8069 | AP 0.8858 | F1 0.8069 | Time 3.13\n",
      "Epoch 0001: Loss 0.0112 | AUC 0.4910 | Recall 0.3470 | Precision 0.3470 | AP 0.3532 | F1 0.3470 | Time 3.02\n",
      "Epoch 0002: Loss 0.0012 | AUC 0.4841 | Recall 0.3360 | Precision 0.3360 | AP 0.3421 | F1 0.3360 | Time 3.04\n",
      "Epoch 0003: Loss 0.0269 | AUC 0.4826 | Recall 0.3380 | Precision 0.3380 | AP 0.3478 | F1 0.3380 | Time 3.39\n",
      "Epoch 0004: Loss 0.0032 | AUC 0.4872 | Recall 0.3336 | Precision 0.3336 | AP 0.3417 | F1 0.3336 | Time 3.32\n",
      "Epoch 0005: Loss 0.0012 | AUC 0.5005 | Recall 0.3475 | Precision 0.3475 | AP 0.3531 | F1 0.3475 | Time 3.46\n",
      "Epoch 0006: Loss 0.0012 | AUC 0.4815 | Recall 0.3362 | Precision 0.3362 | AP 0.3392 | F1 0.3362 | Time 3.30\n",
      "Epoch 0007: Loss 0.0012 | AUC 0.5086 | Recall 0.3483 | Precision 0.3483 | AP 0.3550 | F1 0.3483 | Time 3.31\n",
      "Epoch 0008: Loss 0.0012 | AUC 0.4979 | Recall 0.3525 | Precision 0.3525 | AP 0.3553 | F1 0.3525 | Time 3.60\n",
      "Epoch 0009: Loss 0.0012 | AUC 0.5066 | Recall 0.3557 | Precision 0.3557 | AP 0.3520 | F1 0.3557 | Time 3.10\n",
      "Epoch 0010: Loss 0.0012 | AUC 0.4842 | Recall 0.3366 | Precision 0.3366 | AP 0.3401 | F1 0.3366 | Time 3.04\n",
      "Epoch 0011: Loss 0.0026 | AUC 0.4933 | Recall 0.3384 | Precision 0.3384 | AP 0.3451 | F1 0.3384 | Time 3.12\n",
      "Epoch 0012: Loss 0.0012 | AUC 0.4953 | Recall 0.3436 | Precision 0.3436 | AP 0.3492 | F1 0.3436 | Time 3.10\n",
      "Epoch 0013: Loss 0.0012 | AUC 0.4832 | Recall 0.3362 | Precision 0.3362 | AP 0.3458 | F1 0.3362 | Time 3.04\n",
      "Epoch 0014: Loss 0.0012 | AUC 0.4853 | Recall 0.3358 | Precision 0.3358 | AP 0.3457 | F1 0.3358 | Time 3.02\n",
      "Epoch 0015: Loss 0.0012 | AUC 0.4892 | Recall 0.3392 | Precision 0.3392 | AP 0.3508 | F1 0.3392 | Time 3.18\n",
      "Epoch 0016: Loss 0.0012 | AUC 0.4832 | Recall 0.3358 | Precision 0.3358 | AP 0.3407 | F1 0.3358 | Time 3.14\n",
      "Epoch 0017: Loss 0.0012 | AUC 0.4750 | Recall 0.3297 | Precision 0.3297 | AP 0.3394 | F1 0.3297 | Time 3.30\n",
      "Epoch 0018: Loss 0.0012 | AUC 0.4816 | Recall 0.3354 | Precision 0.3354 | AP 0.3452 | F1 0.3354 | Time 3.35\n",
      "Epoch 0019: Loss 0.0012 | AUC 0.4798 | Recall 0.3338 | Precision 0.3338 | AP 0.3466 | F1 0.3338 | Time 3.20\n",
      "Epoch 0020: Loss 0.0012 | AUC 0.5007 | Recall 0.3537 | Precision 0.3537 | AP 0.3600 | F1 0.3537 | Time 3.04\n",
      "Epoch 0021: Loss 0.0012 | AUC 0.4904 | Recall 0.3464 | Precision 0.3464 | AP 0.3509 | F1 0.3464 | Time 3.05\n",
      "Epoch 0022: Loss 0.0012 | AUC 0.5007 | Recall 0.3548 | Precision 0.3548 | AP 0.3616 | F1 0.3548 | Time 3.01\n",
      "Epoch 0023: Loss 0.0012 | AUC 0.4941 | Recall 0.3397 | Precision 0.3397 | AP 0.3500 | F1 0.3397 | Time 3.19\n",
      "Epoch 0024: Loss 0.0012 | AUC 0.4805 | Recall 0.3299 | Precision 0.3299 | AP 0.3417 | F1 0.3299 | Time 3.06\n",
      "Epoch 0025: Loss 0.0012 | AUC 0.4775 | Recall 0.3302 | Precision 0.3302 | AP 0.3395 | F1 0.3302 | Time 3.00\n",
      "Epoch 0026: Loss 0.0012 | AUC 0.4840 | Recall 0.3406 | Precision 0.3406 | AP 0.3446 | F1 0.3406 | Time 3.00\n",
      "Epoch 0027: Loss 0.0012 | AUC 0.4879 | Recall 0.3406 | Precision 0.3406 | AP 0.3439 | F1 0.3406 | Time 2.99\n",
      "Epoch 0028: Loss 0.0012 | AUC 0.4937 | Recall 0.3423 | Precision 0.3423 | AP 0.3452 | F1 0.3423 | Time 3.05\n",
      "Epoch 0029: Loss 0.0012 | AUC 0.4929 | Recall 0.3457 | Precision 0.3457 | AP 0.3451 | F1 0.3457 | Time 2.99\n",
      "Epoch 0030: Loss 0.0012 | AUC 0.4943 | Recall 0.3432 | Precision 0.3432 | AP 0.3476 | F1 0.3432 | Time 3.05\n",
      "Epoch 0031: Loss 0.0012 | AUC 0.4881 | Recall 0.3341 | Precision 0.3341 | AP 0.3438 | F1 0.3341 | Time 3.13\n",
      "Epoch 0032: Loss 0.0012 | AUC 0.4821 | Recall 0.3379 | Precision 0.3379 | AP 0.3399 | F1 0.3379 | Time 3.03\n",
      "Epoch 0033: Loss 0.0012 | AUC 0.4906 | Recall 0.3388 | Precision 0.3388 | AP 0.3474 | F1 0.3386 | Time 3.20\n",
      "Epoch 0034: Loss 0.0012 | AUC 0.4853 | Recall 0.3377 | Precision 0.3377 | AP 0.3465 | F1 0.3377 | Time 3.02\n",
      "Epoch 0035: Loss 0.0012 | AUC 0.4951 | Recall 0.3419 | Precision 0.3419 | AP 0.3501 | F1 0.3419 | Time 3.09\n",
      "Epoch 0036: Loss 0.0012 | AUC 0.4932 | Recall 0.3466 | Precision 0.3466 | AP 0.3487 | F1 0.3466 | Time 3.00\n",
      "Epoch 0037: Loss 0.0012 | AUC 0.5017 | Recall 0.3516 | Precision 0.3516 | AP 0.3488 | F1 0.3516 | Time 3.01\n",
      "Epoch 0038: Loss 0.0012 | AUC 0.4879 | Recall 0.3382 | Precision 0.3382 | AP 0.3395 | F1 0.3382 | Time 3.65\n",
      "Epoch 0039: Loss 0.0012 | AUC 0.4904 | Recall 0.3416 | Precision 0.3416 | AP 0.3456 | F1 0.3416 | Time 3.85\n",
      "Epoch 0040: Loss 0.0012 | AUC 0.4856 | Recall 0.3328 | Precision 0.3328 | AP 0.3419 | F1 0.3328 | Time 3.57\n",
      "Epoch 0041: Loss 0.0012 | AUC 0.4817 | Recall 0.3323 | Precision 0.3323 | AP 0.3399 | F1 0.3323 | Time 3.59\n",
      "Epoch 0042: Loss 0.0012 | AUC 0.4993 | Recall 0.3444 | Precision 0.3444 | AP 0.3481 | F1 0.3444 | Time 3.92\n",
      "Epoch 0043: Loss 0.0012 | AUC 0.4879 | Recall 0.3371 | Precision 0.3371 | AP 0.3487 | F1 0.3371 | Time 3.80\n",
      "Epoch 0044: Loss 0.0012 | AUC 0.4937 | Recall 0.3445 | Precision 0.3445 | AP 0.3479 | F1 0.3445 | Time 3.68\n",
      "Epoch 0045: Loss 0.0012 | AUC 0.4834 | Recall 0.3297 | Precision 0.3297 | AP 0.3402 | F1 0.3297 | Time 3.45\n",
      "Epoch 0046: Loss 0.0012 | AUC 0.4878 | Recall 0.3351 | Precision 0.3351 | AP 0.3457 | F1 0.3351 | Time 3.57\n",
      "Epoch 0047: Loss 0.0012 | AUC 0.4864 | Recall 0.3304 | Precision 0.3304 | AP 0.3400 | F1 0.3304 | Time 3.03\n",
      "Epoch 0048: Loss 0.0012 | AUC 0.4802 | Recall 0.3345 | Precision 0.3345 | AP 0.3395 | F1 0.3345 | Time 3.01\n",
      "Epoch 0049: Loss 0.0012 | AUC 0.4955 | Recall 0.3416 | Precision 0.3416 | AP 0.3466 | F1 0.3416 | Time 3.01\n",
      "Epoch 0050: Loss 0.0012 | AUC 0.4909 | Recall 0.3449 | Precision 0.3449 | AP 0.3447 | F1 0.3449 | Time 2.99\n",
      "Epoch 0051: Loss 0.0012 | AUC 0.4916 | Recall 0.3367 | Precision 0.3367 | AP 0.3396 | F1 0.3367 | Time 3.04\n",
      "Epoch 0052: Loss 0.0012 | AUC 0.4910 | Recall 0.3425 | Precision 0.3425 | AP 0.3490 | F1 0.3425 | Time 3.08\n",
      "Epoch 0053: Loss 0.0012 | AUC 0.4983 | Recall 0.3494 | Precision 0.3494 | AP 0.3563 | F1 0.3494 | Time 3.03\n",
      "Epoch 0054: Loss 0.0012 | AUC 0.4835 | Recall 0.3395 | Precision 0.3395 | AP 0.3439 | F1 0.3395 | Time 3.21\n",
      "Epoch 0055: Loss 0.0012 | AUC 0.4916 | Recall 0.3470 | Precision 0.3470 | AP 0.3496 | F1 0.3470 | Time 3.03\n",
      "Epoch 0056: Loss 0.0012 | AUC 0.4787 | Recall 0.3358 | Precision 0.3358 | AP 0.3392 | F1 0.3358 | Time 3.13\n",
      "Epoch 0057: Loss 0.0012 | AUC 0.4844 | Recall 0.3351 | Precision 0.3351 | AP 0.3472 | F1 0.3351 | Time 3.15\n",
      "Epoch 0058: Loss 0.0012 | AUC 0.4701 | Recall 0.3280 | Precision 0.3280 | AP 0.3353 | F1 0.3280 | Time 3.02\n",
      "Epoch 0059: Loss 0.0012 | AUC 0.4727 | Recall 0.3269 | Precision 0.3269 | AP 0.3453 | F1 0.3267 | Time 3.05\n",
      "Epoch 0060: Loss 0.0012 | AUC 0.4770 | Recall 0.3332 | Precision 0.3332 | AP 0.3401 | F1 0.3332 | Time 2.95\n",
      "Epoch 0061: Loss 0.0012 | AUC 0.4849 | Recall 0.3436 | Precision 0.3436 | AP 0.3460 | F1 0.3436 | Time 3.41\n",
      "Epoch 0062: Loss 0.0012 | AUC 0.4788 | Recall 0.3304 | Precision 0.3304 | AP 0.3421 | F1 0.3304 | Time 3.09\n",
      "Epoch 0063: Loss 0.0012 | AUC 0.4764 | Recall 0.3332 | Precision 0.3332 | AP 0.3419 | F1 0.3332 | Time 3.03\n",
      "Epoch 0064: Loss 0.0012 | AUC 0.4779 | Recall 0.3343 | Precision 0.3343 | AP 0.3406 | F1 0.3343 | Time 3.03\n",
      "Epoch 0065: Loss 0.0012 | AUC 0.4711 | Recall 0.3224 | Precision 0.3224 | AP 0.3355 | F1 0.3224 | Time 2.98\n",
      "Epoch 0066: Loss 0.0012 | AUC 0.4774 | Recall 0.3364 | Precision 0.3364 | AP 0.3434 | F1 0.3364 | Time 3.01\n",
      "Epoch 0067: Loss 0.0012 | AUC 0.4845 | Recall 0.3423 | Precision 0.3423 | AP 0.3501 | F1 0.3423 | Time 3.03\n",
      "Epoch 0068: Loss 0.0012 | AUC 0.4807 | Recall 0.3449 | Precision 0.3449 | AP 0.3445 | F1 0.3449 | Time 3.02\n",
      "Epoch 0069: Loss 0.0012 | AUC 0.4837 | Recall 0.3408 | Precision 0.3408 | AP 0.3469 | F1 0.3409 | Time 3.16\n",
      "Epoch 0070: Loss 0.0012 | AUC 0.4791 | Recall 0.3284 | Precision 0.3284 | AP 0.3447 | F1 0.3284 | Time 3.03\n",
      "Epoch 0071: Loss 0.0012 | AUC 0.4799 | Recall 0.3310 | Precision 0.3310 | AP 0.3458 | F1 0.3310 | Time 3.03\n",
      "Epoch 0072: Loss 0.0012 | AUC 0.4857 | Recall 0.3392 | Precision 0.3392 | AP 0.3448 | F1 0.3392 | Time 2.99\n",
      "Epoch 0073: Loss 0.0012 | AUC 0.4777 | Recall 0.3280 | Precision 0.3280 | AP 0.3408 | F1 0.3280 | Time 3.01\n",
      "Epoch 0074: Loss 0.0012 | AUC 0.4715 | Recall 0.3295 | Precision 0.3295 | AP 0.3381 | F1 0.3292 | Time 2.96\n",
      "Epoch 0075: Loss 0.0012 | AUC 0.4573 | Recall 0.3104 | Precision 0.3104 | AP 0.3261 | F1 0.3102 | Time 3.00\n",
      "Epoch 0076: Loss 0.0012 | AUC 0.4788 | Recall 0.3325 | Precision 0.3325 | AP 0.3422 | F1 0.3325 | Time 3.04\n",
      "Epoch 0077: Loss 0.0012 | AUC 0.4938 | Recall 0.3432 | Precision 0.3432 | AP 0.3493 | F1 0.3431 | Time 3.02\n",
      "Epoch 0078: Loss 0.0012 | AUC 0.4882 | Recall 0.3384 | Precision 0.3384 | AP 0.3479 | F1 0.3383 | Time 3.08\n",
      "Epoch 0079: Loss 0.0012 | AUC 0.4894 | Recall 0.3405 | Precision 0.3405 | AP 0.3414 | F1 0.3405 | Time 3.04\n",
      "Epoch 0080: Loss 0.0012 | AUC 0.5040 | Recall 0.3484 | Precision 0.3484 | AP 0.3540 | F1 0.3485 | Time 3.01\n",
      "Epoch 0081: Loss 0.0012 | AUC 0.5062 | Recall 0.3529 | Precision 0.3529 | AP 0.3491 | F1 0.3529 | Time 3.01\n",
      "Epoch 0082: Loss 0.0012 | AUC 0.4973 | Recall 0.3512 | Precision 0.3512 | AP 0.3507 | F1 0.3512 | Time 3.20\n",
      "Epoch 0083: Loss 0.0012 | AUC 0.5017 | Recall 0.3468 | Precision 0.3468 | AP 0.3492 | F1 0.3465 | Time 3.16\n",
      "Epoch 0084: Loss 0.0012 | AUC 0.5027 | Recall 0.3494 | Precision 0.3494 | AP 0.3481 | F1 0.3491 | Time 3.21\n",
      "Epoch 0085: Loss 0.0012 | AUC 0.5011 | Recall 0.3462 | Precision 0.3462 | AP 0.3476 | F1 0.3462 | Time 2.98\n",
      "Epoch 0086: Loss 0.0012 | AUC 0.5149 | Recall 0.3600 | Precision 0.3600 | AP 0.3590 | F1 0.3592 | Time 3.03\n",
      "Epoch 0087: Loss 0.0012 | AUC 0.5009 | Recall 0.3507 | Precision 0.3507 | AP 0.3480 | F1 0.3508 | Time 3.03\n",
      "Epoch 0088: Loss 0.0012 | AUC 0.5074 | Recall 0.3516 | Precision 0.3516 | AP 0.3542 | F1 0.3511 | Time 3.01\n",
      "Epoch 0089: Loss 0.0012 | AUC 0.5079 | Recall 0.3538 | Precision 0.3538 | AP 0.3553 | F1 0.3535 | Time 3.40\n",
      "Epoch 0090: Loss 0.0012 | AUC 0.5119 | Recall 0.3590 | Precision 0.3590 | AP 0.3583 | F1 0.3590 | Time 3.27\n",
      "Epoch 0091: Loss 0.0012 | AUC 0.5247 | Recall 0.3709 | Precision 0.3709 | AP 0.3653 | F1 0.3702 | Time 3.14\n",
      "Epoch 0092: Loss 0.0012 | AUC 0.5019 | Recall 0.3496 | Precision 0.3496 | AP 0.3558 | F1 0.3495 | Time 3.00\n",
      "Epoch 0093: Loss 0.0012 | AUC 0.5086 | Recall 0.3548 | Precision 0.3548 | AP 0.3553 | F1 0.3547 | Time 2.99\n",
      "Epoch 0094: Loss 0.0012 | AUC 0.5030 | Recall 0.3509 | Precision 0.3509 | AP 0.3480 | F1 0.3506 | Time 3.11\n",
      "Epoch 0095: Loss 0.0012 | AUC 0.5091 | Recall 0.3590 | Precision 0.3590 | AP 0.3579 | F1 0.3588 | Time 3.06\n",
      "Epoch 0096: Loss 0.0012 | AUC 0.5138 | Recall 0.3594 | Precision 0.3594 | AP 0.3553 | F1 0.3588 | Time 3.16\n",
      "Epoch 0097: Loss 0.0012 | AUC 0.5180 | Recall 0.3616 | Precision 0.3616 | AP 0.3626 | F1 0.3609 | Time 3.04\n",
      "Epoch 0098: Loss 0.0012 | AUC 0.5034 | Recall 0.3509 | Precision 0.3509 | AP 0.3505 | F1 0.3501 | Time 3.00\n",
      "Epoch 0099: Loss 0.0012 | AUC 0.5136 | Recall 0.3574 | Precision 0.3574 | AP 0.3552 | F1 0.3560 | Time 3.02\n",
      "Test: Loss 0.0000 | AUC 0.5296 | Recall 0.9132 | Precision 0.9132 | AP 0.4960 | F1 0.2159 | Time 0.76\n",
      "Epoch 0000: Loss 0.0000 | AUC 0.9536 | Recall 0.8394 | Precision 0.8394 | AP 0.9281 | F1 0.8394 | Time 3.03\n",
      "Epoch 0001: Loss 0.0202 | AUC 0.4818 | Recall 0.3319 | Precision 0.3319 | AP 0.3445 | F1 0.3319 | Time 3.05\n",
      "Epoch 0002: Loss 0.0022 | AUC 0.4784 | Recall 0.3308 | Precision 0.3308 | AP 0.3377 | F1 0.3308 | Time 2.98\n",
      "Epoch 0003: Loss 0.0241 | AUC 0.4783 | Recall 0.3263 | Precision 0.3263 | AP 0.3424 | F1 0.3263 | Time 3.02\n",
      "Epoch 0004: Loss 0.0024 | AUC 0.4822 | Recall 0.3265 | Precision 0.3265 | AP 0.3405 | F1 0.3265 | Time 3.01\n",
      "Epoch 0005: Loss 0.0022 | AUC 0.4832 | Recall 0.3384 | Precision 0.3384 | AP 0.3404 | F1 0.3384 | Time 3.05\n",
      "Epoch 0006: Loss 0.0022 | AUC 0.4864 | Recall 0.3427 | Precision 0.3427 | AP 0.3488 | F1 0.3427 | Time 3.15\n",
      "Epoch 0007: Loss 0.0022 | AUC 0.4975 | Recall 0.3451 | Precision 0.3451 | AP 0.3535 | F1 0.3451 | Time 3.11\n",
      "Epoch 0008: Loss 0.0022 | AUC 0.4782 | Recall 0.3271 | Precision 0.3271 | AP 0.3416 | F1 0.3271 | Time 3.11\n",
      "Epoch 0009: Loss 0.0022 | AUC 0.4869 | Recall 0.3416 | Precision 0.3416 | AP 0.3476 | F1 0.3416 | Time 3.02\n",
      "Epoch 0010: Loss 0.0022 | AUC 0.4826 | Recall 0.3356 | Precision 0.3356 | AP 0.3416 | F1 0.3356 | Time 3.16\n",
      "Epoch 0011: Loss 0.0022 | AUC 0.4841 | Recall 0.3347 | Precision 0.3347 | AP 0.3399 | F1 0.3347 | Time 3.21\n",
      "Epoch 0012: Loss 0.0022 | AUC 0.4814 | Recall 0.3301 | Precision 0.3301 | AP 0.3378 | F1 0.3301 | Time 2.98\n",
      "Epoch 0013: Loss 0.0022 | AUC 0.4880 | Recall 0.3369 | Precision 0.3369 | AP 0.3430 | F1 0.3369 | Time 3.04\n",
      "Epoch 0014: Loss 0.0022 | AUC 0.5093 | Recall 0.3499 | Precision 0.3499 | AP 0.3577 | F1 0.3499 | Time 3.00\n",
      "Epoch 0015: Loss 0.0022 | AUC 0.5075 | Recall 0.3629 | Precision 0.3629 | AP 0.3589 | F1 0.3629 | Time 2.99\n",
      "Epoch 0016: Loss 0.0022 | AUC 0.5044 | Recall 0.3510 | Precision 0.3510 | AP 0.3496 | F1 0.3511 | Time 3.01\n",
      "Epoch 0017: Loss 0.0022 | AUC 0.4852 | Recall 0.3354 | Precision 0.3354 | AP 0.3415 | F1 0.3354 | Time 3.01\n",
      "Epoch 0018: Loss 0.0022 | AUC 0.4873 | Recall 0.3403 | Precision 0.3403 | AP 0.3446 | F1 0.3403 | Time 3.01\n",
      "Epoch 0019: Loss 0.0022 | AUC 0.4791 | Recall 0.3286 | Precision 0.3286 | AP 0.3377 | F1 0.3286 | Time 3.04\n",
      "Epoch 0020: Loss 0.0022 | AUC 0.4976 | Recall 0.3483 | Precision 0.3483 | AP 0.3523 | F1 0.3483 | Time 3.05\n",
      "Epoch 0021: Loss 0.0022 | AUC 0.4944 | Recall 0.3455 | Precision 0.3455 | AP 0.3521 | F1 0.3455 | Time 2.98\n",
      "Epoch 0022: Loss 0.0022 | AUC 0.4799 | Recall 0.3291 | Precision 0.3291 | AP 0.3382 | F1 0.3291 | Time 3.05\n",
      "Epoch 0023: Loss 0.0022 | AUC 0.4835 | Recall 0.3319 | Precision 0.3319 | AP 0.3397 | F1 0.3319 | Time 2.94\n",
      "Epoch 0024: Loss 0.0022 | AUC 0.5070 | Recall 0.3551 | Precision 0.3551 | AP 0.3569 | F1 0.3551 | Time 3.00\n",
      "Epoch 0025: Loss 0.0022 | AUC 0.4885 | Recall 0.3356 | Precision 0.3356 | AP 0.3434 | F1 0.3356 | Time 3.05\n",
      "Epoch 0026: Loss 0.0022 | AUC 0.4878 | Recall 0.3390 | Precision 0.3390 | AP 0.3399 | F1 0.3390 | Time 3.22\n",
      "Epoch 0027: Loss 0.0022 | AUC 0.5024 | Recall 0.3510 | Precision 0.3510 | AP 0.3523 | F1 0.3510 | Time 3.02\n",
      "Epoch 0028: Loss 0.0022 | AUC 0.4935 | Recall 0.3406 | Precision 0.3406 | AP 0.3467 | F1 0.3406 | Time 2.99\n",
      "Epoch 0029: Loss 0.0022 | AUC 0.4894 | Recall 0.3419 | Precision 0.3419 | AP 0.3451 | F1 0.3419 | Time 3.05\n",
      "Epoch 0030: Loss 0.0022 | AUC 0.4844 | Recall 0.3436 | Precision 0.3436 | AP 0.3458 | F1 0.3436 | Time 2.98\n",
      "Epoch 0031: Loss 0.0022 | AUC 0.4840 | Recall 0.3405 | Precision 0.3405 | AP 0.3456 | F1 0.3405 | Time 2.96\n",
      "Epoch 0032: Loss 0.0022 | AUC 0.4896 | Recall 0.3330 | Precision 0.3330 | AP 0.3422 | F1 0.3330 | Time 3.00\n",
      "Epoch 0033: Loss 0.0022 | AUC 0.4782 | Recall 0.3237 | Precision 0.3237 | AP 0.3335 | F1 0.3237 | Time 3.22\n",
      "Epoch 0034: Loss 0.0022 | AUC 0.4752 | Recall 0.3258 | Precision 0.3258 | AP 0.3358 | F1 0.3258 | Time 3.13\n",
      "Epoch 0035: Loss 0.0022 | AUC 0.5023 | Recall 0.3574 | Precision 0.3574 | AP 0.3522 | F1 0.3574 | Time 3.01\n",
      "Epoch 0036: Loss 0.0022 | AUC 0.4881 | Recall 0.3388 | Precision 0.3388 | AP 0.3389 | F1 0.3388 | Time 2.97\n",
      "Epoch 0037: Loss 0.0022 | AUC 0.4952 | Recall 0.3401 | Precision 0.3401 | AP 0.3523 | F1 0.3401 | Time 2.98\n",
      "Epoch 0038: Loss 0.0022 | AUC 0.4938 | Recall 0.3440 | Precision 0.3440 | AP 0.3478 | F1 0.3440 | Time 2.95\n",
      "Epoch 0039: Loss 0.0022 | AUC 0.4920 | Recall 0.3399 | Precision 0.3399 | AP 0.3431 | F1 0.3399 | Time 3.00\n",
      "Epoch 0040: Loss 0.0022 | AUC 0.4881 | Recall 0.3395 | Precision 0.3395 | AP 0.3395 | F1 0.3395 | Time 3.19\n",
      "Epoch 0041: Loss 0.0022 | AUC 0.4966 | Recall 0.3462 | Precision 0.3462 | AP 0.3470 | F1 0.3462 | Time 3.02\n",
      "Epoch 0042: Loss 0.0022 | AUC 0.5033 | Recall 0.3538 | Precision 0.3538 | AP 0.3481 | F1 0.3538 | Time 2.97\n",
      "Epoch 0043: Loss 0.0022 | AUC 0.5093 | Recall 0.3555 | Precision 0.3555 | AP 0.3523 | F1 0.3555 | Time 3.03\n",
      "Epoch 0044: Loss 0.0022 | AUC 0.5042 | Recall 0.3494 | Precision 0.3494 | AP 0.3495 | F1 0.3494 | Time 2.96\n",
      "Epoch 0045: Loss 0.0022 | AUC 0.4928 | Recall 0.3414 | Precision 0.3414 | AP 0.3447 | F1 0.3414 | Time 2.98\n",
      "Epoch 0046: Loss 0.0022 | AUC 0.5064 | Recall 0.3559 | Precision 0.3559 | AP 0.3540 | F1 0.3559 | Time 3.02\n",
      "Epoch 0047: Loss 0.0022 | AUC 0.4969 | Recall 0.3419 | Precision 0.3419 | AP 0.3458 | F1 0.3419 | Time 2.95\n",
      "Epoch 0048: Loss 0.0022 | AUC 0.4845 | Recall 0.3345 | Precision 0.3345 | AP 0.3377 | F1 0.3345 | Time 3.01\n",
      "Epoch 0049: Loss 0.0022 | AUC 0.5065 | Recall 0.3444 | Precision 0.3444 | AP 0.3522 | F1 0.3444 | Time 2.97\n",
      "Epoch 0050: Loss 0.0022 | AUC 0.4918 | Recall 0.3410 | Precision 0.3410 | AP 0.3501 | F1 0.3410 | Time 2.96\n",
      "Epoch 0051: Loss 0.0022 | AUC 0.4954 | Recall 0.3444 | Precision 0.3444 | AP 0.3496 | F1 0.3444 | Time 2.98\n",
      "Epoch 0052: Loss 0.0022 | AUC 0.5018 | Recall 0.3425 | Precision 0.3425 | AP 0.3528 | F1 0.3425 | Time 3.02\n",
      "Epoch 0053: Loss 0.0022 | AUC 0.5024 | Recall 0.3481 | Precision 0.3481 | AP 0.3554 | F1 0.3481 | Time 3.05\n",
      "Epoch 0054: Loss 0.0022 | AUC 0.5011 | Recall 0.3514 | Precision 0.3514 | AP 0.3539 | F1 0.3514 | Time 3.24\n",
      "Epoch 0055: Loss 0.0022 | AUC 0.4921 | Recall 0.3442 | Precision 0.3442 | AP 0.3467 | F1 0.3442 | Time 3.05\n",
      "Epoch 0056: Loss 0.0022 | AUC 0.4806 | Recall 0.3317 | Precision 0.3317 | AP 0.3424 | F1 0.3317 | Time 3.02\n",
      "Epoch 0057: Loss 0.0022 | AUC 0.4851 | Recall 0.3334 | Precision 0.3334 | AP 0.3459 | F1 0.3334 | Time 3.09\n",
      "Epoch 0058: Loss 0.0022 | AUC 0.4961 | Recall 0.3425 | Precision 0.3425 | AP 0.3490 | F1 0.3425 | Time 2.96\n",
      "Epoch 0059: Loss 0.0022 | AUC 0.4802 | Recall 0.3349 | Precision 0.3349 | AP 0.3426 | F1 0.3349 | Time 2.99\n",
      "Epoch 0060: Loss 0.0022 | AUC 0.4885 | Recall 0.3427 | Precision 0.3427 | AP 0.3484 | F1 0.3428 | Time 2.97\n",
      "Epoch 0061: Loss 0.0022 | AUC 0.5041 | Recall 0.3453 | Precision 0.3453 | AP 0.3509 | F1 0.3453 | Time 3.00\n",
      "Epoch 0062: Loss 0.0022 | AUC 0.4874 | Recall 0.3384 | Precision 0.3384 | AP 0.3408 | F1 0.3384 | Time 3.01\n",
      "Epoch 0063: Loss 0.0022 | AUC 0.4815 | Recall 0.3403 | Precision 0.3403 | AP 0.3429 | F1 0.3403 | Time 3.01\n",
      "Epoch 0064: Loss 0.0022 | AUC 0.4804 | Recall 0.3314 | Precision 0.3314 | AP 0.3377 | F1 0.3314 | Time 2.99\n",
      "Epoch 0065: Loss 0.0022 | AUC 0.4735 | Recall 0.3269 | Precision 0.3269 | AP 0.3345 | F1 0.3269 | Time 2.93\n",
      "Epoch 0066: Loss 0.0022 | AUC 0.4887 | Recall 0.3440 | Precision 0.3440 | AP 0.3458 | F1 0.3440 | Time 2.98\n",
      "Epoch 0067: Loss 0.0022 | AUC 0.4929 | Recall 0.3412 | Precision 0.3412 | AP 0.3518 | F1 0.3412 | Time 2.97\n",
      "Epoch 0068: Loss 0.0022 | AUC 0.4989 | Recall 0.3466 | Precision 0.3466 | AP 0.3555 | F1 0.3466 | Time 3.19\n",
      "Epoch 0069: Loss 0.0022 | AUC 0.4845 | Recall 0.3317 | Precision 0.3317 | AP 0.3444 | F1 0.3318 | Time 2.98\n",
      "Epoch 0070: Loss 0.0022 | AUC 0.4997 | Recall 0.3403 | Precision 0.3403 | AP 0.3515 | F1 0.3403 | Time 3.06\n",
      "Epoch 0071: Loss 0.0022 | AUC 0.4952 | Recall 0.3429 | Precision 0.3429 | AP 0.3493 | F1 0.3429 | Time 3.04\n",
      "Epoch 0072: Loss 0.0022 | AUC 0.4881 | Recall 0.3421 | Precision 0.3421 | AP 0.3487 | F1 0.3422 | Time 3.03\n",
      "Epoch 0073: Loss 0.0022 | AUC 0.4884 | Recall 0.3354 | Precision 0.3354 | AP 0.3430 | F1 0.3353 | Time 2.95\n",
      "Epoch 0074: Loss 0.0022 | AUC 0.4848 | Recall 0.3345 | Precision 0.3345 | AP 0.3353 | F1 0.3345 | Time 3.05\n",
      "Epoch 0075: Loss 0.0022 | AUC 0.4960 | Recall 0.3449 | Precision 0.3449 | AP 0.3459 | F1 0.3446 | Time 3.03\n",
      "Epoch 0076: Loss 0.0022 | AUC 0.5067 | Recall 0.3479 | Precision 0.3479 | AP 0.3510 | F1 0.3478 | Time 2.96\n",
      "Epoch 0077: Loss 0.0022 | AUC 0.5166 | Recall 0.3683 | Precision 0.3683 | AP 0.3594 | F1 0.3683 | Time 3.09\n",
      "Epoch 0078: Loss 0.0022 | AUC 0.4900 | Recall 0.3445 | Precision 0.3445 | AP 0.3451 | F1 0.3444 | Time 3.01\n",
      "Epoch 0079: Loss 0.0022 | AUC 0.5007 | Recall 0.3477 | Precision 0.3477 | AP 0.3477 | F1 0.3476 | Time 3.27\n",
      "Epoch 0080: Loss 0.0022 | AUC 0.4946 | Recall 0.3468 | Precision 0.3468 | AP 0.3472 | F1 0.3466 | Time 3.05\n",
      "Epoch 0081: Loss 0.0022 | AUC 0.4891 | Recall 0.3405 | Precision 0.3405 | AP 0.3425 | F1 0.3404 | Time 3.15\n",
      "Epoch 0082: Loss 0.0022 | AUC 0.4928 | Recall 0.3440 | Precision 0.3440 | AP 0.3461 | F1 0.3436 | Time 3.09\n",
      "Epoch 0083: Loss 0.0022 | AUC 0.4849 | Recall 0.3349 | Precision 0.3349 | AP 0.3428 | F1 0.3349 | Time 3.02\n",
      "Epoch 0084: Loss 0.0022 | AUC 0.4842 | Recall 0.3379 | Precision 0.3379 | AP 0.3494 | F1 0.3372 | Time 3.03\n",
      "Epoch 0085: Loss 0.0022 | AUC 0.4926 | Recall 0.3462 | Precision 0.3462 | AP 0.3485 | F1 0.3462 | Time 2.98\n",
      "Epoch 0086: Loss 0.0022 | AUC 0.4799 | Recall 0.3367 | Precision 0.3367 | AP 0.3427 | F1 0.3366 | Time 2.98\n",
      "Epoch 0087: Loss 0.0022 | AUC 0.4711 | Recall 0.3250 | Precision 0.3250 | AP 0.3392 | F1 0.3251 | Time 3.02\n",
      "Epoch 0088: Loss 0.0022 | AUC 0.4922 | Recall 0.3375 | Precision 0.3375 | AP 0.3495 | F1 0.3375 | Time 2.97\n",
      "Epoch 0089: Loss 0.0022 | AUC 0.4936 | Recall 0.3436 | Precision 0.3436 | AP 0.3494 | F1 0.3428 | Time 3.03\n",
      "Epoch 0090: Loss 0.0022 | AUC 0.4817 | Recall 0.3351 | Precision 0.3351 | AP 0.3457 | F1 0.3346 | Time 3.02\n",
      "Epoch 0091: Loss 0.0022 | AUC 0.4811 | Recall 0.3328 | Precision 0.3328 | AP 0.3391 | F1 0.3318 | Time 2.95\n",
      "Epoch 0092: Loss 0.0022 | AUC 0.4965 | Recall 0.3470 | Precision 0.3470 | AP 0.3523 | F1 0.3472 | Time 2.99\n",
      "Epoch 0093: Loss 0.0022 | AUC 0.4982 | Recall 0.3510 | Precision 0.3510 | AP 0.3542 | F1 0.3510 | Time 3.14\n",
      "Epoch 0094: Loss 0.0022 | AUC 0.5038 | Recall 0.3484 | Precision 0.3484 | AP 0.3507 | F1 0.3472 | Time 2.98\n",
      "Epoch 0095: Loss 0.0022 | AUC 0.4922 | Recall 0.3354 | Precision 0.3354 | AP 0.3438 | F1 0.3347 | Time 2.98\n",
      "Epoch 0096: Loss 0.0022 | AUC 0.4900 | Recall 0.3349 | Precision 0.3349 | AP 0.3403 | F1 0.3344 | Time 3.03\n",
      "Epoch 0097: Loss 0.0022 | AUC 0.5065 | Recall 0.3527 | Precision 0.3527 | AP 0.3508 | F1 0.3508 | Time 3.01\n",
      "Epoch 0098: Loss 0.0022 | AUC 0.5144 | Recall 0.3633 | Precision 0.3633 | AP 0.3610 | F1 0.3631 | Time 2.99\n",
      "Epoch 0099: Loss 0.0022 | AUC 0.4941 | Recall 0.3410 | Precision 0.3410 | AP 0.3399 | F1 0.3398 | Time 2.95\n",
      "Test: Loss 0.0000 | AUC 0.5296 | Recall 0.9132 | Precision 0.9132 | AP 0.4960 | F1 0.2159 | Time 0.74\n"
     ]
    }
   ],
   "source": [
    "f1_ocgnn = []\n",
    "precision_ocgnn = []\n",
    "recall_ocgnn = []\n",
    "train_durration_ocgnn = []\n",
    "predict_durration_ocgnn = []\n",
    "for i in range(3):\n",
    "    start_time = time.time()\n",
    "    ocgnn_model, graph_test = make_ocgnn_model(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n",
    "    make_model_runtime = time.time() - start_time\n",
    "    train_durration_ocgnn.append(make_model_runtime)\n",
    "\n",
    "    start_time = time.time()\n",
    "    f1_score, precision_score, recall_score = predict_ocgnn(label_test, ocgnn_model, graph_test)\n",
    "    predict_runtime_ocgnn = time.time() - start_time\n",
    "    predict_durration_ocgnn.append(predict_runtime_ocgnn)\n",
    "    f1_ocgnn.append(f1_score)\n",
    "    precision_ocgnn.append(precision_score)\n",
    "    recall_ocgnn.append(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.6217), tensor(0.6217), tensor(0.6217)]\n",
      "[tensor(0.4510), tensor(0.4510), tensor(0.4510)]\n",
      "[tensor(1.), tensor(1.), tensor(1.)]\n"
     ]
    }
   ],
   "source": [
    "print(f1_ocgnn)\n",
    "print(precision_ocgnn)\n",
    "print(recall_ocgnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[355.066321849823, 350.1418790817261, 337.6466917991638]\n",
      "[2.354825735092163, 2.1781935691833496, 2.3047642707824707]\n"
     ]
    }
   ],
   "source": [
    "print(train_durration_ocgnn)\n",
    "print(predict_durration_ocgnn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gae_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features,\n",
    "                        label_test):\n",
    "    \n",
    "    node_features = torch.tensor(node_features)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train.cpu()\n",
    "    pyG_train.x = train_node_features.cpu()\n",
    "    label_train = label_train.cpu()\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    gae_model = GAE(hid_dim=64, num_layers=128, weight_decay=3.7,\n",
    "                contamination=0.37, lr=0.001, epoch=100, gpu=-1,\n",
    "                num_neigh=-1, verbose=3)\n",
    "    \n",
    "    gae_compile = gae_model.fit(pyG_train, label_train)\n",
    "    return gae_compile, pyG_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gae(label_test, gae_compile, pyG_test):\n",
    "    gae_ip_pred_res, gae_ip_score_res, gae_ip_prob_res, gae_ip_conf_res = gae_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, return_prob=True, prob_method='linear', return_conf=True)\n",
    "    f1_score_ip = eval_f1(label_test, gae_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, gae_ip_score_res, k=len(label_test))\n",
    "    recall = eval_recall_at_k(label_test, gae_ip_score_res, k=len(label_test))\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    return f1_score_ip, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 115987.6484 | AUC 0.7865 | Recall 0.6501 | Precision 0.6501 | AP 0.6582 | F1 0.7300 | Time 5.41\n",
      "Epoch 0001: Loss 115986.6797 | AUC 0.7869 | Recall 0.6616 | Precision 0.6616 | AP 0.6611 | F1 0.6624 | Time 5.46\n",
      "Epoch 0002: Loss 115985.1797 | AUC 0.7873 | Recall 0.6620 | Precision 0.6620 | AP 0.6615 | F1 0.6625 | Time 5.49\n",
      "Epoch 0003: Loss 115983.5625 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6618 | F1 0.6622 | Time 5.50\n",
      "Epoch 0004: Loss 115981.9375 | AUC 0.7873 | Recall 0.6618 | Precision 0.6618 | AP 0.6617 | F1 0.6621 | Time 6.03\n",
      "Epoch 0005: Loss 115980.2188 | AUC 0.7873 | Recall 0.6620 | Precision 0.6620 | AP 0.6617 | F1 0.6621 | Time 5.93\n",
      "Epoch 0006: Loss 115978.4375 | AUC 0.7873 | Recall 0.6618 | Precision 0.6618 | AP 0.6617 | F1 0.6621 | Time 5.62\n",
      "Epoch 0007: Loss 115976.5625 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6622 | Time 5.66\n",
      "Epoch 0008: Loss 115974.5781 | AUC 0.7873 | Recall 0.6620 | Precision 0.6620 | AP 0.6619 | F1 0.6621 | Time 5.57\n",
      "Epoch 0009: Loss 115972.4688 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6618 | F1 0.6621 | Time 5.52\n",
      "Epoch 0010: Loss 115970.1953 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6621 | Time 5.77\n",
      "Epoch 0011: Loss 115967.8047 | AUC 0.7873 | Recall 0.6620 | Precision 0.6620 | AP 0.6619 | F1 0.6621 | Time 5.51\n",
      "Epoch 0012: Loss 115965.2188 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6619 | F1 0.6621 | Time 5.59\n",
      "Epoch 0013: Loss 115962.4297 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.58\n",
      "Epoch 0014: Loss 115959.3984 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6620 | Time 5.58\n",
      "Epoch 0015: Loss 115956.1016 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6620 | Time 5.63\n",
      "Epoch 0016: Loss 115952.4766 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.66\n",
      "Epoch 0017: Loss 115948.4844 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.55\n",
      "Epoch 0018: Loss 115944.0547 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.58\n",
      "Epoch 0019: Loss 115939.1172 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.56\n",
      "Epoch 0020: Loss 115933.5547 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.61\n",
      "Epoch 0021: Loss 115927.2734 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.63\n",
      "Epoch 0022: Loss 115920.1641 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.63\n",
      "Epoch 0023: Loss 115912.0781 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.87\n",
      "Epoch 0024: Loss 115902.8516 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.74\n",
      "Epoch 0025: Loss 115892.2188 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.66\n",
      "Epoch 0026: Loss 115879.8594 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.54\n",
      "Epoch 0027: Loss 115865.5000 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.64\n",
      "Epoch 0028: Loss 115848.7266 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.58\n",
      "Epoch 0029: Loss 115828.9141 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6624 | F1 0.6620 | Time 5.57\n",
      "Epoch 0030: Loss 115805.4531 | AUC 0.7876 | Recall 0.6620 | Precision 0.6620 | AP 0.6625 | F1 0.6620 | Time 6.09\n",
      "Epoch 0031: Loss 115777.4453 | AUC 0.7876 | Recall 0.6620 | Precision 0.6620 | AP 0.6624 | F1 0.6620 | Time 5.71\n",
      "Epoch 0032: Loss 115743.9062 | AUC 0.7876 | Recall 0.6621 | Precision 0.6621 | AP 0.6625 | F1 0.6621 | Time 5.58\n",
      "Epoch 0033: Loss 115703.4688 | AUC 0.7876 | Recall 0.6621 | Precision 0.6621 | AP 0.6625 | F1 0.6621 | Time 5.90\n",
      "Epoch 0034: Loss 115654.3906 | AUC 0.7876 | Recall 0.6621 | Precision 0.6621 | AP 0.6625 | F1 0.6621 | Time 5.90\n",
      "Epoch 0035: Loss 115594.2578 | AUC 0.7876 | Recall 0.6623 | Precision 0.6623 | AP 0.6625 | F1 0.6623 | Time 5.92\n",
      "Epoch 0036: Loss 115520.2656 | AUC 0.7877 | Recall 0.6623 | Precision 0.6623 | AP 0.6626 | F1 0.6623 | Time 6.08\n",
      "Epoch 0037: Loss 115428.8672 | AUC 0.7879 | Recall 0.6625 | Precision 0.6625 | AP 0.6627 | F1 0.6628 | Time 6.98\n",
      "Epoch 0038: Loss 115315.1250 | AUC 0.7879 | Recall 0.6618 | Precision 0.6618 | AP 0.6625 | F1 0.6620 | Time 7.89\n",
      "Epoch 0039: Loss 115172.9219 | AUC 0.7876 | Recall 0.6616 | Precision 0.6616 | AP 0.6624 | F1 0.6616 | Time 8.66\n",
      "Epoch 0040: Loss 114994.1016 | AUC 0.7877 | Recall 0.6616 | Precision 0.6616 | AP 0.6625 | F1 0.6616 | Time 6.15\n",
      "Epoch 0041: Loss 114768.5703 | AUC 0.7879 | Recall 0.6612 | Precision 0.6612 | AP 0.6624 | F1 0.6613 | Time 5.86\n",
      "Epoch 0042: Loss 114483.8672 | AUC 0.7881 | Recall 0.6601 | Precision 0.6601 | AP 0.6623 | F1 0.6601 | Time 7.04\n",
      "Epoch 0043: Loss 114124.9141 | AUC 0.7887 | Recall 0.6597 | Precision 0.6597 | AP 0.6626 | F1 0.6597 | Time 7.34\n",
      "Epoch 0044: Loss 113674.1875 | AUC 0.7770 | Recall 0.6625 | Precision 0.6625 | AP 0.6593 | F1 0.6625 | Time 6.76\n",
      "Epoch 0045: Loss 113114.4922 | AUC 0.6475 | Recall 0.6402 | Precision 0.6402 | AP 0.6058 | F1 0.6406 | Time 6.28\n",
      "Epoch 0046: Loss 112434.3984 | AUC 0.6417 | Recall 0.4027 | Precision 0.4027 | AP 0.5300 | F1 0.4027 | Time 6.58\n",
      "Epoch 0047: Loss 111645.3047 | AUC 0.4726 | Recall 0.3025 | Precision 0.3025 | AP 0.4411 | F1 0.3025 | Time 5.78\n",
      "Epoch 0048: Loss 110822.0078 | AUC 0.3743 | Recall 0.2232 | Precision 0.2232 | AP 0.3781 | F1 0.2232 | Time 7.09\n",
      "Epoch 0049: Loss 110209.7656 | AUC 0.2825 | Recall 0.1487 | Precision 0.1487 | AP 0.3293 | F1 0.1487 | Time 6.23\n",
      "Epoch 0050: Loss 110428.8047 | AUC 0.2491 | Recall 0.1280 | Precision 0.1280 | AP 0.3138 | F1 0.1280 | Time 6.56\n",
      "Epoch 0051: Loss 111280.4766 | AUC 0.2410 | Recall 0.1403 | Precision 0.1403 | AP 0.3075 | F1 0.1403 | Time 6.10\n",
      "Epoch 0052: Loss 111175.0156 | AUC 0.2395 | Recall 0.1332 | Precision 0.1332 | AP 0.3083 | F1 0.1332 | Time 7.02\n",
      "Epoch 0053: Loss 110638.7344 | AUC 0.2444 | Recall 0.1240 | Precision 0.1240 | AP 0.3120 | F1 0.1240 | Time 6.57\n",
      "Epoch 0054: Loss 110252.1172 | AUC 0.2548 | Recall 0.1308 | Precision 0.1308 | AP 0.3163 | F1 0.1308 | Time 6.33\n",
      "Epoch 0055: Loss 110145.2812 | AUC 0.2701 | Recall 0.1412 | Precision 0.1412 | AP 0.3230 | F1 0.1412 | Time 6.35\n",
      "Epoch 0056: Loss 110218.5078 | AUC 0.2840 | Recall 0.1490 | Precision 0.1490 | AP 0.3298 | F1 0.1490 | Time 6.40\n",
      "Epoch 0057: Loss 110350.9609 | AUC 0.3014 | Recall 0.1608 | Precision 0.1608 | AP 0.3381 | F1 0.1608 | Time 7.53\n",
      "Epoch 0058: Loss 110468.7812 | AUC 0.3200 | Recall 0.1710 | Precision 0.1710 | AP 0.3461 | F1 0.1710 | Time 7.32\n",
      "Epoch 0059: Loss 110539.7891 | AUC 0.3312 | Recall 0.1801 | Precision 0.1801 | AP 0.3514 | F1 0.1801 | Time 8.10\n",
      "Epoch 0060: Loss 110556.0469 | AUC 0.3336 | Recall 0.1814 | Precision 0.1814 | AP 0.3528 | F1 0.1814 | Time 9.85\n",
      "Epoch 0061: Loss 110521.8906 | AUC 0.3282 | Recall 0.1778 | Precision 0.1778 | AP 0.3497 | F1 0.1778 | Time 9.18\n",
      "Epoch 0062: Loss 110448.7344 | AUC 0.3169 | Recall 0.1700 | Precision 0.1700 | AP 0.3447 | F1 0.1700 | Time 7.97\n",
      "Epoch 0063: Loss 110353.0625 | AUC 0.3017 | Recall 0.1609 | Precision 0.1609 | AP 0.3382 | F1 0.1609 | Time 8.10\n",
      "Epoch 0064: Loss 110255.7656 | AUC 0.2886 | Recall 0.1516 | Precision 0.1516 | AP 0.3317 | F1 0.1516 | Time 8.30\n",
      "Epoch 0065: Loss 110179.9766 | AUC 0.2788 | Recall 0.1487 | Precision 0.1487 | AP 0.3276 | F1 0.1487 | Time 8.03\n",
      "Epoch 0066: Loss 110146.0312 | AUC 0.2709 | Recall 0.1424 | Precision 0.1424 | AP 0.3234 | F1 0.1424 | Time 8.32\n",
      "Epoch 0067: Loss 110162.8359 | AUC 0.2627 | Recall 0.1349 | Precision 0.1349 | AP 0.3190 | F1 0.1349 | Time 8.62\n",
      "Epoch 0068: Loss 110215.5781 | AUC 0.2578 | Recall 0.1314 | Precision 0.1314 | AP 0.3172 | F1 0.1314 | Time 8.11\n",
      "Epoch 0069: Loss 110268.6250 | AUC 0.2540 | Recall 0.1312 | Precision 0.1312 | AP 0.3159 | F1 0.1312 | Time 8.48\n",
      "Epoch 0070: Loss 110287.6094 | AUC 0.2529 | Recall 0.1308 | Precision 0.1308 | AP 0.3154 | F1 0.1308 | Time 8.84\n",
      "Epoch 0071: Loss 110264.9141 | AUC 0.2544 | Recall 0.1312 | Precision 0.1312 | AP 0.3161 | F1 0.1312 | Time 8.66\n",
      "Epoch 0072: Loss 110217.6953 | AUC 0.2577 | Recall 0.1314 | Precision 0.1314 | AP 0.3172 | F1 0.1314 | Time 9.32\n",
      "Epoch 0073: Loss 110172.9688 | AUC 0.2613 | Recall 0.1332 | Precision 0.1332 | AP 0.3184 | F1 0.1332 | Time 8.54\n",
      "Epoch 0074: Loss 110148.5469 | AUC 0.2673 | Recall 0.1394 | Precision 0.1394 | AP 0.3212 | F1 0.1394 | Time 8.27\n",
      "Epoch 0075: Loss 110147.3203 | AUC 0.2715 | Recall 0.1427 | Precision 0.1427 | AP 0.3237 | F1 0.1427 | Time 8.43\n",
      "Epoch 0076: Loss 110161.7188 | AUC 0.2757 | Recall 0.1468 | Precision 0.1468 | AP 0.3260 | F1 0.1468 | Time 8.19\n",
      "Epoch 0077: Loss 110180.8672 | AUC 0.2791 | Recall 0.1487 | Precision 0.1487 | AP 0.3277 | F1 0.1487 | Time 8.00\n",
      "Epoch 0078: Loss 110195.6641 | AUC 0.2817 | Recall 0.1490 | Precision 0.1490 | AP 0.3288 | F1 0.1491 | Time 7.76\n",
      "Epoch 0079: Loss 110201.0391 | AUC 0.2823 | Recall 0.1494 | Precision 0.1494 | AP 0.3290 | F1 0.1494 | Time 7.91\n",
      "Epoch 0080: Loss 110196.0938 | AUC 0.2818 | Recall 0.1492 | Precision 0.1492 | AP 0.3288 | F1 0.1492 | Time 8.00\n",
      "Epoch 0081: Loss 110183.3281 | AUC 0.2798 | Recall 0.1487 | Precision 0.1487 | AP 0.3280 | F1 0.1487 | Time 8.21\n",
      "Epoch 0082: Loss 110167.4141 | AUC 0.2770 | Recall 0.1477 | Precision 0.1477 | AP 0.3266 | F1 0.1477 | Time 8.27\n",
      "Epoch 0083: Loss 110153.6094 | AUC 0.2735 | Recall 0.1451 | Precision 0.1451 | AP 0.3248 | F1 0.1451 | Time 8.15\n",
      "Epoch 0084: Loss 110146.1250 | AUC 0.2710 | Recall 0.1424 | Precision 0.1424 | AP 0.3235 | F1 0.1424 | Time 7.93\n",
      "Epoch 0085: Loss 110146.4453 | AUC 0.2684 | Recall 0.1407 | Precision 0.1407 | AP 0.3218 | F1 0.1407 | Time 7.86\n",
      "Epoch 0086: Loss 110152.7500 | AUC 0.2654 | Recall 0.1375 | Precision 0.1375 | AP 0.3201 | F1 0.1375 | Time 7.77\n",
      "Epoch 0087: Loss 110160.6172 | AUC 0.2631 | Recall 0.1349 | Precision 0.1349 | AP 0.3191 | F1 0.1349 | Time 7.94\n",
      "Epoch 0088: Loss 110165.4062 | AUC 0.2624 | Recall 0.1349 | Precision 0.1349 | AP 0.3189 | F1 0.1349 | Time 8.07\n",
      "Epoch 0089: Loss 110164.7344 | AUC 0.2625 | Recall 0.1349 | Precision 0.1349 | AP 0.3189 | F1 0.1349 | Time 8.01\n",
      "Epoch 0090: Loss 110159.4453 | AUC 0.2634 | Recall 0.1355 | Precision 0.1355 | AP 0.3193 | F1 0.1355 | Time 7.90\n",
      "Epoch 0091: Loss 110152.4844 | AUC 0.2655 | Recall 0.1379 | Precision 0.1379 | AP 0.3201 | F1 0.1379 | Time 7.84\n",
      "Epoch 0092: Loss 110147.2109 | AUC 0.2680 | Recall 0.1399 | Precision 0.1399 | AP 0.3216 | F1 0.1399 | Time 7.74\n",
      "Epoch 0093: Loss 110145.3203 | AUC 0.2700 | Recall 0.1416 | Precision 0.1416 | AP 0.3227 | F1 0.1416 | Time 7.80\n",
      "Epoch 0094: Loss 110146.6719 | AUC 0.2713 | Recall 0.1425 | Precision 0.1425 | AP 0.3236 | F1 0.1425 | Time 7.62\n",
      "Epoch 0095: Loss 110149.7266 | AUC 0.2724 | Recall 0.1438 | Precision 0.1438 | AP 0.3242 | F1 0.1438 | Time 7.75\n",
      "Epoch 0096: Loss 110152.6250 | AUC 0.2733 | Recall 0.1450 | Precision 0.1450 | AP 0.3246 | F1 0.1450 | Time 8.07\n",
      "Epoch 0097: Loss 110154.0078 | AUC 0.2736 | Recall 0.1451 | Precision 0.1451 | AP 0.3248 | F1 0.1451 | Time 7.86\n",
      "Epoch 0098: Loss 110153.3984 | AUC 0.2735 | Recall 0.1451 | Precision 0.1451 | AP 0.3247 | F1 0.1451 | Time 8.38\n",
      "Epoch 0099: Loss 110151.2266 | AUC 0.2728 | Recall 0.1437 | Precision 0.1437 | AP 0.3244 | F1 0.1437 | Time 8.16\n",
      "Test: Loss 16.8954 | AUC 0.2730 | Recall 0.3133 | Precision 0.3133 | AP 0.4128 | F1 0.3133 | Time 1.72\n",
      "Epoch 0000: Loss 115987.6484 | AUC 0.7865 | Recall 0.6501 | Precision 0.6501 | AP 0.6582 | F1 0.7300 | Time 5.69\n",
      "Epoch 0001: Loss 115986.9609 | AUC 0.7873 | Recall 0.6618 | Precision 0.6618 | AP 0.6615 | F1 0.6622 | Time 5.65\n",
      "Epoch 0002: Loss 115985.8203 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6618 | F1 0.6642 | Time 5.62\n",
      "Epoch 0003: Loss 115984.5547 | AUC 0.7873 | Recall 0.6618 | Precision 0.6618 | AP 0.6619 | F1 0.6623 | Time 5.68\n",
      "Epoch 0004: Loss 115983.2266 | AUC 0.7874 | Recall 0.6618 | Precision 0.6618 | AP 0.6619 | F1 0.6621 | Time 5.58\n",
      "Epoch 0005: Loss 115981.8516 | AUC 0.7873 | Recall 0.6620 | Precision 0.6620 | AP 0.6619 | F1 0.6620 | Time 5.75\n",
      "Epoch 0006: Loss 115980.4453 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6618 | F1 0.6621 | Time 5.62\n",
      "Epoch 0007: Loss 115978.9609 | AUC 0.7873 | Recall 0.6620 | Precision 0.6620 | AP 0.6619 | F1 0.6621 | Time 5.98\n",
      "Epoch 0008: Loss 115977.3828 | AUC 0.7873 | Recall 0.6620 | Precision 0.6620 | AP 0.6619 | F1 0.6621 | Time 5.97\n",
      "Epoch 0009: Loss 115975.7734 | AUC 0.7873 | Recall 0.6620 | Precision 0.6620 | AP 0.6619 | F1 0.6621 | Time 5.73\n",
      "Epoch 0010: Loss 115974.0625 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6621 | Time 5.87\n",
      "Epoch 0011: Loss 115972.2734 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6621 | Time 5.49\n",
      "Epoch 0012: Loss 115970.3516 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6620 | Time 5.53\n",
      "Epoch 0013: Loss 115968.3203 | AUC 0.7873 | Recall 0.6620 | Precision 0.6620 | AP 0.6619 | F1 0.6621 | Time 5.70\n",
      "Epoch 0014: Loss 115966.1406 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6620 | Time 5.40\n",
      "Epoch 0015: Loss 115963.8047 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.46\n",
      "Epoch 0016: Loss 115961.2812 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.45\n",
      "Epoch 0017: Loss 115958.5156 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.49\n",
      "Epoch 0018: Loss 115955.4922 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.50\n",
      "Epoch 0019: Loss 115952.1719 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.47\n",
      "Epoch 0020: Loss 115948.4844 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.51\n",
      "Epoch 0021: Loss 115944.4062 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.43\n",
      "Epoch 0022: Loss 115939.8359 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.46\n",
      "Epoch 0023: Loss 115934.6562 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.47\n",
      "Epoch 0024: Loss 115928.7969 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.47\n",
      "Epoch 0025: Loss 115922.0938 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.44\n",
      "Epoch 0026: Loss 115914.4531 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.39\n",
      "Epoch 0027: Loss 115905.5703 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.46\n",
      "Epoch 0028: Loss 115895.2891 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.63\n",
      "Epoch 0029: Loss 115883.2656 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.63\n",
      "Epoch 0030: Loss 115869.1250 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6624 | F1 0.6620 | Time 5.59\n",
      "Epoch 0031: Loss 115852.3516 | AUC 0.7876 | Recall 0.6621 | Precision 0.6621 | AP 0.6624 | F1 0.6621 | Time 5.46\n",
      "Epoch 0032: Loss 115832.3359 | AUC 0.7876 | Recall 0.6621 | Precision 0.6621 | AP 0.6624 | F1 0.6621 | Time 5.49\n",
      "Epoch 0033: Loss 115808.3125 | AUC 0.7876 | Recall 0.6623 | Precision 0.6623 | AP 0.6625 | F1 0.6623 | Time 5.50\n",
      "Epoch 0034: Loss 115779.2188 | AUC 0.7877 | Recall 0.6623 | Precision 0.6623 | AP 0.6626 | F1 0.6623 | Time 5.61\n",
      "Epoch 0035: Loss 115743.8438 | AUC 0.7877 | Recall 0.6625 | Precision 0.6625 | AP 0.6625 | F1 0.6628 | Time 5.52\n",
      "Epoch 0036: Loss 115700.3984 | AUC 0.7876 | Recall 0.6623 | Precision 0.6623 | AP 0.6625 | F1 0.6623 | Time 5.47\n",
      "Epoch 0037: Loss 115646.7266 | AUC 0.7877 | Recall 0.6625 | Precision 0.6625 | AP 0.6626 | F1 0.6628 | Time 5.51\n",
      "Epoch 0038: Loss 115580.0078 | AUC 0.7877 | Recall 0.6625 | Precision 0.6625 | AP 0.6626 | F1 0.6626 | Time 5.49\n",
      "Epoch 0039: Loss 115496.3906 | AUC 0.7878 | Recall 0.6625 | Precision 0.6625 | AP 0.6626 | F1 0.6625 | Time 5.68\n",
      "Epoch 0040: Loss 115391.1094 | AUC 0.7879 | Recall 0.6623 | Precision 0.6623 | AP 0.6627 | F1 0.6623 | Time 5.64\n",
      "Epoch 0041: Loss 115257.6719 | AUC 0.7878 | Recall 0.6616 | Precision 0.6616 | AP 0.6626 | F1 0.6616 | Time 5.47\n",
      "Epoch 0042: Loss 115087.7812 | AUC 0.7877 | Recall 0.6616 | Precision 0.6616 | AP 0.6625 | F1 0.6616 | Time 5.56\n",
      "Epoch 0043: Loss 114870.7656 | AUC 0.7878 | Recall 0.6616 | Precision 0.6616 | AP 0.6625 | F1 0.6616 | Time 5.89\n",
      "Epoch 0044: Loss 114593.1250 | AUC 0.7879 | Recall 0.6601 | Precision 0.6601 | AP 0.6623 | F1 0.6602 | Time 5.57\n",
      "Epoch 0045: Loss 114238.1328 | AUC 0.7881 | Recall 0.6586 | Precision 0.6586 | AP 0.6622 | F1 0.6586 | Time 5.81\n",
      "Epoch 0046: Loss 113785.9062 | AUC 0.7821 | Recall 0.6607 | Precision 0.6607 | AP 0.6609 | F1 0.6607 | Time 5.57\n",
      "Epoch 0047: Loss 113216.7578 | AUC 0.6740 | Recall 0.6646 | Precision 0.6646 | AP 0.6224 | F1 0.6646 | Time 5.90\n",
      "Epoch 0048: Loss 112517.0469 | AUC 0.6488 | Recall 0.4126 | Precision 0.4126 | AP 0.5360 | F1 0.4126 | Time 5.69\n",
      "Epoch 0049: Loss 111697.5703 | AUC 0.4794 | Recall 0.3050 | Precision 0.3050 | AP 0.4446 | F1 0.3050 | Time 6.00\n",
      "Epoch 0050: Loss 110838.1641 | AUC 0.3770 | Recall 0.2247 | Precision 0.2247 | AP 0.3799 | F1 0.2247 | Time 5.65\n",
      "Epoch 0051: Loss 110204.1250 | AUC 0.2822 | Recall 0.1489 | Precision 0.1489 | AP 0.3291 | F1 0.1489 | Time 5.64\n",
      "Epoch 0052: Loss 110469.5078 | AUC 0.2480 | Recall 0.1267 | Precision 0.1267 | AP 0.3133 | F1 0.1267 | Time 5.57\n",
      "Epoch 0053: Loss 111344.5938 | AUC 0.2426 | Recall 0.1459 | Precision 0.1459 | AP 0.3077 | F1 0.1459 | Time 5.94\n",
      "Epoch 0054: Loss 111187.3359 | AUC 0.2396 | Recall 0.1342 | Precision 0.1342 | AP 0.3081 | F1 0.1342 | Time 6.56\n",
      "Epoch 0055: Loss 110616.7188 | AUC 0.2448 | Recall 0.1243 | Precision 0.1243 | AP 0.3121 | F1 0.1243 | Time 6.10\n",
      "Epoch 0056: Loss 110232.3594 | AUC 0.2559 | Recall 0.1308 | Precision 0.1308 | AP 0.3167 | F1 0.1308 | Time 6.73\n",
      "Epoch 0057: Loss 110147.2266 | AUC 0.2713 | Recall 0.1427 | Precision 0.1427 | AP 0.3237 | F1 0.1427 | Time 6.98\n",
      "Epoch 0058: Loss 110243.1641 | AUC 0.2870 | Recall 0.1507 | Precision 0.1507 | AP 0.3310 | F1 0.1507 | Time 6.20\n",
      "Epoch 0059: Loss 110390.4609 | AUC 0.3074 | Recall 0.1641 | Precision 0.1641 | AP 0.3406 | F1 0.1641 | Time 6.97\n",
      "Epoch 0060: Loss 110513.6797 | AUC 0.3273 | Recall 0.1765 | Precision 0.1765 | AP 0.3493 | F1 0.1765 | Time 7.33\n",
      "Epoch 0061: Loss 110582.0703 | AUC 0.3373 | Recall 0.1853 | Precision 0.1853 | AP 0.3550 | F1 0.1853 | Time 7.19\n",
      "Epoch 0062: Loss 110589.3906 | AUC 0.3384 | Recall 0.1857 | Precision 0.1857 | AP 0.3556 | F1 0.1857 | Time 7.92\n",
      "Epoch 0063: Loss 110542.0078 | AUC 0.3317 | Recall 0.1797 | Precision 0.1797 | AP 0.3518 | F1 0.1797 | Time 8.42\n",
      "Epoch 0064: Loss 110453.7500 | AUC 0.3178 | Recall 0.1700 | Precision 0.1700 | AP 0.3451 | F1 0.1700 | Time 9.61\n",
      "Epoch 0065: Loss 110344.3984 | AUC 0.3010 | Recall 0.1598 | Precision 0.1598 | AP 0.3379 | F1 0.1598 | Time 8.54\n",
      "Epoch 0066: Loss 110239.0312 | AUC 0.2865 | Recall 0.1505 | Precision 0.1505 | AP 0.3307 | F1 0.1505 | Time 8.23\n",
      "Epoch 0067: Loss 110165.3516 | AUC 0.2763 | Recall 0.1474 | Precision 0.1474 | AP 0.3264 | F1 0.1474 | Time 8.15\n",
      "Epoch 0068: Loss 110145.8750 | AUC 0.2684 | Recall 0.1401 | Precision 0.1401 | AP 0.3219 | F1 0.1401 | Time 8.08\n",
      "Epoch 0069: Loss 110183.9297 | AUC 0.2602 | Recall 0.1319 | Precision 0.1319 | AP 0.3180 | F1 0.1320 | Time 8.13\n",
      "Epoch 0070: Loss 110251.6875 | AUC 0.2549 | Recall 0.1310 | Precision 0.1310 | AP 0.3164 | F1 0.1310 | Time 8.46\n",
      "Epoch 0071: Loss 110301.7344 | AUC 0.2522 | Recall 0.1305 | Precision 0.1305 | AP 0.3150 | F1 0.1305 | Time 8.97\n",
      "Epoch 0072: Loss 110299.7344 | AUC 0.2523 | Recall 0.1306 | Precision 0.1306 | AP 0.3151 | F1 0.1306 | Time 9.18\n",
      "Epoch 0073: Loss 110252.8750 | AUC 0.2548 | Recall 0.1310 | Precision 0.1310 | AP 0.3163 | F1 0.1310 | Time 9.21\n",
      "Epoch 0074: Loss 110194.6562 | AUC 0.2592 | Recall 0.1318 | Precision 0.1318 | AP 0.3176 | F1 0.1318 | Time 8.59\n",
      "Epoch 0075: Loss 110155.1719 | AUC 0.2643 | Recall 0.1366 | Precision 0.1366 | AP 0.3197 | F1 0.1366 | Time 8.84\n",
      "Epoch 0076: Loss 110145.1641 | AUC 0.2704 | Recall 0.1420 | Precision 0.1420 | AP 0.3231 | F1 0.1420 | Time 8.66\n",
      "Epoch 0077: Loss 110158.2734 | AUC 0.2748 | Recall 0.1463 | Precision 0.1463 | AP 0.3256 | F1 0.1463 | Time 8.48\n",
      "Epoch 0078: Loss 110180.9844 | AUC 0.2789 | Recall 0.1485 | Precision 0.1485 | AP 0.3276 | F1 0.1485 | Time 8.51\n",
      "Epoch 0079: Loss 110200.8125 | AUC 0.2823 | Recall 0.1492 | Precision 0.1492 | AP 0.3291 | F1 0.1492 | Time 8.35\n",
      "Epoch 0080: Loss 110210.0312 | AUC 0.2833 | Recall 0.1492 | Precision 0.1492 | AP 0.3295 | F1 0.1492 | Time 8.63\n",
      "Epoch 0081: Loss 110206.4453 | AUC 0.2830 | Recall 0.1494 | Precision 0.1494 | AP 0.3294 | F1 0.1494 | Time 8.30\n",
      "Epoch 0082: Loss 110192.3672 | AUC 0.2811 | Recall 0.1489 | Precision 0.1489 | AP 0.3286 | F1 0.1489 | Time 8.43\n",
      "Epoch 0083: Loss 110173.3281 | AUC 0.2777 | Recall 0.1481 | Precision 0.1481 | AP 0.3270 | F1 0.1481 | Time 8.36\n",
      "Epoch 0084: Loss 110156.0312 | AUC 0.2741 | Recall 0.1459 | Precision 0.1459 | AP 0.3252 | F1 0.1459 | Time 8.28\n",
      "Epoch 0085: Loss 110146.1328 | AUC 0.2711 | Recall 0.1424 | Precision 0.1424 | AP 0.3235 | F1 0.1424 | Time 8.35\n",
      "Epoch 0086: Loss 110146.0938 | AUC 0.2683 | Recall 0.1401 | Precision 0.1401 | AP 0.3218 | F1 0.1401 | Time 8.98\n",
      "Epoch 0087: Loss 110153.8359 | AUC 0.2647 | Recall 0.1366 | Precision 0.1366 | AP 0.3199 | F1 0.1366 | Time 8.01\n",
      "Epoch 0088: Loss 110163.6875 | AUC 0.2625 | Recall 0.1345 | Precision 0.1345 | AP 0.3189 | F1 0.1345 | Time 8.01\n",
      "Epoch 0089: Loss 110169.4844 | AUC 0.2615 | Recall 0.1336 | Precision 0.1336 | AP 0.3184 | F1 0.1336 | Time 7.97\n",
      "Epoch 0090: Loss 110168.1406 | AUC 0.2618 | Recall 0.1338 | Precision 0.1338 | AP 0.3186 | F1 0.1338 | Time 7.99\n",
      "Epoch 0091: Loss 110161.0234 | AUC 0.2629 | Recall 0.1347 | Precision 0.1347 | AP 0.3191 | F1 0.1347 | Time 7.96\n",
      "Epoch 0092: Loss 110152.3984 | AUC 0.2652 | Recall 0.1370 | Precision 0.1370 | AP 0.3201 | F1 0.1370 | Time 8.38\n",
      "Epoch 0093: Loss 110146.3906 | AUC 0.2681 | Recall 0.1396 | Precision 0.1396 | AP 0.3217 | F1 0.1396 | Time 8.48\n",
      "Epoch 0094: Loss 110144.9453 | AUC 0.2702 | Recall 0.1412 | Precision 0.1412 | AP 0.3230 | F1 0.1412 | Time 7.89\n",
      "Epoch 0095: Loss 110147.3828 | AUC 0.2716 | Recall 0.1427 | Precision 0.1427 | AP 0.3238 | F1 0.1427 | Time 7.90\n",
      "Epoch 0096: Loss 110151.3828 | AUC 0.2728 | Recall 0.1446 | Precision 0.1446 | AP 0.3245 | F1 0.1446 | Time 8.08\n",
      "Epoch 0097: Loss 110154.5234 | AUC 0.2737 | Recall 0.1455 | Precision 0.1455 | AP 0.3249 | F1 0.1455 | Time 7.70\n",
      "Epoch 0098: Loss 110155.4922 | AUC 0.2740 | Recall 0.1455 | Precision 0.1455 | AP 0.3252 | F1 0.1455 | Time 7.90\n",
      "Epoch 0099: Loss 110154.0000 | AUC 0.2736 | Recall 0.1451 | Precision 0.1451 | AP 0.3248 | F1 0.1451 | Time 8.09\n",
      "Test: Loss 16.8981 | AUC 0.2739 | Recall 0.3140 | Precision 0.3140 | AP 0.4133 | F1 0.3140 | Time 1.52\n",
      "Epoch 0000: Loss 115987.6484 | AUC 0.7865 | Recall 0.6501 | Precision 0.6501 | AP 0.6582 | F1 0.7300 | Time 5.47\n",
      "Epoch 0001: Loss 115986.3594 | AUC 0.7874 | Recall 0.6621 | Precision 0.6621 | AP 0.6620 | F1 0.6622 | Time 5.92\n",
      "Epoch 0002: Loss 115984.5391 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6620 | Time 5.69\n",
      "Epoch 0003: Loss 115982.8047 | AUC 0.7874 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6620 | Time 5.84\n",
      "Epoch 0004: Loss 115981.0469 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6621 | Time 6.16\n",
      "Epoch 0005: Loss 115979.1719 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6620 | F1 0.6620 | Time 5.67\n",
      "Epoch 0006: Loss 115977.1719 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.52\n",
      "Epoch 0007: Loss 115975.0234 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.56\n",
      "Epoch 0008: Loss 115972.7266 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 5.61\n",
      "Epoch 0009: Loss 115970.2344 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.81\n",
      "Epoch 0010: Loss 115967.5469 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.92\n",
      "Epoch 0011: Loss 115964.5781 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.83\n",
      "Epoch 0012: Loss 115961.3047 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 6.06\n",
      "Epoch 0013: Loss 115957.6719 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 5.84\n",
      "Epoch 0014: Loss 115953.6172 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6622 | F1 0.6620 | Time 6.18\n",
      "Epoch 0015: Loss 115949.0547 | AUC 0.7876 | Recall 0.6620 | Precision 0.6620 | AP 0.6624 | F1 0.6620 | Time 6.82\n",
      "Epoch 0016: Loss 115943.9141 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.93\n",
      "Epoch 0017: Loss 115938.0625 | AUC 0.7875 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.98\n",
      "Epoch 0018: Loss 115931.3359 | AUC 0.7876 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6620 | Time 5.79\n",
      "Epoch 0019: Loss 115923.5391 | AUC 0.7876 | Recall 0.6620 | Precision 0.6620 | AP 0.6624 | F1 0.6620 | Time 5.77\n",
      "Epoch 0020: Loss 115914.4453 | AUC 0.7876 | Recall 0.6620 | Precision 0.6620 | AP 0.6624 | F1 0.6620 | Time 5.68\n",
      "Epoch 0021: Loss 115903.7109 | AUC 0.7876 | Recall 0.6621 | Precision 0.6621 | AP 0.6624 | F1 0.6621 | Time 5.77\n",
      "Epoch 0022: Loss 115891.0078 | AUC 0.7877 | Recall 0.6621 | Precision 0.6621 | AP 0.6625 | F1 0.6621 | Time 5.74\n",
      "Epoch 0023: Loss 115875.7891 | AUC 0.7877 | Recall 0.6623 | Precision 0.6623 | AP 0.6626 | F1 0.6623 | Time 5.77\n",
      "Epoch 0024: Loss 115857.3906 | AUC 0.7877 | Recall 0.6623 | Precision 0.6623 | AP 0.6626 | F1 0.6623 | Time 6.33\n",
      "Epoch 0025: Loss 115835.0156 | AUC 0.7877 | Recall 0.6625 | Precision 0.6625 | AP 0.6627 | F1 0.6629 | Time 5.62\n",
      "Epoch 0026: Loss 115807.4609 | AUC 0.7878 | Recall 0.6625 | Precision 0.6625 | AP 0.6626 | F1 0.6628 | Time 5.76\n",
      "Epoch 0027: Loss 115773.3359 | AUC 0.7878 | Recall 0.6627 | Precision 0.6627 | AP 0.6626 | F1 0.6628 | Time 5.43\n",
      "Epoch 0028: Loss 115730.7344 | AUC 0.7878 | Recall 0.6627 | Precision 0.6627 | AP 0.6627 | F1 0.6627 | Time 5.47\n",
      "Epoch 0029: Loss 115676.9766 | AUC 0.7877 | Recall 0.6625 | Precision 0.6625 | AP 0.6626 | F1 0.6625 | Time 5.55\n",
      "Epoch 0030: Loss 115608.5078 | AUC 0.7878 | Recall 0.6625 | Precision 0.6625 | AP 0.6627 | F1 0.6626 | Time 5.55\n",
      "Epoch 0031: Loss 115520.4688 | AUC 0.7879 | Recall 0.6625 | Precision 0.6625 | AP 0.6628 | F1 0.6625 | Time 5.49\n",
      "Epoch 0032: Loss 115406.5078 | AUC 0.7881 | Recall 0.6623 | Precision 0.6623 | AP 0.6629 | F1 0.6623 | Time 5.54\n",
      "Epoch 0033: Loss 115258.0938 | AUC 0.7878 | Recall 0.6616 | Precision 0.6616 | AP 0.6627 | F1 0.6616 | Time 5.55\n",
      "Epoch 0034: Loss 115064.2891 | AUC 0.7878 | Recall 0.6618 | Precision 0.6618 | AP 0.6627 | F1 0.6618 | Time 5.74\n",
      "Epoch 0035: Loss 114809.1172 | AUC 0.7881 | Recall 0.6616 | Precision 0.6616 | AP 0.6627 | F1 0.6616 | Time 5.62\n",
      "Epoch 0036: Loss 114472.0781 | AUC 0.7879 | Recall 0.6603 | Precision 0.6603 | AP 0.6622 | F1 0.6603 | Time 5.53\n",
      "Epoch 0037: Loss 114026.9844 | AUC 0.7888 | Recall 0.6597 | Precision 0.6597 | AP 0.6627 | F1 0.6597 | Time 5.76\n",
      "Epoch 0038: Loss 113443.6328 | AUC 0.7427 | Recall 0.6620 | Precision 0.6620 | AP 0.6481 | F1 0.6620 | Time 5.56\n",
      "Epoch 0039: Loss 112694.5234 | AUC 0.6041 | Recall 0.4364 | Precision 0.4364 | AP 0.5312 | F1 0.4364 | Time 5.56\n",
      "Epoch 0040: Loss 111779.0156 | AUC 0.4857 | Recall 0.3085 | Precision 0.3085 | AP 0.4477 | F1 0.3085 | Time 5.80\n",
      "Epoch 0041: Loss 110792.6484 | AUC 0.3700 | Recall 0.2180 | Precision 0.2180 | AP 0.3754 | F1 0.2180 | Time 5.57\n",
      "Epoch 0042: Loss 110151.9844 | AUC 0.2719 | Recall 0.1444 | Precision 0.1444 | AP 0.3242 | F1 0.1444 | Time 5.49\n",
      "Epoch 0043: Loss 111015.8125 | AUC 0.2376 | Recall 0.1230 | Precision 0.1230 | AP 0.3088 | F1 0.1230 | Time 5.60\n",
      "Epoch 0044: Loss 111422.3672 | AUC 0.2441 | Recall 0.1507 | Precision 0.1507 | AP 0.3077 | F1 0.1507 | Time 5.56\n",
      "Epoch 0045: Loss 110809.2969 | AUC 0.2402 | Recall 0.1197 | Precision 0.1197 | AP 0.3103 | F1 0.1197 | Time 5.59\n",
      "Epoch 0046: Loss 110277.8203 | AUC 0.2529 | Recall 0.1299 | Precision 0.1299 | AP 0.3155 | F1 0.1299 | Time 5.64\n",
      "Epoch 0047: Loss 110145.3828 | AUC 0.2708 | Recall 0.1416 | Precision 0.1416 | AP 0.3236 | F1 0.1416 | Time 5.53\n",
      "Epoch 0048: Loss 110260.2109 | AUC 0.2888 | Recall 0.1513 | Precision 0.1513 | AP 0.3318 | F1 0.1513 | Time 5.52\n",
      "Epoch 0049: Loss 110435.7188 | AUC 0.3147 | Recall 0.1680 | Precision 0.1680 | AP 0.3437 | F1 0.1680 | Time 5.60\n",
      "Epoch 0050: Loss 110572.7109 | AUC 0.3360 | Recall 0.1832 | Precision 0.1832 | AP 0.3544 | F1 0.1832 | Time 5.61\n",
      "Epoch 0051: Loss 110636.6406 | AUC 0.3457 | Recall 0.1929 | Precision 0.1929 | AP 0.3599 | F1 0.1929 | Time 5.67\n",
      "Epoch 0052: Loss 110624.5938 | AUC 0.3437 | Recall 0.1918 | Precision 0.1918 | AP 0.3587 | F1 0.1918 | Time 5.76\n",
      "Epoch 0053: Loss 110548.3438 | AUC 0.3330 | Recall 0.1799 | Precision 0.1799 | AP 0.3525 | F1 0.1799 | Time 6.12\n",
      "Epoch 0054: Loss 110428.9297 | AUC 0.3136 | Recall 0.1678 | Precision 0.1678 | AP 0.3433 | F1 0.1678 | Time 6.33\n",
      "Epoch 0055: Loss 110296.4766 | AUC 0.2932 | Recall 0.1552 | Precision 0.1552 | AP 0.3339 | F1 0.1552 | Time 6.22\n",
      "Epoch 0056: Loss 110188.8281 | AUC 0.2803 | Recall 0.1485 | Precision 0.1485 | AP 0.3283 | F1 0.1485 | Time 6.13\n",
      "Epoch 0057: Loss 110144.1328 | AUC 0.2699 | Recall 0.1407 | Precision 0.1407 | AP 0.3230 | F1 0.1407 | Time 6.13\n",
      "Epoch 0058: Loss 110180.1016 | AUC 0.2598 | Recall 0.1314 | Precision 0.1314 | AP 0.3180 | F1 0.1314 | Time 6.11\n",
      "Epoch 0059: Loss 110267.6562 | AUC 0.2536 | Recall 0.1305 | Precision 0.1305 | AP 0.3159 | F1 0.1305 | Time 6.23\n",
      "Epoch 0060: Loss 110335.1016 | AUC 0.2509 | Recall 0.1284 | Precision 0.1284 | AP 0.3146 | F1 0.1284 | Time 6.62\n",
      "Epoch 0061: Loss 110328.3750 | AUC 0.2511 | Recall 0.1288 | Precision 0.1288 | AP 0.3147 | F1 0.1288 | Time 6.63\n",
      "Epoch 0062: Loss 110260.5781 | AUC 0.2540 | Recall 0.1306 | Precision 0.1306 | AP 0.3161 | F1 0.1306 | Time 7.52\n",
      "Epoch 0063: Loss 110186.5703 | AUC 0.2594 | Recall 0.1310 | Precision 0.1310 | AP 0.3178 | F1 0.1310 | Time 7.12\n",
      "Epoch 0064: Loss 110147.5078 | AUC 0.2666 | Recall 0.1377 | Precision 0.1377 | AP 0.3210 | F1 0.1377 | Time 7.16\n",
      "Epoch 0065: Loss 110149.7031 | AUC 0.2722 | Recall 0.1437 | Precision 0.1437 | AP 0.3242 | F1 0.1437 | Time 7.56\n",
      "Epoch 0066: Loss 110176.5547 | AUC 0.2782 | Recall 0.1476 | Precision 0.1476 | AP 0.3274 | F1 0.1476 | Time 7.46\n",
      "Epoch 0067: Loss 110206.9375 | AUC 0.2826 | Recall 0.1489 | Precision 0.1489 | AP 0.3293 | F1 0.1489 | Time 7.66\n",
      "Epoch 0068: Loss 110225.6641 | AUC 0.2847 | Recall 0.1500 | Precision 0.1500 | AP 0.3302 | F1 0.1500 | Time 7.98\n",
      "Epoch 0069: Loss 110226.2578 | AUC 0.2847 | Recall 0.1500 | Precision 0.1500 | AP 0.3302 | F1 0.1500 | Time 8.00\n",
      "Epoch 0070: Loss 110210.1719 | AUC 0.2830 | Recall 0.1492 | Precision 0.1492 | AP 0.3294 | F1 0.1492 | Time 8.24\n",
      "Epoch 0071: Loss 110184.6250 | AUC 0.2797 | Recall 0.1481 | Precision 0.1481 | AP 0.3280 | F1 0.1481 | Time 8.25\n",
      "Epoch 0072: Loss 110159.8594 | AUC 0.2750 | Recall 0.1463 | Precision 0.1463 | AP 0.3258 | F1 0.1463 | Time 8.09\n",
      "Epoch 0073: Loss 110145.5703 | AUC 0.2709 | Recall 0.1418 | Precision 0.1418 | AP 0.3235 | F1 0.1418 | Time 8.38\n",
      "Epoch 0074: Loss 110146.5547 | AUC 0.2672 | Recall 0.1381 | Precision 0.1381 | AP 0.3213 | F1 0.1381 | Time 8.47\n",
      "Epoch 0075: Loss 110159.3203 | AUC 0.2626 | Recall 0.1340 | Precision 0.1340 | AP 0.3191 | F1 0.1340 | Time 8.31\n",
      "Epoch 0076: Loss 110173.4297 | AUC 0.2606 | Recall 0.1323 | Precision 0.1323 | AP 0.3182 | F1 0.1323 | Time 8.39\n",
      "Epoch 0077: Loss 110178.7500 | AUC 0.2602 | Recall 0.1314 | Precision 0.1314 | AP 0.3180 | F1 0.1314 | Time 8.25\n",
      "Epoch 0078: Loss 110172.2812 | AUC 0.2607 | Recall 0.1323 | Precision 0.1323 | AP 0.3182 | F1 0.1323 | Time 8.46\n",
      "Epoch 0079: Loss 110159.3984 | AUC 0.2626 | Recall 0.1340 | Precision 0.1340 | AP 0.3191 | F1 0.1340 | Time 8.81\n",
      "Epoch 0080: Loss 110148.2969 | AUC 0.2663 | Recall 0.1375 | Precision 0.1375 | AP 0.3209 | F1 0.1375 | Time 8.69\n",
      "Epoch 0081: Loss 110144.1172 | AUC 0.2694 | Recall 0.1407 | Precision 0.1407 | AP 0.3226 | F1 0.1407 | Time 8.68\n",
      "Epoch 0082: Loss 110146.8906 | AUC 0.2714 | Recall 0.1422 | Precision 0.1422 | AP 0.3239 | F1 0.1422 | Time 8.32\n",
      "Epoch 0083: Loss 110153.0078 | AUC 0.2731 | Recall 0.1451 | Precision 0.1451 | AP 0.3247 | F1 0.1451 | Time 8.28\n",
      "Epoch 0084: Loss 110158.2812 | AUC 0.2746 | Recall 0.1463 | Precision 0.1463 | AP 0.3256 | F1 0.1463 | Time 8.12\n",
      "Epoch 0085: Loss 110159.8672 | AUC 0.2750 | Recall 0.1461 | Precision 0.1461 | AP 0.3258 | F1 0.1461 | Time 8.23\n",
      "Epoch 0086: Loss 110157.3125 | AUC 0.2743 | Recall 0.1461 | Precision 0.1461 | AP 0.3254 | F1 0.1461 | Time 8.38\n",
      "Epoch 0087: Loss 110152.1484 | AUC 0.2728 | Recall 0.1450 | Precision 0.1450 | AP 0.3245 | F1 0.1450 | Time 8.39\n",
      "Epoch 0088: Loss 110147.0391 | AUC 0.2715 | Recall 0.1422 | Precision 0.1422 | AP 0.3239 | F1 0.1422 | Time 8.06\n",
      "Epoch 0089: Loss 110144.2969 | AUC 0.2700 | Recall 0.1409 | Precision 0.1409 | AP 0.3230 | F1 0.1409 | Time 8.22\n",
      "Epoch 0090: Loss 110144.7578 | AUC 0.2685 | Recall 0.1396 | Precision 0.1396 | AP 0.3221 | F1 0.1396 | Time 8.19\n",
      "Epoch 0091: Loss 110147.3672 | AUC 0.2668 | Recall 0.1377 | Precision 0.1377 | AP 0.3211 | F1 0.1377 | Time 8.27\n",
      "Epoch 0092: Loss 110149.9375 | AUC 0.2656 | Recall 0.1371 | Precision 0.1371 | AP 0.3204 | F1 0.1371 | Time 8.17\n",
      "Epoch 0093: Loss 110150.5938 | AUC 0.2653 | Recall 0.1371 | Precision 0.1371 | AP 0.3202 | F1 0.1371 | Time 8.55\n",
      "Epoch 0094: Loss 110149.0938 | AUC 0.2660 | Recall 0.1373 | Precision 0.1373 | AP 0.3208 | F1 0.1373 | Time 9.37\n",
      "Epoch 0095: Loss 110146.5469 | AUC 0.2672 | Recall 0.1381 | Precision 0.1381 | AP 0.3213 | F1 0.1381 | Time 8.17\n",
      "Epoch 0096: Loss 110144.6172 | AUC 0.2686 | Recall 0.1396 | Precision 0.1396 | AP 0.3222 | F1 0.1396 | Time 8.64\n",
      "Epoch 0097: Loss 110144.1250 | AUC 0.2699 | Recall 0.1407 | Precision 0.1407 | AP 0.3229 | F1 0.1407 | Time 8.45\n",
      "Epoch 0098: Loss 110144.9844 | AUC 0.2706 | Recall 0.1416 | Precision 0.1416 | AP 0.3234 | F1 0.1416 | Time 8.76\n",
      "Epoch 0099: Loss 110146.3438 | AUC 0.2712 | Recall 0.1420 | Precision 0.1420 | AP 0.3237 | F1 0.1420 | Time 8.39\n",
      "Test: Loss 16.8948 | AUC 0.2729 | Recall 0.3133 | Precision 0.3133 | AP 0.4129 | F1 0.3133 | Time 1.54\n"
     ]
    }
   ],
   "source": [
    "f1_gae = []\n",
    "f1_gae_for = []\n",
    "precision_gae = []\n",
    "recall_gae = []\n",
    "train_time_gae = []\n",
    "test_time_gae = []\n",
    "for i in range(3):\n",
    "    start_time = time.time()\n",
    "    gae_model, graph_test = make_gae_model(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n",
    "    train_time_gae.append(time.time() - start_time)\n",
    "    start_time = time.time()\n",
    "    f1_score_ip, precision_score, recall_score, f1_score = predict_gae(label_test, gae_model, graph_test)\n",
    "    test_time_gae.append(time.time() - start_time)\n",
    "    f1_gae.append(f1_score_ip)\n",
    "    precision_gae.append(precision_score)\n",
    "    recall_gae.append(recall_score)\n",
    "    f1_gae_for.append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18174696435566, 0.18058277799555733, 0.17520357236669296]\n",
      "[tensor(0.4510), tensor(0.4510), tensor(0.4510)]\n",
      "[tensor(1.), tensor(1.), tensor(1.)]\n",
      "[tensor(0.6217), tensor(0.6217), tensor(0.6217)]\n"
     ]
    }
   ],
   "source": [
    "print(f1_gae)\n",
    "print(precision_gae)\n",
    "print(recall_gae)\n",
    "print(f1_gae_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[732.9163107872009, 711.4018852710724, 709.7734138965607]\n",
      "[3.3277716636657715, 2.8125693798065186, 3.137454032897949]\n"
     ]
    }
   ],
   "source": [
    "print(train_time_gae)\n",
    "print(test_time_gae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conad_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features,\n",
    "                        label_test):\n",
    "    \n",
    "    node_features = torch.tensor(node_features)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train.cpu()\n",
    "    pyG_train.x = train_node_features.cpu()\n",
    "    label_train = label_train.cpu()\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    conad_model = CONAD(hid_dim=64, num_layers=64, \n",
    "                        contamination=0.37, lr=0.001, \n",
    "                        epoch=100, gpu=-1, num_neigh=-1, \n",
    "                        weight=0.5, eta=0.5, \n",
    "                        margin=0.5, verbose=3)\n",
    "    conad_compile = conad_model.fit(pyG_train, label_train)\n",
    "\n",
    "    return conad_compile, pyG_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_conad(label_test, conda_compile, pyG_test):\n",
    "    conad_ip_pred_res, conad_ip_score_res, conad_ip_prob_res, conad_ip_conf_res = conda_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, return_prob=True, prob_method='linear', return_conf=True)\n",
    "    f1_score_ip = eval_f1(label_test, conad_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, conad_ip_score_res, k=len(label_test))\n",
    "    recall = eval_recall_at_k(label_test, conad_ip_score_res, k=len(label_test))\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    return f1_score_ip, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 19.1913 | AUC 0.7865 | Recall 0.6527 | Precision 0.6527 | AP 0.6587 | F1 0.7198 | Time 17.34\n",
      "Epoch 0001: Loss 19.1892 | AUC 0.7872 | Recall 0.6597 | Precision 0.6597 | AP 0.6611 | F1 0.6597 | Time 12.43\n",
      "Epoch 0002: Loss 19.1889 | AUC 0.7870 | Recall 0.6597 | Precision 0.6597 | AP 0.6612 | F1 0.6750 | Time 12.31\n",
      "Epoch 0003: Loss 19.1858 | AUC 0.7875 | Recall 0.6597 | Precision 0.6597 | AP 0.6619 | F1 0.6722 | Time 12.76\n",
      "Epoch 0004: Loss 19.1848 | AUC 0.7876 | Recall 0.6597 | Precision 0.6597 | AP 0.6619 | F1 0.6597 | Time 12.71\n",
      "Epoch 0005: Loss 19.1805 | AUC 0.7875 | Recall 0.6597 | Precision 0.6597 | AP 0.6618 | F1 0.6597 | Time 12.49\n",
      "Epoch 0006: Loss 19.1790 | AUC 0.7875 | Recall 0.6599 | Precision 0.6599 | AP 0.6620 | F1 0.6613 | Time 12.24\n",
      "Epoch 0007: Loss 19.1766 | AUC 0.7872 | Recall 0.6599 | Precision 0.6599 | AP 0.6620 | F1 0.6686 | Time 12.27\n",
      "Epoch 0008: Loss 19.1746 | AUC 0.7875 | Recall 0.6599 | Precision 0.6599 | AP 0.6620 | F1 0.6683 | Time 12.89\n",
      "Epoch 0009: Loss 19.1706 | AUC 0.7876 | Recall 0.6599 | Precision 0.6599 | AP 0.6622 | F1 0.6675 | Time 13.59\n",
      "Epoch 0010: Loss 19.1688 | AUC 0.7876 | Recall 0.6599 | Precision 0.6599 | AP 0.6622 | F1 0.6662 | Time 12.50\n",
      "Epoch 0011: Loss 19.1621 | AUC 0.7876 | Recall 0.6599 | Precision 0.6599 | AP 0.6622 | F1 0.6629 | Time 12.67\n",
      "Epoch 0012: Loss 19.1576 | AUC 0.7876 | Recall 0.6603 | Precision 0.6603 | AP 0.6622 | F1 0.6655 | Time 12.33\n",
      "Epoch 0013: Loss 19.1506 | AUC 0.7876 | Recall 0.6608 | Precision 0.6608 | AP 0.6624 | F1 0.6647 | Time 12.32\n",
      "Epoch 0014: Loss 19.1448 | AUC 0.7877 | Recall 0.6612 | Precision 0.6612 | AP 0.6625 | F1 0.6648 | Time 12.16\n",
      "Epoch 0015: Loss 19.1380 | AUC 0.7880 | Recall 0.6616 | Precision 0.6616 | AP 0.6627 | F1 0.6644 | Time 12.52\n",
      "Epoch 0016: Loss 19.1276 | AUC 0.7877 | Recall 0.6616 | Precision 0.6616 | AP 0.6626 | F1 0.6641 | Time 12.31\n",
      "Epoch 0017: Loss 19.1160 | AUC 0.7879 | Recall 0.6618 | Precision 0.6618 | AP 0.6627 | F1 0.6631 | Time 12.43\n",
      "Epoch 0018: Loss 19.1020 | AUC 0.7879 | Recall 0.6620 | Precision 0.6620 | AP 0.6627 | F1 0.6628 | Time 12.29\n",
      "Epoch 0019: Loss 19.0806 | AUC 0.7879 | Recall 0.6620 | Precision 0.6620 | AP 0.6628 | F1 0.6625 | Time 12.95\n",
      "Epoch 0020: Loss 19.0568 | AUC 0.7879 | Recall 0.6621 | Precision 0.6621 | AP 0.6628 | F1 0.6623 | Time 13.39\n",
      "Epoch 0021: Loss 19.0238 | AUC 0.7878 | Recall 0.6621 | Precision 0.6621 | AP 0.6629 | F1 0.6622 | Time 15.97\n",
      "Epoch 0022: Loss 18.9822 | AUC 0.7878 | Recall 0.6625 | Precision 0.6625 | AP 0.6628 | F1 0.6625 | Time 12.56\n",
      "Epoch 0023: Loss 18.9219 | AUC 0.7877 | Recall 0.6631 | Precision 0.6631 | AP 0.6629 | F1 0.6631 | Time 12.19\n",
      "Epoch 0024: Loss 18.8369 | AUC 0.7876 | Recall 0.6633 | Precision 0.6633 | AP 0.6628 | F1 0.6633 | Time 11.96\n",
      "Epoch 0025: Loss 18.7129 | AUC 0.7872 | Recall 0.6633 | Precision 0.6633 | AP 0.6626 | F1 0.6633 | Time 12.28\n",
      "Epoch 0026: Loss 18.5239 | AUC 0.7864 | Recall 0.6627 | Precision 0.6627 | AP 0.6620 | F1 0.6627 | Time 12.50\n",
      "Epoch 0027: Loss 18.2207 | AUC 0.7836 | Recall 0.6586 | Precision 0.6586 | AP 0.6597 | F1 0.6586 | Time 12.75\n",
      "Epoch 0028: Loss 17.7058 | AUC 0.7780 | Recall 0.6501 | Precision 0.6501 | AP 0.6551 | F1 0.6501 | Time 13.59\n",
      "Epoch 0029: Loss 16.7144 | AUC 0.7555 | Recall 0.6179 | Precision 0.6179 | AP 0.6369 | F1 0.6179 | Time 12.46\n",
      "Epoch 0030: Loss 14.8169 | AUC 0.6846 | Recall 0.5127 | Precision 0.5127 | AP 0.5776 | F1 0.5127 | Time 12.98\n",
      "Epoch 0031: Loss 16.2047 | AUC 0.8112 | Recall 0.7326 | Precision 0.7326 | AP 0.6909 | F1 0.7326 | Time 12.64\n",
      "Epoch 0032: Loss 14.4879 | AUC 0.7821 | Recall 0.6647 | Precision 0.6647 | AP 0.6553 | F1 0.6647 | Time 12.52\n",
      "Epoch 0033: Loss 14.4423 | AUC 0.6161 | Recall 0.6060 | Precision 0.6060 | AP 0.5816 | F1 0.6060 | Time 12.59\n",
      "Epoch 0034: Loss 14.7546 | AUC 0.6512 | Recall 0.5603 | Precision 0.5603 | AP 0.5843 | F1 0.5603 | Time 12.37\n",
      "Epoch 0035: Loss 14.9080 | AUC 0.6959 | Recall 0.5897 | Precision 0.5897 | AP 0.6075 | F1 0.5897 | Time 12.69\n",
      "Epoch 0036: Loss 14.8872 | AUC 0.6909 | Recall 0.6040 | Precision 0.6040 | AP 0.6110 | F1 0.6040 | Time 12.66\n",
      "Epoch 0037: Loss 14.7273 | AUC 0.6352 | Recall 0.6110 | Precision 0.6110 | AP 0.5930 | F1 0.6110 | Time 12.28\n",
      "Epoch 0038: Loss 14.4779 | AUC 0.5933 | Recall 0.5243 | Precision 0.5243 | AP 0.5513 | F1 0.5243 | Time 12.48\n",
      "Epoch 0039: Loss 14.3424 | AUC 0.6627 | Recall 0.4233 | Precision 0.4233 | AP 0.5413 | F1 0.4233 | Time 12.23\n",
      "Epoch 0040: Loss 14.8761 | AUC 0.5743 | Recall 0.3739 | Precision 0.3739 | AP 0.4944 | F1 0.3739 | Time 12.11\n",
      "Epoch 0041: Loss 14.8176 | AUC 0.5761 | Recall 0.3774 | Precision 0.3774 | AP 0.4965 | F1 0.3774 | Time 12.30\n",
      "Epoch 0042: Loss 14.3566 | AUC 0.6599 | Recall 0.4168 | Precision 0.4168 | AP 0.5386 | F1 0.4168 | Time 12.24\n",
      "Epoch 0043: Loss 14.4555 | AUC 0.5929 | Recall 0.4811 | Precision 0.4811 | AP 0.5387 | F1 0.4811 | Time 12.29\n",
      "Epoch 0044: Loss 14.6296 | AUC 0.6161 | Recall 0.6092 | Precision 0.6092 | AP 0.5845 | F1 0.6092 | Time 12.20\n",
      "Epoch 0045: Loss 14.7143 | AUC 0.6310 | Recall 0.6281 | Precision 0.6281 | AP 0.5958 | F1 0.6281 | Time 12.58\n",
      "Epoch 0046: Loss 14.6879 | AUC 0.6248 | Recall 0.6283 | Precision 0.6283 | AP 0.5929 | F1 0.6283 | Time 12.08\n",
      "Epoch 0047: Loss 14.5718 | AUC 0.6042 | Recall 0.5759 | Precision 0.5759 | AP 0.5708 | F1 0.5759 | Time 12.94\n",
      "Epoch 0048: Loss 14.4191 | AUC 0.5928 | Recall 0.4635 | Precision 0.4635 | AP 0.5335 | F1 0.4635 | Time 12.55\n",
      "Epoch 0049: Loss 14.3358 | AUC 0.6556 | Recall 0.4341 | Precision 0.4341 | AP 0.5429 | F1 0.4341 | Time 12.52\n",
      "Epoch 0050: Loss 14.6155 | AUC 0.6208 | Recall 0.4103 | Precision 0.4103 | AP 0.5209 | F1 0.4103 | Time 12.10\n",
      "Epoch 0051: Loss 14.5896 | AUC 0.6292 | Recall 0.4196 | Precision 0.4196 | AP 0.5258 | F1 0.4196 | Time 12.53\n",
      "Epoch 0052: Loss 14.3234 | AUC 0.6611 | Recall 0.4427 | Precision 0.4427 | AP 0.5467 | F1 0.4427 | Time 12.20\n",
      "Epoch 0053: Loss 14.3649 | AUC 0.6121 | Recall 0.4637 | Precision 0.4637 | AP 0.5388 | F1 0.4637 | Time 12.94\n",
      "Epoch 0054: Loss 14.4167 | AUC 0.5965 | Recall 0.5111 | Precision 0.5111 | AP 0.5462 | F1 0.5111 | Time 12.10\n",
      "Epoch 0055: Loss 14.4328 | AUC 0.5946 | Recall 0.5350 | Precision 0.5350 | AP 0.5533 | F1 0.5350 | Time 12.36\n",
      "Epoch 0056: Loss 14.3958 | AUC 0.6008 | Recall 0.5226 | Precision 0.5226 | AP 0.5506 | F1 0.5226 | Time 12.05\n",
      "Epoch 0057: Loss 14.3343 | AUC 0.6302 | Recall 0.5109 | Precision 0.5109 | AP 0.5570 | F1 0.5109 | Time 12.07\n",
      "Epoch 0058: Loss 14.3113 | AUC 0.6835 | Recall 0.5289 | Precision 0.5289 | AP 0.5799 | F1 0.5289 | Time 12.69\n",
      "Epoch 0059: Loss 14.3483 | AUC 0.7256 | Recall 0.5594 | Precision 0.5594 | AP 0.6007 | F1 0.5594 | Time 12.91\n",
      "Epoch 0060: Loss 14.3836 | AUC 0.7337 | Recall 0.5636 | Precision 0.5636 | AP 0.6057 | F1 0.5636 | Time 12.29\n",
      "Epoch 0061: Loss 14.3247 | AUC 0.7149 | Recall 0.5581 | Precision 0.5581 | AP 0.5977 | F1 0.5581 | Time 12.07\n",
      "Epoch 0062: Loss 14.3116 | AUC 0.6756 | Recall 0.5421 | Precision 0.5421 | AP 0.5809 | F1 0.5421 | Time 13.68\n",
      "Epoch 0063: Loss 14.3276 | AUC 0.6391 | Recall 0.5365 | Precision 0.5365 | AP 0.5668 | F1 0.5365 | Time 12.67\n",
      "Epoch 0064: Loss 14.3491 | AUC 0.6228 | Recall 0.5393 | Precision 0.5393 | AP 0.5621 | F1 0.5393 | Time 13.00\n",
      "Epoch 0065: Loss 14.3454 | AUC 0.6253 | Recall 0.5387 | Precision 0.5387 | AP 0.5627 | F1 0.5387 | Time 12.79\n",
      "Epoch 0066: Loss 14.3210 | AUC 0.6450 | Recall 0.5378 | Precision 0.5378 | AP 0.5688 | F1 0.5378 | Time 12.67\n",
      "Epoch 0067: Loss 14.3119 | AUC 0.6776 | Recall 0.5432 | Precision 0.5432 | AP 0.5817 | F1 0.5432 | Time 12.09\n",
      "Epoch 0068: Loss 14.3168 | AUC 0.7049 | Recall 0.5532 | Precision 0.5532 | AP 0.5930 | F1 0.5532 | Time 12.37\n",
      "Epoch 0069: Loss 14.3304 | AUC 0.7189 | Recall 0.5596 | Precision 0.5596 | AP 0.5987 | F1 0.5596 | Time 12.45\n",
      "Epoch 0070: Loss 14.3277 | AUC 0.7168 | Recall 0.5527 | Precision 0.5527 | AP 0.5962 | F1 0.5527 | Time 12.49\n",
      "Epoch 0071: Loss 14.3145 | AUC 0.7018 | Recall 0.5406 | Precision 0.5406 | AP 0.5882 | F1 0.5406 | Time 12.73\n",
      "Epoch 0072: Loss 14.3090 | AUC 0.6804 | Recall 0.5278 | Precision 0.5278 | AP 0.5780 | F1 0.5278 | Time 12.36\n",
      "Epoch 0073: Loss 14.3126 | AUC 0.6596 | Recall 0.5194 | Precision 0.5194 | AP 0.5684 | F1 0.5194 | Time 12.69\n",
      "Epoch 0074: Loss 14.3170 | AUC 0.6473 | Recall 0.5122 | Precision 0.5122 | AP 0.5626 | F1 0.5122 | Time 12.02\n",
      "Epoch 0075: Loss 14.3214 | AUC 0.6449 | Recall 0.5099 | Precision 0.5099 | AP 0.5610 | F1 0.5099 | Time 12.54\n",
      "Epoch 0076: Loss 14.3171 | AUC 0.6511 | Recall 0.5098 | Precision 0.5098 | AP 0.5630 | F1 0.5098 | Time 12.29\n",
      "Epoch 0077: Loss 14.3090 | AUC 0.6648 | Recall 0.5129 | Precision 0.5129 | AP 0.5680 | F1 0.5129 | Time 13.02\n",
      "Epoch 0078: Loss 14.3080 | AUC 0.6814 | Recall 0.5181 | Precision 0.5181 | AP 0.5747 | F1 0.5181 | Time 12.58\n",
      "Epoch 0079: Loss 14.3097 | AUC 0.6945 | Recall 0.5190 | Precision 0.5190 | AP 0.5795 | F1 0.5190 | Time 12.60\n",
      "Epoch 0080: Loss 14.3134 | AUC 0.7004 | Recall 0.5196 | Precision 0.5196 | AP 0.5812 | F1 0.5196 | Time 12.74\n",
      "Epoch 0081: Loss 14.3117 | AUC 0.6978 | Recall 0.5187 | Precision 0.5187 | AP 0.5800 | F1 0.5187 | Time 12.42\n",
      "Epoch 0082: Loss 14.3095 | AUC 0.6887 | Recall 0.5159 | Precision 0.5159 | AP 0.5765 | F1 0.5159 | Time 12.60\n",
      "Epoch 0083: Loss 14.3085 | AUC 0.6761 | Recall 0.5109 | Precision 0.5109 | AP 0.5711 | F1 0.5109 | Time 13.05\n",
      "Epoch 0084: Loss 14.3097 | AUC 0.6665 | Recall 0.5073 | Precision 0.5073 | AP 0.5671 | F1 0.5073 | Time 13.02\n",
      "Epoch 0085: Loss 14.3104 | AUC 0.6622 | Recall 0.5068 | Precision 0.5068 | AP 0.5654 | F1 0.5068 | Time 12.45\n",
      "Epoch 0086: Loss 14.3095 | AUC 0.6638 | Recall 0.5070 | Precision 0.5070 | AP 0.5660 | F1 0.5070 | Time 12.40\n",
      "Epoch 0087: Loss 14.3102 | AUC 0.6702 | Recall 0.5085 | Precision 0.5085 | AP 0.5685 | F1 0.5085 | Time 12.78\n",
      "Epoch 0088: Loss 14.3069 | AUC 0.6792 | Recall 0.5131 | Precision 0.5131 | AP 0.5723 | F1 0.5131 | Time 12.59\n",
      "Epoch 0089: Loss 14.3084 | AUC 0.6879 | Recall 0.5161 | Precision 0.5161 | AP 0.5760 | F1 0.5161 | Time 12.73\n",
      "Epoch 0090: Loss 14.3083 | AUC 0.6927 | Recall 0.5163 | Precision 0.5163 | AP 0.5779 | F1 0.5163 | Time 12.43\n",
      "Epoch 0091: Loss 14.3098 | AUC 0.6927 | Recall 0.5163 | Precision 0.5163 | AP 0.5779 | F1 0.5163 | Time 12.71\n",
      "Epoch 0092: Loss 14.3076 | AUC 0.6885 | Recall 0.5168 | Precision 0.5168 | AP 0.5763 | F1 0.5168 | Time 13.42\n",
      "Epoch 0093: Loss 14.3070 | AUC 0.6813 | Recall 0.5135 | Precision 0.5135 | AP 0.5734 | F1 0.5135 | Time 14.75\n",
      "Epoch 0094: Loss 14.3069 | AUC 0.6746 | Recall 0.5112 | Precision 0.5112 | AP 0.5705 | F1 0.5112 | Time 14.09\n",
      "Epoch 0095: Loss 14.3069 | AUC 0.6707 | Recall 0.5098 | Precision 0.5098 | AP 0.5689 | F1 0.5098 | Time 13.25\n",
      "Epoch 0096: Loss 14.3078 | AUC 0.6705 | Recall 0.5099 | Precision 0.5099 | AP 0.5688 | F1 0.5099 | Time 12.64\n",
      "Epoch 0097: Loss 14.3084 | AUC 0.6734 | Recall 0.5114 | Precision 0.5114 | AP 0.5701 | F1 0.5114 | Time 12.86\n",
      "Epoch 0098: Loss 14.3058 | AUC 0.6789 | Recall 0.5140 | Precision 0.5140 | AP 0.5725 | F1 0.5140 | Time 12.86\n",
      "Epoch 0099: Loss 14.3078 | AUC 0.6844 | Recall 0.5168 | Precision 0.5168 | AP 0.5750 | F1 0.5168 | Time 13.28\n",
      "Test: Loss 0.0039 | AUC 0.7022 | Recall 0.5582 | Precision 0.5582 | AP 0.6775 | F1 0.5582 | Time 1.92\n",
      "Epoch 0000: Loss 19.1912 | AUC 0.7865 | Recall 0.6527 | Precision 0.6527 | AP 0.6587 | F1 0.7198 | Time 14.68\n",
      "Epoch 0001: Loss 19.1906 | AUC 0.7875 | Recall 0.6599 | Precision 0.6599 | AP 0.6620 | F1 0.6733 | Time 13.08\n",
      "Epoch 0002: Loss 19.1898 | AUC 0.7857 | Recall 0.6586 | Precision 0.6586 | AP 0.6602 | F1 0.6582 | Time 12.80\n",
      "Epoch 0003: Loss 19.1830 | AUC 0.7877 | Recall 0.6599 | Precision 0.6599 | AP 0.6623 | F1 0.6599 | Time 12.74\n",
      "Epoch 0004: Loss 19.1789 | AUC 0.7876 | Recall 0.6601 | Precision 0.6601 | AP 0.6623 | F1 0.6645 | Time 12.46\n",
      "Epoch 0005: Loss 19.1754 | AUC 0.7877 | Recall 0.6605 | Precision 0.6605 | AP 0.6624 | F1 0.6645 | Time 14.93\n",
      "Epoch 0006: Loss 19.1707 | AUC 0.7877 | Recall 0.6610 | Precision 0.6610 | AP 0.6625 | F1 0.6641 | Time 13.65\n",
      "Epoch 0007: Loss 19.1641 | AUC 0.7874 | Recall 0.6616 | Precision 0.6616 | AP 0.6625 | F1 0.6631 | Time 13.03\n",
      "Epoch 0008: Loss 19.1559 | AUC 0.7876 | Recall 0.6614 | Precision 0.6614 | AP 0.6626 | F1 0.6636 | Time 12.67\n",
      "Epoch 0009: Loss 19.1444 | AUC 0.7873 | Recall 0.6616 | Precision 0.6616 | AP 0.6625 | F1 0.6624 | Time 12.56\n",
      "Epoch 0010: Loss 19.1300 | AUC 0.7869 | Recall 0.6620 | Precision 0.6620 | AP 0.6623 | F1 0.6625 | Time 13.27\n",
      "Epoch 0011: Loss 19.1120 | AUC 0.7865 | Recall 0.6618 | Precision 0.6618 | AP 0.6620 | F1 0.6618 | Time 13.57\n",
      "Epoch 0012: Loss 19.0868 | AUC 0.7857 | Recall 0.6614 | Precision 0.6614 | AP 0.6615 | F1 0.6614 | Time 12.57\n",
      "Epoch 0013: Loss 19.0508 | AUC 0.7840 | Recall 0.6590 | Precision 0.6590 | AP 0.6602 | F1 0.6590 | Time 13.60\n",
      "Epoch 0014: Loss 18.9973 | AUC 0.7812 | Recall 0.6556 | Precision 0.6556 | AP 0.6580 | F1 0.6556 | Time 12.67\n",
      "Epoch 0015: Loss 18.9101 | AUC 0.7780 | Recall 0.6519 | Precision 0.6519 | AP 0.6556 | F1 0.6519 | Time 12.49\n",
      "Epoch 0016: Loss 18.7552 | AUC 0.7759 | Recall 0.6486 | Precision 0.6486 | AP 0.6537 | F1 0.6486 | Time 12.66\n",
      "Epoch 0017: Loss 18.4302 | AUC 0.7589 | Recall 0.6235 | Precision 0.6235 | AP 0.6402 | F1 0.6235 | Time 11.90\n",
      "Epoch 0018: Loss 17.6404 | AUC 0.6915 | Recall 0.5400 | Precision 0.5400 | AP 0.5902 | F1 0.5400 | Time 12.56\n",
      "Epoch 0019: Loss 15.9148 | AUC 0.5812 | Recall 0.4451 | Precision 0.4451 | AP 0.5189 | F1 0.4451 | Time 12.75\n",
      "Epoch 0020: Loss 15.8703 | AUC 0.8766 | Recall 0.8300 | Precision 0.8300 | AP 0.7574 | F1 0.8300 | Time 12.09\n",
      "Epoch 0021: Loss 15.3711 | AUC 0.8463 | Recall 0.8123 | Precision 0.8123 | AP 0.7299 | F1 0.8123 | Time 12.02\n",
      "Epoch 0022: Loss 14.8788 | AUC 0.7214 | Recall 0.5754 | Precision 0.5754 | AP 0.6069 | F1 0.5754 | Time 12.97\n",
      "Epoch 0023: Loss 15.1987 | AUC 0.6383 | Recall 0.4783 | Precision 0.4783 | AP 0.5475 | F1 0.4783 | Time 13.33\n",
      "Epoch 0024: Loss 15.1027 | AUC 0.6475 | Recall 0.4815 | Precision 0.4815 | AP 0.5518 | F1 0.4815 | Time 13.04\n",
      "Epoch 0025: Loss 14.6719 | AUC 0.7102 | Recall 0.5512 | Precision 0.5512 | AP 0.5963 | F1 0.5512 | Time 13.50\n",
      "Epoch 0026: Loss 14.6520 | AUC 0.7665 | Recall 0.7235 | Precision 0.7235 | AP 0.6721 | F1 0.7235 | Time 13.41\n",
      "Epoch 0027: Loss 14.7906 | AUC 0.8216 | Recall 0.7686 | Precision 0.7686 | AP 0.7064 | F1 0.7686 | Time 13.66\n",
      "Epoch 0028: Loss 14.6820 | AUC 0.8119 | Recall 0.7539 | Precision 0.7539 | AP 0.6967 | F1 0.7539 | Time 12.95\n",
      "Epoch 0029: Loss 14.4746 | AUC 0.7519 | Recall 0.7244 | Precision 0.7244 | AP 0.6644 | F1 0.7244 | Time 12.63\n",
      "Epoch 0030: Loss 14.4271 | AUC 0.6776 | Recall 0.6517 | Precision 0.6517 | AP 0.6170 | F1 0.6517 | Time 13.38\n",
      "Epoch 0031: Loss 14.4515 | AUC 0.6391 | Recall 0.5958 | Precision 0.5958 | AP 0.5882 | F1 0.5958 | Time 12.98\n",
      "Epoch 0032: Loss 14.4379 | AUC 0.6250 | Recall 0.6042 | Precision 0.6042 | AP 0.5846 | F1 0.6042 | Time 14.28\n",
      "Epoch 0033: Loss 14.3659 | AUC 0.6320 | Recall 0.5926 | Precision 0.5926 | AP 0.5814 | F1 0.5926 | Time 13.55\n",
      "Epoch 0034: Loss 14.3142 | AUC 0.6793 | Recall 0.5477 | Precision 0.5477 | AP 0.5836 | F1 0.5477 | Time 12.92\n",
      "Epoch 0035: Loss 14.3554 | AUC 0.7118 | Recall 0.5192 | Precision 0.5192 | AP 0.5842 | F1 0.5192 | Time 13.07\n",
      "Epoch 0036: Loss 14.3647 | AUC 0.7001 | Recall 0.4969 | Precision 0.4969 | AP 0.5735 | F1 0.4969 | Time 13.53\n",
      "Epoch 0037: Loss 14.3220 | AUC 0.6521 | Recall 0.4893 | Precision 0.4893 | AP 0.5579 | F1 0.4893 | Time 12.50\n",
      "Epoch 0038: Loss 14.3727 | AUC 0.6037 | Recall 0.4947 | Precision 0.4947 | AP 0.5439 | F1 0.4947 | Time 12.82\n",
      "Epoch 0039: Loss 14.4162 | AUC 0.5961 | Recall 0.5217 | Precision 0.5217 | AP 0.5490 | F1 0.5217 | Time 12.48\n",
      "Epoch 0040: Loss 14.3920 | AUC 0.5984 | Recall 0.4878 | Precision 0.4878 | AP 0.5403 | F1 0.4878 | Time 12.36\n",
      "Epoch 0041: Loss 14.3502 | AUC 0.6263 | Recall 0.4592 | Precision 0.4592 | AP 0.5418 | F1 0.4592 | Time 13.62\n",
      "Epoch 0042: Loss 14.3239 | AUC 0.6682 | Recall 0.4414 | Precision 0.4414 | AP 0.5478 | F1 0.4414 | Time 12.76\n",
      "Epoch 0043: Loss 14.4943 | AUC 0.6454 | Recall 0.4241 | Precision 0.4241 | AP 0.5334 | F1 0.4241 | Time 12.71\n",
      "Epoch 0044: Loss 14.3299 | AUC 0.6572 | Recall 0.4417 | Precision 0.4417 | AP 0.5451 | F1 0.4417 | Time 13.37\n",
      "Epoch 0045: Loss 14.3633 | AUC 0.6190 | Recall 0.4546 | Precision 0.4546 | AP 0.5388 | F1 0.4546 | Time 13.43\n",
      "Epoch 0046: Loss 14.3905 | AUC 0.6003 | Recall 0.4674 | Precision 0.4674 | AP 0.5361 | F1 0.4674 | Time 12.37\n",
      "Epoch 0047: Loss 14.3931 | AUC 0.5994 | Recall 0.4691 | Precision 0.4691 | AP 0.5363 | F1 0.4691 | Time 13.15\n",
      "Epoch 0048: Loss 14.3686 | AUC 0.6123 | Recall 0.4579 | Precision 0.4579 | AP 0.5375 | F1 0.4579 | Time 12.51\n",
      "Epoch 0049: Loss 14.3369 | AUC 0.6435 | Recall 0.4482 | Precision 0.4482 | AP 0.5439 | F1 0.4482 | Time 12.82\n",
      "Epoch 0050: Loss 14.3355 | AUC 0.6713 | Recall 0.4393 | Precision 0.4393 | AP 0.5475 | F1 0.4393 | Time 12.30\n",
      "Epoch 0051: Loss 14.3343 | AUC 0.6729 | Recall 0.4414 | Precision 0.4414 | AP 0.5486 | F1 0.4414 | Time 12.23\n",
      "Epoch 0052: Loss 14.3286 | AUC 0.6509 | Recall 0.4525 | Precision 0.4525 | AP 0.5472 | F1 0.4525 | Time 12.02\n",
      "Epoch 0053: Loss 14.3434 | AUC 0.6323 | Recall 0.4635 | Precision 0.4635 | AP 0.5446 | F1 0.4635 | Time 12.14\n",
      "Epoch 0054: Loss 14.3466 | AUC 0.6256 | Recall 0.4676 | Precision 0.4676 | AP 0.5440 | F1 0.4676 | Time 12.58\n",
      "Epoch 0055: Loss 14.3382 | AUC 0.6342 | Recall 0.4687 | Precision 0.4687 | AP 0.5469 | F1 0.4687 | Time 12.87\n",
      "Epoch 0056: Loss 14.3237 | AUC 0.6543 | Recall 0.4724 | Precision 0.4724 | AP 0.5536 | F1 0.4724 | Time 12.44\n",
      "Epoch 0057: Loss 14.3142 | AUC 0.6793 | Recall 0.4746 | Precision 0.4746 | AP 0.5610 | F1 0.4746 | Time 12.84\n",
      "Epoch 0058: Loss 14.3386 | AUC 0.6896 | Recall 0.4765 | Precision 0.4765 | AP 0.5639 | F1 0.4765 | Time 12.65\n",
      "Epoch 0059: Loss 14.3104 | AUC 0.6814 | Recall 0.4863 | Precision 0.4863 | AP 0.5652 | F1 0.4863 | Time 12.27\n",
      "Epoch 0060: Loss 14.3156 | AUC 0.6634 | Recall 0.4934 | Precision 0.4934 | AP 0.5626 | F1 0.4934 | Time 12.11\n",
      "Epoch 0061: Loss 14.3192 | AUC 0.6510 | Recall 0.4990 | Precision 0.4990 | AP 0.5608 | F1 0.4990 | Time 11.96\n",
      "Epoch 0062: Loss 14.3217 | AUC 0.6486 | Recall 0.5062 | Precision 0.5062 | AP 0.5617 | F1 0.5062 | Time 11.78\n",
      "Epoch 0063: Loss 14.3168 | AUC 0.6560 | Recall 0.5144 | Precision 0.5144 | AP 0.5660 | F1 0.5144 | Time 11.81\n",
      "Epoch 0064: Loss 14.3124 | AUC 0.6708 | Recall 0.5202 | Precision 0.5202 | AP 0.5727 | F1 0.5202 | Time 12.97\n",
      "Epoch 0065: Loss 14.3122 | AUC 0.6866 | Recall 0.5250 | Precision 0.5250 | AP 0.5795 | F1 0.5250 | Time 12.45\n",
      "Epoch 0066: Loss 14.3153 | AUC 0.6994 | Recall 0.5317 | Precision 0.5317 | AP 0.5850 | F1 0.5317 | Time 12.82\n",
      "Epoch 0067: Loss 14.3184 | AUC 0.7044 | Recall 0.5348 | Precision 0.5348 | AP 0.5877 | F1 0.5348 | Time 13.04\n",
      "Epoch 0068: Loss 14.3171 | AUC 0.7016 | Recall 0.5382 | Precision 0.5382 | AP 0.5879 | F1 0.5382 | Time 12.59\n",
      "Epoch 0069: Loss 14.3129 | AUC 0.6920 | Recall 0.5387 | Precision 0.5387 | AP 0.5854 | F1 0.5387 | Time 13.31\n",
      "Epoch 0070: Loss 14.3116 | AUC 0.6791 | Recall 0.5397 | Precision 0.5397 | AP 0.5812 | F1 0.5397 | Time 12.68\n",
      "Epoch 0071: Loss 14.3137 | AUC 0.6665 | Recall 0.5371 | Precision 0.5371 | AP 0.5769 | F1 0.5371 | Time 12.59\n",
      "Epoch 0072: Loss 14.3165 | AUC 0.6592 | Recall 0.5387 | Precision 0.5387 | AP 0.5746 | F1 0.5387 | Time 12.40\n",
      "Epoch 0073: Loss 14.3148 | AUC 0.6593 | Recall 0.5395 | Precision 0.5395 | AP 0.5748 | F1 0.5395 | Time 12.17\n",
      "Epoch 0074: Loss 14.3148 | AUC 0.6663 | Recall 0.5395 | Precision 0.5395 | AP 0.5773 | F1 0.5395 | Time 12.61\n",
      "Epoch 0075: Loss 14.3135 | AUC 0.6768 | Recall 0.5413 | Precision 0.5413 | AP 0.5812 | F1 0.5413 | Time 12.47\n",
      "Epoch 0076: Loss 14.3122 | AUC 0.6874 | Recall 0.5428 | Precision 0.5428 | AP 0.5850 | F1 0.5428 | Time 12.02\n",
      "Epoch 0077: Loss 14.3127 | AUC 0.6943 | Recall 0.5432 | Precision 0.5432 | AP 0.5873 | F1 0.5432 | Time 12.23\n",
      "Epoch 0078: Loss 14.3142 | AUC 0.6963 | Recall 0.5430 | Precision 0.5430 | AP 0.5877 | F1 0.5430 | Time 12.84\n",
      "Epoch 0079: Loss 14.3145 | AUC 0.6933 | Recall 0.5404 | Precision 0.5404 | AP 0.5862 | F1 0.5404 | Time 13.01\n",
      "Epoch 0080: Loss 14.3132 | AUC 0.6866 | Recall 0.5367 | Precision 0.5367 | AP 0.5832 | F1 0.5367 | Time 12.39\n",
      "Epoch 0081: Loss 14.3115 | AUC 0.6786 | Recall 0.5347 | Precision 0.5347 | AP 0.5797 | F1 0.5347 | Time 12.82\n",
      "Epoch 0082: Loss 14.3123 | AUC 0.6720 | Recall 0.5317 | Precision 0.5317 | AP 0.5768 | F1 0.5317 | Time 12.37\n",
      "Epoch 0083: Loss 14.3117 | AUC 0.6696 | Recall 0.5295 | Precision 0.5295 | AP 0.5754 | F1 0.5295 | Time 12.73\n",
      "Epoch 0084: Loss 14.3130 | AUC 0.6711 | Recall 0.5280 | Precision 0.5280 | AP 0.5754 | F1 0.5280 | Time 12.83\n",
      "Epoch 0085: Loss 14.3122 | AUC 0.6761 | Recall 0.5274 | Precision 0.5274 | AP 0.5767 | F1 0.5274 | Time 12.51\n",
      "Epoch 0086: Loss 14.3111 | AUC 0.6823 | Recall 0.5265 | Precision 0.5265 | AP 0.5783 | F1 0.5265 | Time 13.03\n",
      "Epoch 0087: Loss 14.3107 | AUC 0.6875 | Recall 0.5243 | Precision 0.5243 | AP 0.5795 | F1 0.5243 | Time 12.24\n",
      "Epoch 0088: Loss 14.3112 | AUC 0.6904 | Recall 0.5233 | Precision 0.5233 | AP 0.5799 | F1 0.5233 | Time 12.44\n",
      "Epoch 0089: Loss 14.3116 | AUC 0.6898 | Recall 0.5217 | Precision 0.5217 | AP 0.5792 | F1 0.5217 | Time 12.88\n",
      "Epoch 0090: Loss 14.3104 | AUC 0.6866 | Recall 0.5198 | Precision 0.5198 | AP 0.5777 | F1 0.5198 | Time 13.91\n",
      "Epoch 0091: Loss 14.3109 | AUC 0.6823 | Recall 0.5192 | Precision 0.5192 | AP 0.5758 | F1 0.5192 | Time 12.97\n",
      "Epoch 0092: Loss 14.3105 | AUC 0.6781 | Recall 0.5177 | Precision 0.5177 | AP 0.5739 | F1 0.5177 | Time 14.06\n",
      "Epoch 0093: Loss 14.3113 | AUC 0.6761 | Recall 0.5159 | Precision 0.5159 | AP 0.5727 | F1 0.5159 | Time 13.05\n",
      "Epoch 0094: Loss 14.3097 | AUC 0.6768 | Recall 0.5142 | Precision 0.5142 | AP 0.5725 | F1 0.5142 | Time 12.31\n",
      "Epoch 0095: Loss 14.3106 | AUC 0.6797 | Recall 0.5137 | Precision 0.5137 | AP 0.5730 | F1 0.5137 | Time 12.38\n",
      "Epoch 0096: Loss 14.3095 | AUC 0.6833 | Recall 0.5127 | Precision 0.5127 | AP 0.5738 | F1 0.5127 | Time 12.50\n",
      "Epoch 0097: Loss 14.3099 | AUC 0.6860 | Recall 0.5122 | Precision 0.5122 | AP 0.5742 | F1 0.5122 | Time 12.11\n",
      "Epoch 0098: Loss 14.3103 | AUC 0.6867 | Recall 0.5111 | Precision 0.5111 | AP 0.5741 | F1 0.5111 | Time 12.68\n",
      "Epoch 0099: Loss 14.3108 | AUC 0.6851 | Recall 0.5096 | Precision 0.5096 | AP 0.5732 | F1 0.5096 | Time 12.93\n",
      "Test: Loss 0.0039 | AUC 0.6959 | Recall 0.5431 | Precision 0.5431 | AP 0.6730 | F1 0.5431 | Time 1.73\n",
      "Epoch 0000: Loss 19.1910 | AUC 0.7865 | Recall 0.6527 | Precision 0.6527 | AP 0.6587 | F1 0.7198 | Time 13.06\n",
      "Epoch 0001: Loss 19.1896 | AUC 0.7876 | Recall 0.6599 | Precision 0.6599 | AP 0.6620 | F1 0.6709 | Time 12.41\n",
      "Epoch 0002: Loss 19.1829 | AUC 0.7877 | Recall 0.6608 | Precision 0.6608 | AP 0.6624 | F1 0.6669 | Time 12.69\n",
      "Epoch 0003: Loss 19.1804 | AUC 0.7876 | Recall 0.6607 | Precision 0.6607 | AP 0.6624 | F1 0.6661 | Time 12.73\n",
      "Epoch 0004: Loss 19.1757 | AUC 0.7872 | Recall 0.6607 | Precision 0.6607 | AP 0.6623 | F1 0.6646 | Time 12.87\n",
      "Epoch 0005: Loss 19.1686 | AUC 0.7873 | Recall 0.6614 | Precision 0.6614 | AP 0.6624 | F1 0.6626 | Time 12.57\n",
      "Epoch 0006: Loss 19.1616 | AUC 0.7869 | Recall 0.6614 | Precision 0.6614 | AP 0.6622 | F1 0.6614 | Time 12.60\n",
      "Epoch 0007: Loss 19.1508 | AUC 0.7865 | Recall 0.6612 | Precision 0.6612 | AP 0.6620 | F1 0.6613 | Time 12.89\n",
      "Epoch 0008: Loss 19.1355 | AUC 0.7860 | Recall 0.6612 | Precision 0.6612 | AP 0.6617 | F1 0.6612 | Time 12.69\n",
      "Epoch 0009: Loss 19.1153 | AUC 0.7847 | Recall 0.6603 | Precision 0.6603 | AP 0.6608 | F1 0.6603 | Time 12.62\n",
      "Epoch 0010: Loss 19.0867 | AUC 0.7826 | Recall 0.6569 | Precision 0.6569 | AP 0.6591 | F1 0.6569 | Time 12.50\n",
      "Epoch 0011: Loss 19.0411 | AUC 0.7793 | Recall 0.6538 | Precision 0.6538 | AP 0.6566 | F1 0.6538 | Time 12.48\n",
      "Epoch 0012: Loss 18.9611 | AUC 0.7773 | Recall 0.6514 | Precision 0.6514 | AP 0.6551 | F1 0.6514 | Time 12.33\n",
      "Epoch 0013: Loss 18.8009 | AUC 0.7750 | Recall 0.6476 | Precision 0.6476 | AP 0.6530 | F1 0.6476 | Time 12.13\n",
      "Epoch 0014: Loss 18.4381 | AUC 0.7527 | Recall 0.6153 | Precision 0.6153 | AP 0.6356 | F1 0.6153 | Time 12.40\n",
      "Epoch 0015: Loss 17.5301 | AUC 0.6711 | Recall 0.5215 | Precision 0.5215 | AP 0.5772 | F1 0.5215 | Time 12.68\n",
      "Epoch 0016: Loss 15.6642 | AUC 0.5940 | Recall 0.4564 | Precision 0.4564 | AP 0.5258 | F1 0.4564 | Time 11.95\n",
      "Epoch 0017: Loss 16.5563 | AUC 0.8912 | Recall 0.8285 | Precision 0.8285 | AP 0.7723 | F1 0.8285 | Time 12.54\n",
      "Epoch 0018: Loss 15.3305 | AUC 0.8434 | Recall 0.8080 | Precision 0.8080 | AP 0.7272 | F1 0.8080 | Time 12.52\n",
      "Epoch 0019: Loss 14.9654 | AUC 0.6981 | Recall 0.5497 | Precision 0.5497 | AP 0.5900 | F1 0.5497 | Time 12.36\n",
      "Epoch 0020: Loss 15.4326 | AUC 0.6149 | Recall 0.4622 | Precision 0.4622 | AP 0.5349 | F1 0.4622 | Time 12.31\n",
      "Epoch 0021: Loss 15.5072 | AUC 0.6107 | Recall 0.4590 | Precision 0.4590 | AP 0.5330 | F1 0.4590 | Time 12.27\n",
      "Epoch 0022: Loss 15.1413 | AUC 0.6438 | Recall 0.4776 | Precision 0.4776 | AP 0.5494 | F1 0.4776 | Time 12.42\n",
      "Epoch 0023: Loss 14.6332 | AUC 0.7229 | Recall 0.5835 | Precision 0.5835 | AP 0.6105 | F1 0.5835 | Time 12.74\n",
      "Epoch 0024: Loss 14.7041 | AUC 0.7892 | Recall 0.7714 | Precision 0.7714 | AP 0.6914 | F1 0.7714 | Time 12.28\n",
      "Epoch 0025: Loss 14.9235 | AUC 0.8398 | Recall 0.7696 | Precision 0.7696 | AP 0.7134 | F1 0.7696 | Time 12.50\n",
      "Epoch 0026: Loss 14.7769 | AUC 0.8273 | Recall 0.7552 | Precision 0.7552 | AP 0.7034 | F1 0.7552 | Time 12.71\n",
      "Epoch 0027: Loss 14.5077 | AUC 0.7656 | Recall 0.7356 | Precision 0.7356 | AP 0.6730 | F1 0.7356 | Time 12.41\n",
      "Epoch 0028: Loss 14.4518 | AUC 0.6850 | Recall 0.6410 | Precision 0.6410 | AP 0.6181 | F1 0.6410 | Time 12.35\n",
      "Epoch 0029: Loss 14.5078 | AUC 0.6493 | Recall 0.5620 | Precision 0.5620 | AP 0.5823 | F1 0.5620 | Time 12.53\n",
      "Epoch 0030: Loss 14.5443 | AUC 0.6373 | Recall 0.5464 | Precision 0.5464 | AP 0.5733 | F1 0.5464 | Time 12.23\n",
      "Epoch 0031: Loss 14.5044 | AUC 0.6249 | Recall 0.5646 | Precision 0.5646 | AP 0.5750 | F1 0.5646 | Time 11.97\n",
      "Epoch 0032: Loss 14.3993 | AUC 0.6281 | Recall 0.6266 | Precision 0.6266 | AP 0.5892 | F1 0.6266 | Time 12.31\n",
      "Epoch 0033: Loss 14.3181 | AUC 0.6822 | Recall 0.5724 | Precision 0.5724 | AP 0.5916 | F1 0.5724 | Time 12.58\n",
      "Epoch 0034: Loss 14.3661 | AUC 0.7262 | Recall 0.5516 | Precision 0.5516 | AP 0.5984 | F1 0.5516 | Time 12.97\n",
      "Epoch 0035: Loss 14.4615 | AUC 0.7098 | Recall 0.5289 | Precision 0.5289 | AP 0.5851 | F1 0.5289 | Time 12.27\n",
      "Epoch 0036: Loss 14.3153 | AUC 0.6666 | Recall 0.5228 | Precision 0.5228 | AP 0.5717 | F1 0.5228 | Time 13.05\n",
      "Epoch 0037: Loss 14.3921 | AUC 0.6079 | Recall 0.5532 | Precision 0.5532 | AP 0.5612 | F1 0.5532 | Time 13.10\n",
      "Epoch 0038: Loss 14.4754 | AUC 0.5974 | Recall 0.5928 | Precision 0.5928 | AP 0.5709 | F1 0.5928 | Time 13.10\n",
      "Epoch 0039: Loss 14.4722 | AUC 0.5952 | Recall 0.5839 | Precision 0.5839 | AP 0.5678 | F1 0.5839 | Time 12.62\n",
      "Epoch 0040: Loss 14.3953 | AUC 0.6025 | Recall 0.5291 | Precision 0.5291 | AP 0.5525 | F1 0.5291 | Time 12.65\n",
      "Epoch 0041: Loss 14.3239 | AUC 0.6480 | Recall 0.4854 | Precision 0.4854 | AP 0.5553 | F1 0.4854 | Time 12.42\n",
      "Epoch 0042: Loss 14.3675 | AUC 0.6827 | Recall 0.4668 | Precision 0.4668 | AP 0.5585 | F1 0.4668 | Time 12.12\n",
      "Epoch 0043: Loss 14.3277 | AUC 0.6858 | Recall 0.4691 | Precision 0.4691 | AP 0.5606 | F1 0.4691 | Time 12.41\n",
      "Epoch 0044: Loss 14.3205 | AUC 0.6575 | Recall 0.4763 | Precision 0.4763 | AP 0.5554 | F1 0.4763 | Time 12.67\n",
      "Epoch 0045: Loss 14.3384 | AUC 0.6290 | Recall 0.4811 | Precision 0.4811 | AP 0.5483 | F1 0.4811 | Time 13.74\n",
      "Epoch 0046: Loss 14.3522 | AUC 0.6162 | Recall 0.4873 | Precision 0.4873 | AP 0.5456 | F1 0.4873 | Time 12.96\n",
      "Epoch 0047: Loss 14.3510 | AUC 0.6178 | Recall 0.4869 | Precision 0.4869 | AP 0.5460 | F1 0.4869 | Time 13.39\n",
      "Epoch 0048: Loss 14.3357 | AUC 0.6333 | Recall 0.4841 | Precision 0.4841 | AP 0.5501 | F1 0.4841 | Time 12.53\n",
      "Epoch 0049: Loss 14.3191 | AUC 0.6588 | Recall 0.4795 | Precision 0.4795 | AP 0.5568 | F1 0.4795 | Time 14.87\n",
      "Epoch 0050: Loss 14.3163 | AUC 0.6843 | Recall 0.4750 | Precision 0.4750 | AP 0.5624 | F1 0.4750 | Time 12.76\n",
      "Epoch 0051: Loss 14.3531 | AUC 0.6867 | Recall 0.4728 | Precision 0.4728 | AP 0.5615 | F1 0.4728 | Time 12.82\n",
      "Epoch 0052: Loss 14.3139 | AUC 0.6662 | Recall 0.4845 | Precision 0.4845 | AP 0.5605 | F1 0.4845 | Time 12.63\n",
      "Epoch 0053: Loss 14.3338 | AUC 0.6336 | Recall 0.4941 | Precision 0.4941 | AP 0.5533 | F1 0.4941 | Time 12.48\n",
      "Epoch 0054: Loss 14.3531 | AUC 0.6180 | Recall 0.5044 | Precision 0.5044 | AP 0.5509 | F1 0.5044 | Time 14.64\n",
      "Epoch 0055: Loss 14.3501 | AUC 0.6193 | Recall 0.5072 | Precision 0.5072 | AP 0.5521 | F1 0.5072 | Time 14.15\n",
      "Epoch 0056: Loss 14.3299 | AUC 0.6365 | Recall 0.5049 | Precision 0.5049 | AP 0.5571 | F1 0.5049 | Time 14.33\n",
      "Epoch 0057: Loss 14.3150 | AUC 0.6646 | Recall 0.5029 | Precision 0.5029 | AP 0.5656 | F1 0.5029 | Time 13.49\n",
      "Epoch 0058: Loss 14.3135 | AUC 0.6933 | Recall 0.5072 | Precision 0.5072 | AP 0.5748 | F1 0.5072 | Time 15.29\n",
      "Epoch 0059: Loss 14.3417 | AUC 0.7035 | Recall 0.5053 | Precision 0.5053 | AP 0.5772 | F1 0.5053 | Time 14.65\n",
      "Epoch 0060: Loss 14.3136 | AUC 0.6948 | Recall 0.5131 | Precision 0.5131 | AP 0.5777 | F1 0.5131 | Time 15.72\n",
      "Epoch 0061: Loss 14.3130 | AUC 0.6735 | Recall 0.5189 | Precision 0.5189 | AP 0.5730 | F1 0.5189 | Time 14.71\n",
      "Epoch 0062: Loss 14.3183 | AUC 0.6529 | Recall 0.5220 | Precision 0.5220 | AP 0.5674 | F1 0.5220 | Time 13.30\n",
      "Epoch 0063: Loss 14.3218 | AUC 0.6430 | Recall 0.5261 | Precision 0.5261 | AP 0.5654 | F1 0.5261 | Time 12.80\n",
      "Epoch 0064: Loss 14.3235 | AUC 0.6441 | Recall 0.5295 | Precision 0.5295 | AP 0.5667 | F1 0.5295 | Time 12.64\n",
      "Epoch 0065: Loss 14.3176 | AUC 0.6553 | Recall 0.5324 | Precision 0.5324 | AP 0.5710 | F1 0.5324 | Time 13.43\n",
      "Epoch 0066: Loss 14.3142 | AUC 0.6735 | Recall 0.5324 | Precision 0.5324 | AP 0.5775 | F1 0.5324 | Time 14.51\n",
      "Epoch 0067: Loss 14.3144 | AUC 0.6911 | Recall 0.5354 | Precision 0.5354 | AP 0.5840 | F1 0.5354 | Time 13.44\n",
      "Epoch 0068: Loss 14.3165 | AUC 0.7028 | Recall 0.5371 | Precision 0.5371 | AP 0.5880 | F1 0.5371 | Time 12.53\n",
      "Epoch 0069: Loss 14.3187 | AUC 0.7066 | Recall 0.5378 | Precision 0.5378 | AP 0.5893 | F1 0.5378 | Time 13.73\n",
      "Epoch 0070: Loss 14.3178 | AUC 0.7024 | Recall 0.5391 | Precision 0.5391 | AP 0.5884 | F1 0.5391 | Time 13.26\n",
      "Epoch 0071: Loss 14.3133 | AUC 0.6919 | Recall 0.5389 | Precision 0.5389 | AP 0.5854 | F1 0.5389 | Time 12.62\n",
      "Epoch 0072: Loss 14.3133 | AUC 0.6785 | Recall 0.5391 | Precision 0.5391 | AP 0.5808 | F1 0.5391 | Time 12.21\n",
      "Epoch 0073: Loss 14.3142 | AUC 0.6658 | Recall 0.5365 | Precision 0.5365 | AP 0.5763 | F1 0.5365 | Time 12.21\n",
      "Epoch 0074: Loss 14.3156 | AUC 0.6581 | Recall 0.5376 | Precision 0.5376 | AP 0.5737 | F1 0.5376 | Time 12.09\n",
      "Epoch 0075: Loss 14.3172 | AUC 0.6575 | Recall 0.5369 | Precision 0.5369 | AP 0.5734 | F1 0.5369 | Time 12.34\n",
      "Epoch 0076: Loss 14.3141 | AUC 0.6637 | Recall 0.5358 | Precision 0.5358 | AP 0.5752 | F1 0.5358 | Time 12.62\n",
      "Epoch 0077: Loss 14.3130 | AUC 0.6739 | Recall 0.5358 | Precision 0.5358 | AP 0.5785 | F1 0.5358 | Time 12.86\n",
      "Epoch 0078: Loss 14.3110 | AUC 0.6852 | Recall 0.5345 | Precision 0.5345 | AP 0.5820 | F1 0.5345 | Time 14.47\n",
      "Epoch 0079: Loss 14.3137 | AUC 0.6933 | Recall 0.5352 | Precision 0.5352 | AP 0.5845 | F1 0.5352 | Time 12.69\n",
      "Epoch 0080: Loss 14.3127 | AUC 0.6972 | Recall 0.5352 | Precision 0.5352 | AP 0.5852 | F1 0.5352 | Time 13.25\n",
      "Epoch 0081: Loss 14.3153 | AUC 0.6956 | Recall 0.5326 | Precision 0.5326 | AP 0.5843 | F1 0.5326 | Time 12.64\n",
      "Epoch 0082: Loss 14.3121 | AUC 0.6896 | Recall 0.5300 | Precision 0.5300 | AP 0.5820 | F1 0.5300 | Time 12.62\n",
      "Epoch 0083: Loss 14.3127 | AUC 0.6822 | Recall 0.5283 | Precision 0.5283 | AP 0.5791 | F1 0.5283 | Time 13.32\n",
      "Epoch 0084: Loss 14.3118 | AUC 0.6751 | Recall 0.5269 | Precision 0.5269 | AP 0.5764 | F1 0.5269 | Time 12.94\n",
      "Epoch 0085: Loss 14.3139 | AUC 0.6708 | Recall 0.5259 | Precision 0.5259 | AP 0.5745 | F1 0.5259 | Time 13.54\n",
      "Epoch 0086: Loss 14.3123 | AUC 0.6704 | Recall 0.5246 | Precision 0.5246 | AP 0.5740 | F1 0.5246 | Time 13.89\n",
      "Epoch 0087: Loss 14.3107 | AUC 0.6737 | Recall 0.5235 | Precision 0.5235 | AP 0.5747 | F1 0.5235 | Time 12.76\n",
      "Epoch 0088: Loss 14.3111 | AUC 0.6794 | Recall 0.5224 | Precision 0.5224 | AP 0.5761 | F1 0.5224 | Time 13.19\n",
      "Epoch 0089: Loss 14.3102 | AUC 0.6850 | Recall 0.5196 | Precision 0.5196 | AP 0.5774 | F1 0.5196 | Time 14.53\n",
      "Epoch 0090: Loss 14.3116 | AUC 0.6892 | Recall 0.5189 | Precision 0.5189 | AP 0.5782 | F1 0.5189 | Time 13.82\n",
      "Epoch 0091: Loss 14.3112 | AUC 0.6905 | Recall 0.5174 | Precision 0.5174 | AP 0.5781 | F1 0.5174 | Time 13.17\n",
      "Epoch 0092: Loss 14.3106 | AUC 0.6888 | Recall 0.5170 | Precision 0.5170 | AP 0.5773 | F1 0.5170 | Time 13.08\n",
      "Epoch 0093: Loss 14.3121 | AUC 0.6849 | Recall 0.5177 | Precision 0.5177 | AP 0.5759 | F1 0.5177 | Time 12.78\n",
      "Epoch 0094: Loss 14.3112 | AUC 0.6802 | Recall 0.5164 | Precision 0.5164 | AP 0.5742 | F1 0.5164 | Time 12.32\n",
      "Epoch 0095: Loss 14.3110 | AUC 0.6767 | Recall 0.5153 | Precision 0.5153 | AP 0.5727 | F1 0.5153 | Time 12.13\n",
      "Epoch 0096: Loss 14.3100 | AUC 0.6755 | Recall 0.5138 | Precision 0.5138 | AP 0.5719 | F1 0.5138 | Time 12.45\n",
      "Epoch 0097: Loss 14.3115 | AUC 0.6771 | Recall 0.5131 | Precision 0.5131 | AP 0.5722 | F1 0.5131 | Time 13.41\n",
      "Epoch 0098: Loss 14.3112 | AUC 0.6805 | Recall 0.5127 | Precision 0.5127 | AP 0.5729 | F1 0.5127 | Time 12.78\n",
      "Epoch 0099: Loss 14.3106 | AUC 0.6843 | Recall 0.5124 | Precision 0.5124 | AP 0.5738 | F1 0.5124 | Time 12.88\n",
      "Test: Loss 0.0039 | AUC 0.7001 | Recall 0.5468 | Precision 0.5468 | AP 0.6750 | F1 0.5468 | Time 1.88\n"
     ]
    }
   ],
   "source": [
    "f1_conad = []\n",
    "f1_conad_for = []\n",
    "precision_conad = []\n",
    "recall_conad = []\n",
    "train_time_conad = []\n",
    "test_time_conad = []\n",
    "for i in range(3):\n",
    "    start_time = time.time()\n",
    "    conad_model, graph_test = make_conad_model(train_graph, train_node_features, label_train,\n",
    "                                              test_graph, test_node_features, label_test)\n",
    "    train_time_conad.append(time.time() - start_time)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    f1_score_ip, precision_score, recall_score, f1_score = predict_conad(label_test, conad_model, graph_test)\n",
    "    test_time_conad.append(time.time() - start_time)\n",
    "    \n",
    "    f1_conad.append(f1_score_ip)\n",
    "    precision_conad.append(precision_score)\n",
    "    recall_conad.append(recall_score)\n",
    "    f1_conad_for.append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5595794392523366, 0.5607181851267742, 0.5414656558695153]\n",
      "[tensor(0.4510), tensor(0.4510), tensor(0.4510)]\n",
      "[tensor(1.), tensor(1.), tensor(1.)]\n",
      "[tensor(0.6217), tensor(0.6217), tensor(0.6217)]\n"
     ]
    }
   ],
   "source": [
    "print(f1_conad)\n",
    "print(precision_conad)\n",
    "print(recall_conad)\n",
    "print(f1_conad_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1306.3826067447662, 1317.8039546012878, 1331.694186449051]\n",
      "[3.382333993911743, 3.0480153560638428, 3.529951810836792]\n"
     ]
    }
   ],
   "source": [
    "print(train_time_conad)\n",
    "print(test_time_conad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

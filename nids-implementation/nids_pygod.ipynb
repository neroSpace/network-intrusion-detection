{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from pygod.detector import DOMINANT, OCGNN, GUIDE, GAE, GAAN, AnomalyDAE, CONAD\n",
    "from pygod.metric import eval_average_precision, eval_roc_auc, eval_f1, eval_precision_at_k, eval_recall_at_k\n",
    "from pygod.generator import gen_contextual_outlier, gen_structural_outlier\n",
    "import pickle\n",
    "import time\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw_labeled_path = \"C:\\\\Users\\\\asus\\\\Documents\\\\nids-pcap-dataset\\\\unsw_parquet_used_dataset\\\\unsw_labeled.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw = pd.read_parquet(unsw_labeled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 125180 entries, 1 to 490022\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count   Dtype   \n",
      "---  ------            --------------   -----   \n",
      " 0   source_ip         125180 non-null  object  \n",
      " 1   destination_ip    125180 non-null  object  \n",
      " 2   source_port       125180 non-null  object  \n",
      " 3   destination_port  125180 non-null  object  \n",
      " 4   info_message      125180 non-null  object  \n",
      " 5   attack_category   15657 non-null   category\n",
      " 6   is_malware        125180 non-null  int64   \n",
      " 7   source_ip_info    125180 non-null  object  \n",
      " 8   source_port_info  125180 non-null  object  \n",
      " 9   dest_ip_info      125180 non-null  object  \n",
      " 10  dest_port_info    125180 non-null  object  \n",
      " 11  count_benign      125180 non-null  int64   \n",
      " 12  count_malware     125180 non-null  int64   \n",
      "dtypes: category(1), int64(3), object(9)\n",
      "memory usage: 12.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_ip</th>\n",
       "      <th>destination_ip</th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>info_message</th>\n",
       "      <th>attack_category</th>\n",
       "      <th>is_malware</th>\n",
       "      <th>source_ip_info</th>\n",
       "      <th>source_port_info</th>\n",
       "      <th>dest_ip_info</th>\n",
       "      <th>dest_port_info</th>\n",
       "      <th>count_benign</th>\n",
       "      <th>count_malware</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175.45.176.1</td>\n",
       "      <td>149.171.126.18</td>\n",
       "      <td>4657</td>\n",
       "      <td>80</td>\n",
       "      <td>GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>175.45.176.1 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>4657 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>149.171.126.18 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>80 GET /oKmwKoVbq HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175.45.176.3</td>\n",
       "      <td>149.171.126.18</td>\n",
       "      <td>32473</td>\n",
       "      <td>80</td>\n",
       "      <td>GET /level/15/exec/-/buffers/assigned/dump HTT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>175.45.176.3 GET /level/15/exec/-/buffers/assi...</td>\n",
       "      <td>32473 GET /level/15/exec/-/buffers/assigned/du...</td>\n",
       "      <td>149.171.126.18 GET /level/15/exec/-/buffers/as...</td>\n",
       "      <td>80 GET /level/15/exec/-/buffers/assigned/dump ...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>175.45.176.0</td>\n",
       "      <td>149.171.126.17</td>\n",
       "      <td>49194</td>\n",
       "      <td>80</td>\n",
       "      <td>GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>175.45.176.0 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>49194 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>149.171.126.17 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>80 GET eLWfxXSPkc HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source_ip  destination_ip source_port destination_port   \n",
       "index                                                              \n",
       "1      175.45.176.1  149.171.126.18        4657               80  \\\n",
       "2      175.45.176.3  149.171.126.18       32473               80   \n",
       "6      175.45.176.0  149.171.126.17       49194               80   \n",
       "\n",
       "                                            info_message attack_category   \n",
       "index                                                                      \n",
       "1                               GET /oKmwKoVbq HTTP/1.1              NaN  \\\n",
       "2      GET /level/15/exec/-/buffers/assigned/dump HTT...             NaN   \n",
       "6                               GET eLWfxXSPkc HTTP/1.1              NaN   \n",
       "\n",
       "       is_malware                                     source_ip_info   \n",
       "index                                                                  \n",
       "1               0              175.45.176.1 GET /oKmwKoVbq HTTP/1.1   \\\n",
       "2               1  175.45.176.3 GET /level/15/exec/-/buffers/assi...   \n",
       "6               0              175.45.176.0 GET eLWfxXSPkc HTTP/1.1    \n",
       "\n",
       "                                        source_port_info   \n",
       "index                                                      \n",
       "1                          4657 GET /oKmwKoVbq HTTP/1.1   \\\n",
       "2      32473 GET /level/15/exec/-/buffers/assigned/du...   \n",
       "6                         49194 GET eLWfxXSPkc HTTP/1.1    \n",
       "\n",
       "                                            dest_ip_info   \n",
       "index                                                      \n",
       "1                149.171.126.18 GET /oKmwKoVbq HTTP/1.1   \\\n",
       "2      149.171.126.18 GET /level/15/exec/-/buffers/as...   \n",
       "6                149.171.126.17 GET eLWfxXSPkc HTTP/1.1    \n",
       "\n",
       "                                          dest_port_info  count_benign   \n",
       "index                                                                    \n",
       "1                            80 GET /oKmwKoVbq HTTP/1.1              1  \\\n",
       "2      80 GET /level/15/exec/-/buffers/assigned/dump ...             1   \n",
       "6                            80 GET eLWfxXSPkc HTTP/1.1              1   \n",
       "\n",
       "       count_malware  \n",
       "index                 \n",
       "1                  0  \n",
       "2                  7  \n",
       "6                  0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsw.info()\n",
    "unsw.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, test_size=0.3):\n",
    "    train, test = train_test_split(df, test_size=test_size)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_modeling_1(df):\n",
    "    graph = nx.Graph()\n",
    "    node_features = []\n",
    "    labels = []\n",
    "\n",
    "    for source_port_info in df[\"source_port_info\"].unique():\n",
    "        graph.add_node(source_port_info)\n",
    "        info_message = df[df[\"source_port_info\"] == source_port_info][\"info_message\"].iloc[0]\n",
    "        label = df[df[\"source_port_info\"] == source_port_info][\"is_malware\"].iloc[0]\n",
    "        node_features.append([float(len(info_message))])\n",
    "        labels.append(label)\n",
    "        \n",
    "    for (source_ip), group in df.groupby([\"source_ip\"]):\n",
    "        for i in range(len(group) - 1):\n",
    "            from_node = group.iloc[i][\"source_port_info\"]\n",
    "            to_node = group.iloc[i+1][\"source_port_info\"]\n",
    "            if graph.has_edge(from_node, to_node):\n",
    "                graph[from_node][to_node][\"weight\"] += 1\n",
    "            else:\n",
    "                graph.add_edge(from_node, to_node, weight=1)\n",
    "    return graph, node_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_modeling_2(df):\n",
    "    graph = nx.Graph()\n",
    "    node_features = []\n",
    "    labels = []\n",
    "\n",
    "    for info_message in df[\"info_message\"].unique():\n",
    "        graph.add_node(info_message)\n",
    "        node_features.append([float(len(info_message))])\n",
    "    \n",
    "    for (source_ip), group in df.groupby([\"source_ip\"]):\n",
    "        for i in range(len(group) - 1):\n",
    "            from_node = group.iloc[i][\"info_message\"]\n",
    "            to_node = group.iloc[i+1][\"info_message\"]\n",
    "            if graph.has_edge(from_node, to_node):\n",
    "                graph[from_node][to_node][\"weight\"] += 1\n",
    "            else:\n",
    "                graph.add_edge(from_node, to_node, weight=1)\n",
    "    node_features = torch.tensor(node_features)\n",
    "    labels = torch.tensor(labels)\n",
    "    return graph, node_features, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw['source_port'] = unsw.source_port.astype('int32')\n",
    "unsw['destination_port']= unsw.destination_port.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_train_test(unsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    77233\n",
       "1    10393\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.], dtype=float32), array([1653, 5923], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "nD_train_df = train_df.drop_duplicates(subset=['info_message'], keep='first')\n",
    "label_train = nD_train_df['is_malware'].to_numpy()\n",
    "label_train = torch.tensor(label_train, dtype=torch.float)\n",
    "value_counts = np.unique(label_train, return_counts=True)\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "1    5923\n",
       "0    1653\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nD_train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    33041\n",
       "1      863\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.], dtype=float32), array([ 821, 3060], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "nD_test_df = test_df.drop_duplicates(subset=['info_message'], keep='first')\n",
    "nD_test_df.is_malware.value_counts()\n",
    "label_test = nD_test_df['is_malware'].to_numpy()\n",
    "label_test = torch.tensor(label_test, dtype=torch.float)\n",
    "value_counts = np.unique(label_test, return_counts=True)\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph, train_node_features = graph_modeling_2(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7576\n"
     ]
    }
   ],
   "source": [
    "# number of nodes\n",
    "print(train_graph.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph, test_node_features = graph_modeling_2(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOMINANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyG_train = from_networkx(train_graph)\n",
    "pyG_train.x = train_node_features\n",
    "pyG_test = from_networkx(test_graph)\n",
    "pyG_test.x = test_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_model = DOMINANT(gpu=0, weight=0.02, num_layers=64, hid_dim=64, contamination=0.1, lr=0.001, verbose=3, epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 4.8750 | AUC 0.6216 | Recall 0.7873 | Precision 0.7873 | AP 0.8504 | F1 0.7783 | Time 12.57\n",
      "Epoch 0001: Loss 4.8739 | AUC 0.6213 | Recall 0.7874 | Precision 0.7874 | AP 0.8514 | F1 0.7874 | Time 12.25\n",
      "Epoch 0002: Loss 4.8740 | AUC 0.6214 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.20\n",
      "Epoch 0003: Loss 4.8726 | AUC 0.6213 | Recall 0.7874 | Precision 0.7874 | AP 0.8514 | F1 0.7874 | Time 12.20\n",
      "Epoch 0004: Loss 4.8725 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.24\n",
      "Epoch 0005: Loss 4.8718 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.26\n",
      "Epoch 0006: Loss 4.8707 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.07\n",
      "Epoch 0007: Loss 4.8701 | AUC 0.6214 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 11.98\n",
      "Epoch 0008: Loss 4.8685 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.03\n",
      "Epoch 0009: Loss 4.8667 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 11.99\n",
      "Epoch 0010: Loss 4.8638 | AUC 0.6213 | Recall 0.7878 | Precision 0.7878 | AP 0.8514 | F1 0.7878 | Time 12.04\n",
      "Epoch 0011: Loss 4.8576 | AUC 0.6213 | Recall 0.7878 | Precision 0.7878 | AP 0.8514 | F1 0.7878 | Time 12.25\n",
      "Epoch 0012: Loss 4.8428 | AUC 0.6213 | Recall 0.7876 | Precision 0.7876 | AP 0.8514 | F1 0.7876 | Time 12.03\n",
      "Epoch 0013: Loss 4.8026 | AUC 0.6212 | Recall 0.7878 | Precision 0.7878 | AP 0.8514 | F1 0.7878 | Time 12.02\n",
      "Epoch 0014: Loss 4.6857 | AUC 0.6213 | Recall 0.7879 | Precision 0.7879 | AP 0.8514 | F1 0.7879 | Time 12.03\n",
      "Epoch 0015: Loss 4.4057 | AUC 0.6289 | Recall 0.7966 | Precision 0.7966 | AP 0.8542 | F1 0.7966 | Time 12.00\n",
      "Epoch 0016: Loss 4.8356 | AUC 0.5488 | Recall 0.7424 | Precision 0.7424 | AP 0.8477 | F1 0.7424 | Time 12.07\n",
      "Epoch 0017: Loss 4.4996 | AUC 0.6608 | Recall 0.8207 | Precision 0.8207 | AP 0.8691 | F1 0.8207 | Time 12.01\n",
      "Epoch 0018: Loss 4.3969 | AUC 0.6336 | Recall 0.7979 | Precision 0.7979 | AP 0.8554 | F1 0.7979 | Time 12.06\n",
      "Epoch 0019: Loss 4.5416 | AUC 0.6215 | Recall 0.7878 | Precision 0.7878 | AP 0.8517 | F1 0.7878 | Time 12.09\n",
      "Epoch 0020: Loss 4.6080 | AUC 0.6213 | Recall 0.7879 | Precision 0.7879 | AP 0.8515 | F1 0.7879 | Time 12.17\n",
      "Epoch 0021: Loss 4.6224 | AUC 0.6213 | Recall 0.7879 | Precision 0.7879 | AP 0.8515 | F1 0.7879 | Time 12.00\n",
      "Epoch 0022: Loss 4.6002 | AUC 0.6214 | Recall 0.7879 | Precision 0.7879 | AP 0.8515 | F1 0.7879 | Time 12.05\n",
      "Epoch 0023: Loss 4.5374 | AUC 0.6216 | Recall 0.7879 | Precision 0.7879 | AP 0.8518 | F1 0.7879 | Time 12.02\n",
      "Epoch 0024: Loss 4.4230 | AUC 0.6257 | Recall 0.7891 | Precision 0.7891 | AP 0.8533 | F1 0.7891 | Time 12.17\n",
      "Epoch 0025: Loss 4.3936 | AUC 0.7189 | Recall 0.8464 | Precision 0.8464 | AP 0.8785 | F1 0.8464 | Time 12.02\n",
      "Epoch 0026: Loss 4.4965 | AUC 0.6617 | Recall 0.8214 | Precision 0.8214 | AP 0.8693 | F1 0.8214 | Time 12.11\n",
      "Epoch 0027: Loss 4.5142 | AUC 0.6507 | Recall 0.8158 | Precision 0.8158 | AP 0.8675 | F1 0.8158 | Time 11.95\n",
      "Epoch 0028: Loss 4.4295 | AUC 0.7014 | Recall 0.8393 | Precision 0.8393 | AP 0.8756 | F1 0.8393 | Time 12.04\n",
      "Epoch 0029: Loss 4.3826 | AUC 0.6710 | Recall 0.8199 | Precision 0.8199 | AP 0.8654 | F1 0.8199 | Time 12.25\n",
      "Epoch 0030: Loss 4.4365 | AUC 0.6252 | Recall 0.7885 | Precision 0.7885 | AP 0.8527 | F1 0.7885 | Time 12.26\n",
      "Epoch 0031: Loss 4.4700 | AUC 0.6220 | Recall 0.7881 | Precision 0.7881 | AP 0.8521 | F1 0.7881 | Time 12.05\n",
      "Epoch 0032: Loss 4.4495 | AUC 0.6225 | Recall 0.7876 | Precision 0.7876 | AP 0.8524 | F1 0.7876 | Time 12.05\n",
      "Epoch 0033: Loss 4.4020 | AUC 0.6326 | Recall 0.7967 | Precision 0.7967 | AP 0.8553 | F1 0.7967 | Time 12.04\n",
      "Epoch 0034: Loss 4.3800 | AUC 0.7122 | Recall 0.8416 | Precision 0.8416 | AP 0.8771 | F1 0.8416 | Time 12.06\n",
      "Epoch 0035: Loss 4.4211 | AUC 0.7054 | Recall 0.8411 | Precision 0.8411 | AP 0.8763 | F1 0.8411 | Time 11.98\n",
      "Epoch 0036: Loss 4.4330 | AUC 0.7001 | Recall 0.8386 | Precision 0.8386 | AP 0.8753 | F1 0.8386 | Time 12.12\n",
      "Epoch 0037: Loss 4.3988 | AUC 0.7175 | Recall 0.8460 | Precision 0.8460 | AP 0.8781 | F1 0.8460 | Time 12.16\n",
      "Epoch 0038: Loss 4.3781 | AUC 0.6786 | Recall 0.8199 | Precision 0.8199 | AP 0.8680 | F1 0.8199 | Time 12.30\n",
      "Epoch 0039: Loss 4.3973 | AUC 0.6321 | Recall 0.7981 | Precision 0.7981 | AP 0.8550 | F1 0.7981 | Time 12.06\n",
      "Epoch 0040: Loss 4.4130 | AUC 0.6267 | Recall 0.7945 | Precision 0.7945 | AP 0.8537 | F1 0.7945 | Time 12.20\n",
      "Epoch 0041: Loss 4.3977 | AUC 0.6318 | Recall 0.7977 | Precision 0.7977 | AP 0.8550 | F1 0.7977 | Time 12.00\n",
      "Epoch 0042: Loss 4.3782 | AUC 0.6657 | Recall 0.8151 | Precision 0.8151 | AP 0.8643 | F1 0.8151 | Time 12.03\n",
      "Epoch 0043: Loss 4.3839 | AUC 0.7172 | Recall 0.8477 | Precision 0.8477 | AP 0.8781 | F1 0.8477 | Time 11.95\n",
      "Epoch 0044: Loss 4.3984 | AUC 0.7163 | Recall 0.8448 | Precision 0.8448 | AP 0.8781 | F1 0.8448 | Time 11.94\n",
      "Epoch 0045: Loss 4.3927 | AUC 0.7181 | Recall 0.8465 | Precision 0.8465 | AP 0.8784 | F1 0.8465 | Time 11.95\n",
      "Epoch 0046: Loss 4.3785 | AUC 0.7079 | Recall 0.8372 | Precision 0.8372 | AP 0.8761 | F1 0.8372 | Time 12.01\n",
      "Epoch 0047: Loss 4.3792 | AUC 0.6583 | Recall 0.8126 | Precision 0.8126 | AP 0.8622 | F1 0.8126 | Time 11.99\n",
      "Epoch 0048: Loss 4.3893 | AUC 0.6379 | Recall 0.7981 | Precision 0.7981 | AP 0.8565 | F1 0.7981 | Time 11.99\n",
      "Epoch 0049: Loss 4.3887 | AUC 0.6384 | Recall 0.7981 | Precision 0.7981 | AP 0.8566 | F1 0.7981 | Time 11.94\n",
      "Epoch 0050: Loss 4.3784 | AUC 0.6593 | Recall 0.8128 | Precision 0.8128 | AP 0.8625 | F1 0.8128 | Time 12.02\n",
      "Epoch 0051: Loss 4.3762 | AUC 0.7021 | Recall 0.8329 | Precision 0.8329 | AP 0.8747 | F1 0.8329 | Time 11.94\n",
      "Epoch 0052: Loss 4.3834 | AUC 0.7176 | Recall 0.8487 | Precision 0.8487 | AP 0.8784 | F1 0.8487 | Time 11.97\n",
      "Epoch 0053: Loss 4.3839 | AUC 0.7183 | Recall 0.8486 | Precision 0.8486 | AP 0.8785 | F1 0.8486 | Time 11.96\n",
      "Epoch 0054: Loss 4.3772 | AUC 0.7092 | Recall 0.8381 | Precision 0.8381 | AP 0.8764 | F1 0.8381 | Time 12.25\n",
      "Epoch 0055: Loss 4.3749 | AUC 0.6772 | Recall 0.8193 | Precision 0.8193 | AP 0.8677 | F1 0.8193 | Time 12.29\n",
      "Epoch 0056: Loss 4.3794 | AUC 0.6532 | Recall 0.8082 | Precision 0.8082 | AP 0.8609 | F1 0.8082 | Time 12.07\n",
      "Epoch 0057: Loss 4.3798 | AUC 0.6506 | Recall 0.8060 | Precision 0.8060 | AP 0.8601 | F1 0.8060 | Time 12.02\n",
      "Epoch 0058: Loss 4.3758 | AUC 0.6650 | Recall 0.8150 | Precision 0.8150 | AP 0.8642 | F1 0.8150 | Time 12.07\n",
      "Epoch 0059: Loss 4.3739 | AUC 0.6967 | Recall 0.8280 | Precision 0.8280 | AP 0.8732 | F1 0.8280 | Time 12.02\n",
      "Epoch 0060: Loss 4.3768 | AUC 0.7119 | Recall 0.8420 | Precision 0.8420 | AP 0.8772 | F1 0.8420 | Time 12.04\n",
      "Epoch 0061: Loss 4.3770 | AUC 0.7131 | Recall 0.8426 | Precision 0.8426 | AP 0.8775 | F1 0.8426 | Time 11.98\n",
      "Epoch 0062: Loss 4.3738 | AUC 0.7026 | Recall 0.8327 | Precision 0.8327 | AP 0.8748 | F1 0.8327 | Time 12.32\n",
      "Epoch 0063: Loss 4.3730 | AUC 0.6794 | Recall 0.8200 | Precision 0.8200 | AP 0.8686 | F1 0.8200 | Time 12.04\n",
      "Epoch 0064: Loss 4.3743 | AUC 0.6642 | Recall 0.8150 | Precision 0.8150 | AP 0.8641 | F1 0.8150 | Time 12.07\n",
      "Epoch 0065: Loss 4.3739 | AUC 0.6648 | Recall 0.8151 | Precision 0.8151 | AP 0.8643 | F1 0.8151 | Time 12.11\n",
      "Epoch 0066: Loss 4.3722 | AUC 0.6809 | Recall 0.8197 | Precision 0.8197 | AP 0.8691 | F1 0.8197 | Time 12.06\n",
      "Epoch 0067: Loss 4.3732 | AUC 0.7001 | Recall 0.8307 | Precision 0.8307 | AP 0.8742 | F1 0.8307 | Time 12.02\n",
      "Epoch 0068: Loss 4.3735 | AUC 0.7080 | Recall 0.8371 | Precision 0.8371 | AP 0.8765 | F1 0.8371 | Time 12.05\n",
      "Epoch 0069: Loss 4.3727 | AUC 0.7058 | Recall 0.8352 | Precision 0.8352 | AP 0.8758 | F1 0.8352 | Time 12.00\n",
      "Epoch 0070: Loss 4.3713 | AUC 0.6933 | Recall 0.8254 | Precision 0.8254 | AP 0.8726 | F1 0.8254 | Time 12.04\n",
      "Epoch 0071: Loss 4.3725 | AUC 0.6767 | Recall 0.8192 | Precision 0.8192 | AP 0.8678 | F1 0.8192 | Time 12.01\n",
      "Epoch 0072: Loss 4.3726 | AUC 0.6702 | Recall 0.8151 | Precision 0.8151 | AP 0.8662 | F1 0.8151 | Time 12.05\n",
      "Epoch 0073: Loss 4.3750 | AUC 0.6722 | Recall 0.8185 | Precision 0.8185 | AP 0.8664 | F1 0.8185 | Time 11.98\n",
      "Epoch 0074: Loss 4.3708 | AUC 0.6852 | Recall 0.8220 | Precision 0.8220 | AP 0.8703 | F1 0.8220 | Time 12.05\n",
      "Epoch 0075: Loss 4.3939 | AUC 0.6965 | Recall 0.8261 | Precision 0.8261 | AP 0.8738 | F1 0.8261 | Time 11.99\n",
      "Epoch 0076: Loss 4.3879 | AUC 0.6785 | Recall 0.8209 | Precision 0.8209 | AP 0.8677 | F1 0.8209 | Time 12.03\n",
      "Epoch 0077: Loss 4.4196 | AUC 0.6687 | Recall 0.8156 | Precision 0.8156 | AP 0.8648 | F1 0.8156 | Time 12.01\n",
      "Epoch 0078: Loss 4.4241 | AUC 0.6782 | Recall 0.8210 | Precision 0.8210 | AP 0.8675 | F1 0.8210 | Time 12.04\n",
      "Epoch 0079: Loss 4.4033 | AUC 0.6964 | Recall 0.8276 | Precision 0.8276 | AP 0.8726 | F1 0.8276 | Time 12.04\n",
      "Epoch 0080: Loss 4.3908 | AUC 0.7070 | Recall 0.8352 | Precision 0.8352 | AP 0.8753 | F1 0.8352 | Time 12.07\n",
      "Epoch 0081: Loss 4.3846 | AUC 0.7063 | Recall 0.8347 | Precision 0.8347 | AP 0.8752 | F1 0.8347 | Time 12.12\n",
      "Epoch 0082: Loss 4.3784 | AUC 0.6966 | Recall 0.8269 | Precision 0.8269 | AP 0.8730 | F1 0.8269 | Time 12.04\n",
      "Epoch 0083: Loss 4.3784 | AUC 0.6802 | Recall 0.8197 | Precision 0.8197 | AP 0.8686 | F1 0.8197 | Time 12.05\n",
      "Epoch 0084: Loss 4.3916 | AUC 0.6720 | Recall 0.8161 | Precision 0.8161 | AP 0.8664 | F1 0.8161 | Time 12.06\n",
      "Epoch 0085: Loss 4.3886 | AUC 0.6723 | Recall 0.8163 | Precision 0.8163 | AP 0.8665 | F1 0.8163 | Time 12.08\n",
      "Epoch 0086: Loss 4.3784 | AUC 0.6803 | Recall 0.8197 | Precision 0.8197 | AP 0.8687 | F1 0.8197 | Time 12.03\n",
      "Epoch 0087: Loss 4.3767 | AUC 0.6938 | Recall 0.8271 | Precision 0.8271 | AP 0.8723 | F1 0.8271 | Time 12.03\n",
      "Epoch 0088: Loss 4.3789 | AUC 0.7025 | Recall 0.8320 | Precision 0.8320 | AP 0.8745 | F1 0.8320 | Time 12.02\n",
      "Epoch 0089: Loss 4.3804 | AUC 0.7026 | Recall 0.8320 | Precision 0.8320 | AP 0.8744 | F1 0.8320 | Time 12.00\n",
      "Epoch 0090: Loss 4.3806 | AUC 0.6949 | Recall 0.8268 | Precision 0.8268 | AP 0.8723 | F1 0.8268 | Time 12.08\n",
      "Epoch 0091: Loss 4.3807 | AUC 0.6855 | Recall 0.8219 | Precision 0.8219 | AP 0.8696 | F1 0.8219 | Time 11.97\n",
      "Epoch 0092: Loss 4.3811 | AUC 0.6759 | Recall 0.8199 | Precision 0.8199 | AP 0.8669 | F1 0.8199 | Time 12.05\n",
      "Epoch 0093: Loss 4.3808 | AUC 0.6771 | Recall 0.8195 | Precision 0.8195 | AP 0.8673 | F1 0.8195 | Time 12.01\n",
      "Epoch 0094: Loss 4.3801 | AUC 0.6857 | Recall 0.8220 | Precision 0.8220 | AP 0.8699 | F1 0.8220 | Time 12.05\n",
      "Epoch 0095: Loss 4.3801 | AUC 0.6931 | Recall 0.8261 | Precision 0.8261 | AP 0.8721 | F1 0.8260 | Time 11.99\n",
      "Epoch 0096: Loss 4.3802 | AUC 0.6980 | Recall 0.8276 | Precision 0.8276 | AP 0.8734 | F1 0.8276 | Time 12.04\n",
      "Epoch 0097: Loss 4.3799 | AUC 0.6950 | Recall 0.8275 | Precision 0.8275 | AP 0.8726 | F1 0.8275 | Time 11.98\n",
      "Epoch 0098: Loss 4.3794 | AUC 0.6890 | Recall 0.8227 | Precision 0.8227 | AP 0.8710 | F1 0.8227 | Time 12.08\n",
      "Epoch 0099: Loss 4.3788 | AUC 0.6818 | Recall 0.8193 | Precision 0.8193 | AP 0.8689 | F1 0.8193 | Time 12.03\n"
     ]
    }
   ],
   "source": [
    "dominant_compile = dominant_model.fit(pyG_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0021 | "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3881, 1582]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res \u001b[39m=\u001b[39m dominant_compile\u001b[39m.\u001b[39;49mpredict(data\u001b[39m=\u001b[39;49mpyG_test, label \u001b[39m=\u001b[39;49m label_test,return_pred\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_prob\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, prob_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m, return_conf\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:605\u001b[0m, in \u001b[0;36mDeepDetector.predict\u001b[1;34m(self, data, label, return_pred, return_score, return_prob, prob_method, return_conf, return_emb)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[39mif\u001b[39;00m return_emb:\n\u001b[0;32m    603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(DeepDetector, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mpredict(data,\n\u001b[0;32m    606\u001b[0m                                            label,\n\u001b[0;32m    607\u001b[0m                                            return_pred,\n\u001b[0;32m    608\u001b[0m                                            return_score,\n\u001b[0;32m    609\u001b[0m                                            return_prob,\n\u001b[0;32m    610\u001b[0m                                            prob_method,\n\u001b[0;32m    611\u001b[0m                                            return_conf)\n\u001b[0;32m    612\u001b[0m \u001b[39mif\u001b[39;00m return_emb:\n\u001b[0;32m    613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(output) \u001b[39m==\u001b[39m \u001b[39mtuple\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:188\u001b[0m, in \u001b[0;36mDetector.predict\u001b[1;34m(self, data, label, return_pred, return_score, return_prob, prob_method, return_conf)\u001b[0m\n\u001b[0;32m    183\u001b[0m     logger(score\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_score_,\n\u001b[0;32m    184\u001b[0m            target\u001b[39m=\u001b[39mlabel,\n\u001b[0;32m    185\u001b[0m            verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    186\u001b[0m            train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(data, label)\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m return_pred:\n\u001b[0;32m    190\u001b[0m     pred \u001b[39m=\u001b[39m (score \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold_)\u001b[39m.\u001b[39mlong()\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:528\u001b[0m, in \u001b[0;36mDeepDetector.decision_function\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb[node_idx[:batch_size]] \u001b[39m=\u001b[39m \\\n\u001b[0;32m    524\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39memb[:batch_size]\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    526\u001b[0m     outlier_score[node_idx[:batch_size]] \u001b[39m=\u001b[39m score\n\u001b[1;32m--> 528\u001b[0m logger(loss\u001b[39m=\u001b[39;49mloss\u001b[39m.\u001b[39;49mitem() \u001b[39m/\u001b[39;49m data\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    529\u001b[0m        score\u001b[39m=\u001b[39;49moutlier_score,\n\u001b[0;32m    530\u001b[0m        target\u001b[39m=\u001b[39;49mlabel,\n\u001b[0;32m    531\u001b[0m        time\u001b[39m=\u001b[39;49mtime\u001b[39m.\u001b[39;49mtime() \u001b[39m-\u001b[39;49m start_time,\n\u001b[0;32m    532\u001b[0m        verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    533\u001b[0m        train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    534\u001b[0m \u001b[39mreturn\u001b[39;00m outlier_score\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\utils\\utility.py:236\u001b[0m, in \u001b[0;36mlogger\u001b[1;34m(epoch, loss, score, target, time, verbose, train, deep)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    235\u001b[0m     \u001b[39mif\u001b[39;00m target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m         auc \u001b[39m=\u001b[39m eval_roc_auc(target, score)\n\u001b[0;32m    237\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAUC \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(auc), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\metric\\metric.py:33\u001b[0m, in \u001b[0;36meval_roc_auc\u001b[1;34m(label, score)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval_roc_auc\u001b[39m(label, score):\n\u001b[0;32m     16\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    ROC-AUC score for binary classification.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m        Average ROC-AUC score across different labels.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     roc_auc \u001b[39m=\u001b[39m roc_auc_score(y_true\u001b[39m=\u001b[39;49mlabel, y_score\u001b[39m=\u001b[39;49mscore)\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m roc_auc\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:572\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    570\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n\u001b[0;32m    571\u001b[0m     y_true \u001b[39m=\u001b[39m label_binarize(y_true, classes\u001b[39m=\u001b[39mlabels)[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    573\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39;49mmax_fpr),\n\u001b[0;32m    574\u001b[0m         y_true,\n\u001b[0;32m    575\u001b[0m         y_score,\n\u001b[0;32m    576\u001b[0m         average,\n\u001b[0;32m    577\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    578\u001b[0m     )\n\u001b[0;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    581\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39mmax_fpr),\n\u001b[0;32m    582\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    586\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m binary_metric(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     78\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:344\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_true)) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not defined in that case.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m     )\n\u001b[1;32m--> 344\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m max_fpr \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m auc(fpr, tpr)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:992\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_curve\u001b[39m(\n\u001b[0;32m    905\u001b[0m     y_true, y_score, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, drop_intermediate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    906\u001b[0m ):\n\u001b[0;32m    907\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \n\u001b[0;32m    909\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[39m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[0;32m    993\u001b[0m         y_true, y_score, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    994\u001b[0m     )\n\u001b[0;32m    996\u001b[0m     \u001b[39m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[39m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     \u001b[39m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     \u001b[39m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m     \u001b[39m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m     \u001b[39mif\u001b[39;00m drop_intermediate \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fps) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:751\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m pos_label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m    749\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[1;32m--> 751\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    752\u001b[0m y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n\u001b[0;32m    753\u001b[0m y_score \u001b[39m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3881, 1582]"
     ]
    }
   ],
   "source": [
    "dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res = dominant_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, return_prob=True, prob_method='linear', return_conf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_ip = eval_f1(label_test, dominant_ip_pred_res)\n",
    "print(f1_score_ip)\n",
    "precision = eval_precision_at_k(label_test, dominant_ip_score_res, k=1606)\n",
    "print(precision)\n",
    "recall = eval_recall_at_k(label_test, dominant_ip_score_res, k=1606)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominant(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test):\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train.x = train_node_features\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    model = DOMINANT(gpu=0, weight=0.02, num_layers=64, hid_dim=64, contamination=0.1, lr=0.001, verbose=3, epoch=100)\n",
    "    dominant_compile = model.fit(pyG_train)\n",
    "    dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res = dominant_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, return_prob=True, prob_method='linear', return_conf=True)\n",
    "    f1_score_ip = eval_f1(label_test, dominant_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, dominant_ip_score_res, k=3881)\n",
    "    recall = eval_recall_at_k(label_test, dominant_ip_score_res, k=3881)\n",
    "    return f1_score_ip, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 4.8750 |  | Time 12.97\n",
      "Epoch 0001: Loss 4.8732 |  | Time 12.00\n",
      "Epoch 0002: Loss 4.8745 |  | Time 12.29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m recall \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     f1_score, precision_score, recall_score \u001b[39m=\u001b[39m dominant(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     f1\u001b[39m.\u001b[39mappend(f1_score)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     precision\u001b[39m.\u001b[39mappend(precision_score)\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 23\u001b[0m in \u001b[0;36mdominant\u001b[1;34m(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pyG_test\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m test_node_features\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m DOMINANT(gpu\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weight\u001b[39m=\u001b[39m\u001b[39m0.02\u001b[39m, num_layers\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, hid_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, contamination\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, epoch\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dominant_compile \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(pyG_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dominant_ip_pred_res, dominant_ip_score_res, dominant_ip_prob_res, dominant_ip_conf_res \u001b[39m=\u001b[39m dominant_compile\u001b[39m.\u001b[39mpredict(data\u001b[39m=\u001b[39mpyG_test, label \u001b[39m=\u001b[39m label_test,return_pred\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_prob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prob_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m, return_conf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m f1_score_ip \u001b[39m=\u001b[39m eval_f1(label_test, dominant_ip_pred_res)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\base.py:465\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    462\u001b[0m batch_size \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m node_idx \u001b[39m=\u001b[39m sampled_data\u001b[39m.\u001b[39mn_id\n\u001b[1;32m--> 465\u001b[0m loss, score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_model(sampled_data)\n\u001b[0;32m    466\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_emb:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\detector\\dominant.py:161\u001b[0m, in \u001b[0;36mDOMINANT.forward_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    158\u001b[0m s \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39ms\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    159\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 161\u001b[0m x_, s_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, edge_index)\n\u001b[0;32m    163\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss_func(x[:batch_size],\n\u001b[0;32m    164\u001b[0m                              x_[:batch_size],\n\u001b[0;32m    165\u001b[0m                              s[:batch_size, node_idx],\n\u001b[0;32m    166\u001b[0m                              s_[:batch_size],\n\u001b[0;32m    167\u001b[0m                              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n\u001b[0;32m    169\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(score)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pygod\\nn\\dominant.py:112\u001b[0m, in \u001b[0;36mDOMINANTBase.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mForward computation.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39m    Reconstructed adjacency matrix.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m# encode feature matrix\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared_encoder(x, edge_index)\n\u001b[0;32m    114\u001b[0m \u001b[39m# reconstruct feature matrix\u001b[39;00m\n\u001b[0;32m    115\u001b[0m x_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattr_decoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb, edge_index)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\nn\\models\\basic_gnn.py:222\u001b[0m, in \u001b[0;36mBasicGNN.forward\u001b[1;34m(self, x, edge_index, edge_weight, edge_attr, num_sampled_nodes_per_hop, num_sampled_edges_per_hop)\u001b[0m\n\u001b[0;32m    219\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs[i](x, edge_index, edge_weight\u001b[39m=\u001b[39medge_weight,\n\u001b[0;32m    220\u001b[0m                       edge_attr\u001b[39m=\u001b[39medge_attr)\n\u001b[0;32m    221\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_edge_weight:\n\u001b[1;32m--> 222\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvs[i](x, edge_index, edge_weight\u001b[39m=\u001b[39;49medge_weight)\n\u001b[0;32m    223\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_edge_attr:\n\u001b[0;32m    224\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs[i](x, edge_index, edge_attr\u001b[39m=\u001b[39medge_attr)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    208\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    211\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[0;32m    212\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow, x\u001b[39m.\u001b[39;49mdtype)\n\u001b[0;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[0;32m    214\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:91\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     88\u001b[0m num_nodes \u001b[39m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 91\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m add_remaining_self_loops(\n\u001b[0;32m     92\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m edge_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     edge_weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), ), dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m     96\u001b[0m                              device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\utils\\loop.py:370\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    366\u001b[0m     loop_attr[edge_index[\u001b[39m0\u001b[39m][inv_mask]] \u001b[39m=\u001b[39m edge_attr[inv_mask]\n\u001b[0;32m    368\u001b[0m     edge_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 370\u001b[0m edge_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_index[:, mask], loop_index], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    371\u001b[0m \u001b[39mreturn\u001b[39;00m edge_index, edge_attr\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f1 = []\n",
    "precision = []\n",
    "recall = []\n",
    "for i in range(3):\n",
    "    f1_score, precision_score, recall_score = dominant(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n",
    "    f1.append(f1_score)\n",
    "    precision.append(precision_score)\n",
    "    recall.append(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_train_test(unsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nD_train_df = train_df.drop_duplicates(subset=['source_port_info'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    10070\n",
       "1     5381\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nD_train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = train_df.is_malware.value_counts()\n",
    "if value_counts[0] > 10398:\n",
    "    benign = train_df[train_df['is_malware'] == 0].sample(n=10398)\n",
    "    malicious = train_df[train_df['is_malware'] == 1].sample(n=5699)\n",
    "    train_df = pd.concat([benign, malicious])\n",
    "    # nD_train_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    10398\n",
       "1     5699\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    33097\n",
       "1     4457\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = test_df.is_malware.value_counts()\n",
    "if value_counts[0] > 5321:\n",
    "    df_to_lower = test_df[test_df['is_malware'] == 0].sample(n=5321)\n",
    "    test_df = pd.concat([test_df[test_df['is_malware'] == 1], df_to_lower])\n",
    "    # nD_test_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malware\n",
       "0    5229\n",
       "1    4296\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nD_test_df = test_df.drop_duplicates(subset=['source_port_info'], keep='first')\n",
    "nD_test_df.is_malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph, train_node_features, label_train = graph_modeling_1(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph, test_node_features, label_test = graph_modeling_1(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\tugas-akhir\\network-intrusion-detection\\nids-implementation\\nids_pygod.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(train_graph, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_graph/train_graph.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(train_node_features, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_graph/train_node_features.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/tugas-akhir/network-intrusion-detection/nids-implementation/nids_pygod.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(label_train, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_graph/label_train.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_graph' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(train_graph, open('model_graph/train_graph.pkl', 'wb'))\n",
    "pickle.dump(train_node_features, open('model_graph/train_node_features.pkl', 'wb'))\n",
    "pickle.dump(label_train, open('model_graph/label_train.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_graph, open('model_graph/test_graph.pkl', 'wb'))\n",
    "pickle.dump(test_node_features, open('model_graph/test_node_features.pkl', 'wb'))\n",
    "pickle.dump(label_test, open('model_graph/label_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = pickle.load(open('model_graph/train_graph.pkl', 'rb'))\n",
    "label_train = pickle.load(open('model_graph/label_train.pkl', 'rb'))\n",
    "train_node_features = pickle.load(open('model_graph/train_node_features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph = pickle.load(open('model_graph/test_graph.pkl', 'rb'))\n",
    "label_test = pickle.load(open('model_graph/label_test.pkl', 'rb'))\n",
    "test_node_features = pickle.load(open('model_graph/test_node_features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15451"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1]) tensor([5229, 4296])\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = torch.unique(label_test, return_counts=True)\n",
    "print(unique_values, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9525"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22.],\n",
       "        [34.],\n",
       "        [27.],\n",
       "        ...,\n",
       "        [28.],\n",
       "        [15.],\n",
       "        [28.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch_geometric.nn as pyg_nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOMINANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dominant_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = torch.tensor(label_train)\n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    dominant_model = DOMINANT(gpu=0, weight=0.1, num_layers=8, hid_dim=64, contamination=0.37, lr=0.001, verbose=3, backbone=pyg_nn.EdgeCNN, epoch=100)  \n",
    "    dominant_compile = dominant_model.fit(pyG_train)\n",
    "    return dominant_compile, pyG_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dominant(label_test, dominant_compile, pyG_test):\n",
    "    \n",
    "    label_test = torch.tensor(label_test)\n",
    "    dominant_ip_pred_res, dominant_ip_score_res = dominant_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear')\n",
    "    \n",
    "    unique_values, counts = torch.unique(dominant_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "\n",
    "    f1_pygod = eval_f1(label_test, dominant_ip_pred_res)\n",
    "    precision_pygod = eval_precision_at_k(label_test, dominant_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, dominant_ip_score_res)\n",
    "    f1_score = 2 * (precision_pygod * recall_pygod) / (precision_pygod + recall_pygod)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    return precision_pygod, recall_pygod, f1_pygod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 58.7736 |  | Time 0.85\n",
      "Epoch 0001: Loss 54.3093 |  | Time 0.98\n",
      "Epoch 0002: Loss 49.9715 |  | Time 0.97\n",
      "Epoch 0003: Loss 46.1873 |  | Time 0.97\n",
      "Epoch 0004: Loss 42.1571 |  | Time 0.97\n",
      "Epoch 0005: Loss 38.6776 |  | Time 0.97\n",
      "Epoch 0006: Loss 36.1402 |  | Time 0.97\n",
      "Epoch 0007: Loss 32.9313 |  | Time 0.97\n",
      "Epoch 0008: Loss 30.5166 |  | Time 0.97\n",
      "Epoch 0009: Loss 28.3955 |  | Time 1.00\n",
      "Epoch 0010: Loss 26.4588 |  | Time 0.98\n",
      "Epoch 0011: Loss 24.6071 |  | Time 0.98\n",
      "Epoch 0012: Loss 22.7349 |  | Time 0.96\n",
      "Epoch 0013: Loss 21.0353 |  | Time 0.97\n",
      "Epoch 0014: Loss 19.4021 |  | Time 0.96\n",
      "Epoch 0015: Loss 17.8645 |  | Time 0.98\n",
      "Epoch 0016: Loss 16.4100 |  | Time 0.96\n",
      "Epoch 0017: Loss 15.0537 |  | Time 0.97\n",
      "Epoch 0018: Loss 13.8795 |  | Time 0.98\n",
      "Epoch 0019: Loss 12.8639 |  | Time 0.97\n",
      "Epoch 0020: Loss 12.0131 |  | Time 0.98\n",
      "Epoch 0021: Loss 11.3278 |  | Time 0.97\n",
      "Epoch 0022: Loss 10.7669 |  | Time 0.97\n",
      "Epoch 0023: Loss 10.3089 |  | Time 0.98\n",
      "Epoch 0024: Loss 9.9141 |  | Time 0.98\n",
      "Epoch 0025: Loss 9.5678 |  | Time 0.97\n",
      "Epoch 0026: Loss 9.2666 |  | Time 0.97\n",
      "Epoch 0027: Loss 9.0102 |  | Time 0.97\n",
      "Epoch 0028: Loss 8.8044 |  | Time 0.98\n",
      "Epoch 0029: Loss 8.6475 |  | Time 0.97\n",
      "Epoch 0030: Loss 8.5263 |  | Time 0.98\n",
      "Epoch 0031: Loss 8.4205 |  | Time 0.98\n",
      "Epoch 0032: Loss 8.3049 |  | Time 0.98\n",
      "Epoch 0033: Loss 8.1614 |  | Time 0.97\n",
      "Epoch 0034: Loss 7.9661 |  | Time 0.98\n",
      "Epoch 0035: Loss 7.7620 |  | Time 0.97\n",
      "Epoch 0036: Loss 7.5749 |  | Time 0.97\n",
      "Epoch 0037: Loss 7.3103 |  | Time 0.96\n",
      "Epoch 0038: Loss 6.8921 |  | Time 0.98\n",
      "Epoch 0039: Loss 6.4104 |  | Time 0.97\n",
      "Epoch 0040: Loss 6.3536 |  | Time 0.97\n",
      "Epoch 0041: Loss 5.7962 |  | Time 0.97\n",
      "Epoch 0042: Loss 5.3728 |  | Time 0.97\n",
      "Epoch 0043: Loss 4.4521 |  | Time 0.98\n",
      "Epoch 0044: Loss 4.5474 |  | Time 0.97\n",
      "Epoch 0045: Loss 3.9222 |  | Time 0.97\n",
      "Epoch 0046: Loss 3.6690 |  | Time 0.98\n",
      "Epoch 0047: Loss 4.4023 |  | Time 0.97\n",
      "Epoch 0048: Loss 3.5088 |  | Time 0.98\n",
      "Epoch 0049: Loss 3.4656 |  | Time 0.98\n",
      "Epoch 0050: Loss 3.5992 |  | Time 0.96\n",
      "Epoch 0051: Loss 3.8524 |  | Time 0.97\n",
      "Epoch 0052: Loss 3.5183 |  | Time 0.98\n",
      "Epoch 0053: Loss 3.7185 |  | Time 0.98\n",
      "Epoch 0054: Loss 3.6021 |  | Time 0.98\n",
      "Epoch 0055: Loss 3.4217 |  | Time 0.97\n",
      "Epoch 0056: Loss 3.3680 |  | Time 0.97\n",
      "Epoch 0057: Loss 3.2606 |  | Time 0.97\n",
      "Epoch 0058: Loss 3.1455 |  | Time 0.97\n",
      "Epoch 0059: Loss 3.4196 |  | Time 0.98\n",
      "Epoch 0060: Loss 3.4615 |  | Time 0.96\n",
      "Epoch 0061: Loss 2.9976 |  | Time 0.97\n",
      "Epoch 0062: Loss 3.6564 |  | Time 0.97\n",
      "Epoch 0063: Loss 3.4606 |  | Time 0.98\n",
      "Epoch 0064: Loss 3.0589 |  | Time 0.98\n",
      "Epoch 0065: Loss 3.2678 |  | Time 0.97\n",
      "Epoch 0066: Loss 2.9588 |  | Time 0.97\n",
      "Epoch 0067: Loss 3.2905 |  | Time 0.98\n",
      "Epoch 0068: Loss 3.2176 |  | Time 0.98\n",
      "Epoch 0069: Loss 2.9453 |  | Time 0.97\n",
      "Epoch 0070: Loss 3.0302 |  | Time 0.97\n",
      "Epoch 0071: Loss 2.9628 |  | Time 0.98\n",
      "Epoch 0072: Loss 2.8944 |  | Time 0.98\n",
      "Epoch 0073: Loss 3.0596 |  | Time 0.98\n",
      "Epoch 0074: Loss 2.9295 |  | Time 0.98\n",
      "Epoch 0075: Loss 3.0736 |  | Time 0.98\n",
      "Epoch 0076: Loss 3.0537 |  | Time 0.98\n",
      "Epoch 0077: Loss 2.9102 |  | Time 0.98\n",
      "Epoch 0078: Loss 2.9355 |  | Time 0.98\n",
      "Epoch 0079: Loss 2.8640 |  | Time 0.97\n",
      "Epoch 0080: Loss 2.7811 |  | Time 0.98\n",
      "Epoch 0081: Loss 3.0427 |  | Time 0.97\n",
      "Epoch 0082: Loss 2.9670 |  | Time 0.97\n",
      "Epoch 0083: Loss 2.9014 |  | Time 0.97\n",
      "Epoch 0084: Loss 2.8931 |  | Time 0.98\n",
      "Epoch 0085: Loss 2.9030 |  | Time 0.97\n",
      "Epoch 0086: Loss 2.8967 |  | Time 0.97\n",
      "Epoch 0087: Loss 2.8418 |  | Time 0.97\n",
      "Epoch 0088: Loss 2.7676 |  | Time 0.98\n",
      "Epoch 0089: Loss 2.9682 |  | Time 0.98\n",
      "Epoch 0090: Loss 2.8979 |  | Time 0.98\n",
      "Epoch 0091: Loss 2.8814 |  | Time 0.98\n",
      "Epoch 0092: Loss 2.8650 |  | Time 0.99\n",
      "Epoch 0093: Loss 2.8611 |  | Time 0.97\n",
      "Epoch 0094: Loss 2.8555 |  | Time 0.99\n",
      "Epoch 0095: Loss 2.8266 |  | Time 0.98\n",
      "Epoch 0096: Loss 2.7517 |  | Time 0.98\n",
      "Epoch 0097: Loss 2.9378 |  | Time 0.98\n",
      "Epoch 0098: Loss 2.8880 |  | Time 1.02\n",
      "Epoch 0099: Loss 2.8337 |  | Time 0.98\n",
      "Test: Loss 0.0003 | AUC 0.6705 | Recall 0.5426 | Precision 0.5426 | AP 0.6856 | F1 0.5426 | Time 0.36\n",
      "tensor([0, 1]) tensor([6268, 3257])\n",
      "F1 score:  0.6172381835032438\n",
      "Precision:  0.7156892846177464\n",
      "Recall:  0.5425977653631285\n",
      "ini f1_score:  0.6172381835032438\n",
      "ini precision:  tensor(0.5426)\n",
      "ini recall:  tensor(0.5426)\n"
     ]
    }
   ],
   "source": [
    "dominant_model, graph_test = make_dominant_model(train_graph, train_node_features, label_train, test_graph, test_node_features)\n",
    "precision_score, recall_score, f1_score = predict_dominant(label_test, dominant_model, graph_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ini f1:  0.6043874784323392\n",
      "ini precision:  tensor(0.5808)\n",
      "ini recall:  tensor(0.5808)\n"
     ]
    }
   ],
   "source": [
    "print(\"ini f1: \", f1_score_for)\n",
    "print(\"ini precision: \", precision_score)\n",
    "print(\"ini recall: \", recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250.85678052902222, 315.7898392677307, 317.1439673900604]\n",
      "[4.691774845123291, 5.131696939468384, 4.681374549865723]\n"
     ]
    }
   ],
   "source": [
    "print(train_durration)\n",
    "print(predict_durration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ocgnn(label_test, ocgnn_compile, pyG_test):\n",
    "    \n",
    "    label_test = torch.tensor(label_test)\n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    ocgnn_ip_pred_res, ocgnn_ip_score_res = ocgnn_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "    \n",
    "    unique_values, counts = torch.unique(ocgnn_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "\n",
    "    precision_pygod = eval_precision_at_k(label_test, ocgnn_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, ocgnn_ip_score_res)\n",
    "    f1_score = 2 * (precision_pygod * recall_pygod) / (precision_pygod + recall_pygod)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "\n",
    "    return precision_pygod, recall_pygod, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ocgnn_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features,\n",
    "                        label_test):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = torch.tensor(label_train)\n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train.cpu()\n",
    "    pyG_train.x = train_node_features.cpu()\n",
    "    label_train = label_train.cpu()\n",
    "\n",
    "    ocgnn_model = OCGNN(hid_dim=14, num_layers=32, weight_decay=1, \n",
    "                    contamination=0.37, lr=0.004, epoch=100, gpu=-1, \n",
    "                    verbose=3)\n",
    "    ocgnn_compile = ocgnn_model.fit(pyG_train, label_train)\n",
    "    return ocgnn_compile, pyG_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6956\\3720414239.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6956\\3720414239.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_train = torch.tensor(label_train)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6956\\3720414239.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 0.0000 | AUC 0.9909 | Recall 0.9309 | Precision 0.9309 | AP 0.9840 | F1 0.9309 | Time 0.60\n",
      "Epoch 0001: Loss 0.0008 | AUC 0.6040 | Recall 0.4923 | Precision 0.4923 | AP 0.5459 | F1 0.4923 | Time 0.61\n",
      "Epoch 0002: Loss 0.0004 | AUC 0.9208 | Recall 0.8668 | Precision 0.8668 | AP 0.8908 | F1 0.8668 | Time 0.56\n",
      "Epoch 0003: Loss 0.0009 | AUC 0.1921 | Recall 0.0543 | Precision 0.0543 | AP 0.2340 | F1 0.0542 | Time 0.63\n",
      "Epoch 0004: Loss 0.0006 | AUC 0.6685 | Recall 0.6476 | Precision 0.6476 | AP 0.6067 | F1 0.6477 | Time 0.85\n",
      "Epoch 0005: Loss 0.0004 | AUC 0.8439 | Recall 0.6233 | Precision 0.6233 | AP 0.7514 | F1 0.6233 | Time 0.61\n",
      "Epoch 0006: Loss 0.0004 | AUC 0.6882 | Recall 0.5555 | Precision 0.5555 | AP 0.6082 | F1 0.5555 | Time 0.60\n",
      "Epoch 0007: Loss 0.0004 | AUC 0.4301 | Recall 0.3544 | Precision 0.3544 | AP 0.3205 | F1 0.3545 | Time 0.62\n",
      "Epoch 0008: Loss 0.0004 | AUC 0.4698 | Recall 0.0006 | Precision 0.0006 | AP 0.3347 | F1 0.0000 | Time 0.63\n",
      "Epoch 0009: Loss 0.0004 | AUC 0.4775 | Recall 0.4181 | Precision 0.4181 | AP 0.4075 | F1 0.4222 | Time 0.59\n",
      "Epoch 0010: Loss 0.0004 | AUC 0.5092 | Recall 0.4204 | Precision 0.4204 | AP 0.4438 | F1 0.4872 | Time 0.62\n",
      "Epoch 0011: Loss 0.0004 | AUC 0.6677 | Recall 0.6090 | Precision 0.6090 | AP 0.5507 | F1 0.6545 | Time 0.61\n",
      "Epoch 0012: Loss 0.0004 | AUC 0.3579 | Recall 0.3096 | Precision 0.3096 | AP 0.3274 | F1 0.3103 | Time 0.62\n",
      "Epoch 0013: Loss 0.0004 | AUC 0.6199 | Recall 0.5454 | Precision 0.5454 | AP 0.5435 | F1 0.5595 | Time 0.64\n",
      "Epoch 0014: Loss 0.0004 | AUC 0.3157 | Recall 0.0593 | Precision 0.0593 | AP 0.2681 | F1 0.0560 | Time 0.58\n",
      "Epoch 0015: Loss 0.0004 | AUC 0.6041 | Recall 0.5505 | Precision 0.5505 | AP 0.5442 | F1 0.5559 | Time 0.84\n",
      "Epoch 0016: Loss 0.0004 | AUC 0.4581 | Recall 0.3728 | Precision 0.3728 | AP 0.3649 | F1 0.3745 | Time 0.63\n",
      "Epoch 0017: Loss 0.0004 | AUC 0.3627 | Recall 0.2555 | Precision 0.2555 | AP 0.3158 | F1 0.2645 | Time 0.66\n",
      "Epoch 0018: Loss 0.0004 | AUC 0.3247 | Recall 0.1974 | Precision 0.1974 | AP 0.2959 | F1 0.1976 | Time 0.70\n",
      "Epoch 0019: Loss 0.0004 | AUC 0.5558 | Recall 0.4131 | Precision 0.4131 | AP 0.4182 | F1 0.4130 | Time 0.64\n",
      "Epoch 0020: Loss 0.0004 | AUC 0.4358 | Recall 0.3533 | Precision 0.3533 | AP 0.3526 | F1 0.3351 | Time 0.61\n",
      "Epoch 0021: Loss 0.0004 | AUC 0.5719 | Recall 0.4399 | Precision 0.4399 | AP 0.4257 | F1 0.4399 | Time 0.73\n",
      "Epoch 0022: Loss 0.0005 | AUC 0.3578 | Recall 0.1585 | Precision 0.1585 | AP 0.2974 | F1 0.1588 | Time 0.60\n",
      "Epoch 0023: Loss 0.0004 | AUC 0.6370 | Recall 0.4122 | Precision 0.4122 | AP 0.5010 | F1 0.4122 | Time 0.61\n",
      "Epoch 0024: Loss 0.0004 | AUC 0.6140 | Recall 0.4124 | Precision 0.4124 | AP 0.4766 | F1 0.4124 | Time 0.63\n",
      "Epoch 0025: Loss 0.0004 | AUC 0.6873 | Recall 0.4109 | Precision 0.4109 | AP 0.5242 | F1 0.4106 | Time 0.60\n",
      "Epoch 0026: Loss 0.0004 | AUC 0.4874 | Recall 0.4139 | Precision 0.4139 | AP 0.4163 | F1 0.4210 | Time 0.61\n",
      "Epoch 0027: Loss 0.0004 | AUC 0.4655 | Recall 0.3955 | Precision 0.3955 | AP 0.3793 | F1 0.3963 | Time 0.61\n",
      "Epoch 0028: Loss 0.0004 | AUC 0.5928 | Recall 0.4369 | Precision 0.4369 | AP 0.4211 | F1 0.4350 | Time 0.62\n",
      "Epoch 0029: Loss 0.0004 | AUC 0.4793 | Recall 0.3771 | Precision 0.3771 | AP 0.3986 | F1 0.3757 | Time 0.60\n",
      "Epoch 0030: Loss 0.0004 | AUC 0.5092 | Recall 0.2828 | Precision 0.2828 | AP 0.4171 | F1 0.3307 | Time 0.60\n",
      "Epoch 0031: Loss 0.0004 | AUC 0.5330 | Recall 0.3942 | Precision 0.3942 | AP 0.4293 | F1 0.3918 | Time 0.58\n",
      "Epoch 0032: Loss 0.0004 | AUC 0.4137 | Recall 0.3663 | Precision 0.3663 | AP 0.3954 | F1 0.3695 | Time 0.66\n",
      "Epoch 0033: Loss 0.0004 | AUC 0.6641 | Recall 0.5150 | Precision 0.5150 | AP 0.5124 | F1 0.4598 | Time 0.62\n",
      "Epoch 0034: Loss 0.0004 | AUC 0.5848 | Recall 0.4187 | Precision 0.4187 | AP 0.4578 | F1 0.4228 | Time 0.60\n",
      "Epoch 0035: Loss 0.0004 | AUC 0.4777 | Recall 0.4330 | Precision 0.4330 | AP 0.3584 | F1 0.3101 | Time 0.64\n",
      "Epoch 0036: Loss 0.0004 | AUC 0.5112 | Recall 0.3778 | Precision 0.3778 | AP 0.3762 | F1 0.3728 | Time 0.63\n",
      "Epoch 0037: Loss 0.0004 | AUC 0.4326 | Recall 0.3860 | Precision 0.3860 | AP 0.3185 | F1 0.0555 | Time 0.65\n",
      "Epoch 0038: Loss 0.0004 | AUC 0.4719 | Recall 0.4382 | Precision 0.4382 | AP 0.3447 | F1 0.2178 | Time 0.66\n",
      "Epoch 0039: Loss 0.0004 | AUC 0.4552 | Recall 0.4438 | Precision 0.4438 | AP 0.3375 | F1 0.2458 | Time 0.77\n",
      "Epoch 0040: Loss 0.0004 | AUC 0.4598 | Recall 0.4332 | Precision 0.4332 | AP 0.3393 | F1 0.2880 | Time 0.87\n",
      "Epoch 0041: Loss 0.0004 | AUC 0.4769 | Recall 0.3083 | Precision 0.3083 | AP 0.3558 | F1 0.2933 | Time 0.72\n",
      "Epoch 0042: Loss 0.0004 | AUC 0.4990 | Recall 0.3594 | Precision 0.3594 | AP 0.3689 | F1 0.3558 | Time 0.71\n",
      "Epoch 0043: Loss 0.0004 | AUC 0.4606 | Recall 0.3479 | Precision 0.3479 | AP 0.3454 | F1 0.2828 | Time 0.64\n",
      "Epoch 0044: Loss 0.0004 | AUC 0.5048 | Recall 0.2356 | Precision 0.2356 | AP 0.3652 | F1 0.2856 | Time 0.86\n",
      "Epoch 0045: Loss 0.0004 | AUC 0.4848 | Recall 0.3877 | Precision 0.3877 | AP 0.3616 | F1 0.3102 | Time 0.71\n",
      "Epoch 0046: Loss 0.0004 | AUC 0.4601 | Recall 0.3195 | Precision 0.3195 | AP 0.3355 | F1 0.1370 | Time 0.75\n",
      "Epoch 0047: Loss 0.0004 | AUC 0.4964 | Recall 0.3377 | Precision 0.3377 | AP 0.3648 | F1 0.3121 | Time 0.70\n",
      "Epoch 0048: Loss 0.0004 | AUC 0.5042 | Recall 0.3642 | Precision 0.3642 | AP 0.3725 | F1 0.3576 | Time 0.73\n",
      "Epoch 0049: Loss 0.0004 | AUC 0.5029 | Recall 0.3681 | Precision 0.3681 | AP 0.3744 | F1 0.3659 | Time 0.77\n",
      "Epoch 0050: Loss 0.0004 | AUC 0.5707 | Recall 0.4124 | Precision 0.4124 | AP 0.4630 | F1 0.4115 | Time 0.74\n",
      "Epoch 0051: Loss 0.0004 | AUC 0.5566 | Recall 0.4206 | Precision 0.4206 | AP 0.4404 | F1 0.4143 | Time 0.65\n",
      "Epoch 0052: Loss 0.0004 | AUC 0.5495 | Recall 0.4036 | Precision 0.4036 | AP 0.4370 | F1 0.3991 | Time 0.68\n",
      "Epoch 0053: Loss 0.0004 | AUC 0.5301 | Recall 0.3239 | Precision 0.3239 | AP 0.4212 | F1 0.3598 | Time 0.62\n",
      "Epoch 0054: Loss 0.0004 | AUC 0.5189 | Recall 0.3170 | Precision 0.3170 | AP 0.4161 | F1 0.3554 | Time 0.78\n",
      "Epoch 0055: Loss 0.0004 | AUC 0.4988 | Recall 0.2552 | Precision 0.2552 | AP 0.4006 | F1 0.3135 | Time 0.76\n",
      "Epoch 0056: Loss 0.0004 | AUC 0.5132 | Recall 0.3200 | Precision 0.3200 | AP 0.4148 | F1 0.3500 | Time 0.73\n",
      "Epoch 0057: Loss 0.0004 | AUC 0.5133 | Recall 0.2762 | Precision 0.2762 | AP 0.4083 | F1 0.3329 | Time 0.68\n",
      "Epoch 0058: Loss 0.0004 | AUC 0.5342 | Recall 0.3702 | Precision 0.3702 | AP 0.4274 | F1 0.3817 | Time 0.67\n",
      "Epoch 0059: Loss 0.0004 | AUC 0.5343 | Recall 0.3700 | Precision 0.3700 | AP 0.4262 | F1 0.3816 | Time 0.66\n",
      "Epoch 0060: Loss 0.0004 | AUC 0.5656 | Recall 0.4031 | Precision 0.4031 | AP 0.4405 | F1 0.4001 | Time 0.62\n",
      "Epoch 0061: Loss 0.0004 | AUC 0.5726 | Recall 0.4120 | Precision 0.4120 | AP 0.4490 | F1 0.4106 | Time 0.65\n",
      "Epoch 0062: Loss 0.0004 | AUC 0.4973 | Recall 0.3501 | Precision 0.3501 | AP 0.3618 | F1 0.3215 | Time 0.66\n",
      "Epoch 0063: Loss 0.0004 | AUC 0.4850 | Recall 0.3239 | Precision 0.3239 | AP 0.3552 | F1 0.2890 | Time 0.66\n",
      "Epoch 0064: Loss 0.0004 | AUC 0.4592 | Recall 0.3700 | Precision 0.3700 | AP 0.3380 | F1 0.1795 | Time 0.67\n",
      "Epoch 0065: Loss 0.0004 | AUC 0.4754 | Recall 0.3745 | Precision 0.3745 | AP 0.3496 | F1 0.2116 | Time 0.77\n",
      "Epoch 0066: Loss 0.0004 | AUC 0.4840 | Recall 0.3193 | Precision 0.3193 | AP 0.3552 | F1 0.2846 | Time 0.72\n",
      "Epoch 0067: Loss 0.0004 | AUC 0.4924 | Recall 0.3693 | Precision 0.3693 | AP 0.3595 | F1 0.3667 | Time 0.77\n",
      "Epoch 0068: Loss 0.0004 | AUC 0.5145 | Recall 0.3648 | Precision 0.3648 | AP 0.3784 | F1 0.3660 | Time 0.62\n",
      "Epoch 0069: Loss 0.0004 | AUC 0.4704 | Recall 0.3654 | Precision 0.3654 | AP 0.3494 | F1 0.2200 | Time 0.79\n",
      "Epoch 0070: Loss 0.0004 | AUC 0.4890 | Recall 0.2559 | Precision 0.2559 | AP 0.3553 | F1 0.2565 | Time 0.97\n",
      "Epoch 0071: Loss 0.0004 | AUC 0.4824 | Recall 0.3226 | Precision 0.3226 | AP 0.3562 | F1 0.2934 | Time 0.74\n",
      "Epoch 0072: Loss 0.0004 | AUC 0.4947 | Recall 0.2449 | Precision 0.2449 | AP 0.3587 | F1 0.2825 | Time 0.67\n",
      "Epoch 0073: Loss 0.0004 | AUC 0.5152 | Recall 0.3001 | Precision 0.3001 | AP 0.3692 | F1 0.3463 | Time 0.72\n",
      "Epoch 0074: Loss 0.0004 | AUC 0.4906 | Recall 0.2624 | Precision 0.2624 | AP 0.3621 | F1 0.3108 | Time 0.64\n",
      "Epoch 0075: Loss 0.0004 | AUC 0.5180 | Recall 0.3104 | Precision 0.3104 | AP 0.3716 | F1 0.3546 | Time 0.60\n",
      "Epoch 0076: Loss 0.0004 | AUC 0.5124 | Recall 0.3628 | Precision 0.3628 | AP 0.3730 | F1 0.3657 | Time 0.64\n",
      "Epoch 0077: Loss 0.0004 | AUC 0.4818 | Recall 0.2665 | Precision 0.2665 | AP 0.3512 | F1 0.2237 | Time 0.63\n",
      "Epoch 0078: Loss 0.0004 | AUC 0.5083 | Recall 0.2618 | Precision 0.2618 | AP 0.4084 | F1 0.3156 | Time 0.62\n",
      "Epoch 0079: Loss 0.0004 | AUC 0.5056 | Recall 0.2613 | Precision 0.2613 | AP 0.4059 | F1 0.3155 | Time 0.66\n",
      "Epoch 0080: Loss 0.0004 | AUC 0.5085 | Recall 0.2620 | Precision 0.2620 | AP 0.4077 | F1 0.3158 | Time 0.65\n",
      "Epoch 0081: Loss 0.0004 | AUC 0.4800 | Recall 0.2923 | Precision 0.2923 | AP 0.3869 | F1 0.2233 | Time 0.64\n",
      "Epoch 0082: Loss 0.0004 | AUC 0.4928 | Recall 0.1572 | Precision 0.1572 | AP 0.3972 | F1 0.2249 | Time 0.70\n",
      "Epoch 0083: Loss 0.0004 | AUC 0.5272 | Recall 0.2801 | Precision 0.2801 | AP 0.4190 | F1 0.3295 | Time 0.60\n",
      "Epoch 0084: Loss 0.0004 | AUC 0.5359 | Recall 0.3715 | Precision 0.3715 | AP 0.4252 | F1 0.3828 | Time 0.61\n",
      "Epoch 0085: Loss 0.0004 | AUC 0.4959 | Recall 0.2841 | Precision 0.2841 | AP 0.3943 | F1 0.2232 | Time 0.61\n",
      "Epoch 0086: Loss 0.0004 | AUC 0.4595 | Recall 0.3706 | Precision 0.3706 | AP 0.3406 | F1 0.1995 | Time 0.63\n",
      "Epoch 0087: Loss 0.0004 | AUC 0.4951 | Recall 0.2611 | Precision 0.2611 | AP 0.3636 | F1 0.3095 | Time 0.65\n",
      "Epoch 0088: Loss 0.0004 | AUC 0.4815 | Recall 0.1600 | Precision 0.1600 | AP 0.3514 | F1 0.2248 | Time 0.65\n",
      "Epoch 0089: Loss 0.0004 | AUC 0.4798 | Recall 0.2533 | Precision 0.2533 | AP 0.3509 | F1 0.2198 | Time 0.62\n",
      "Epoch 0090: Loss 0.0004 | AUC 0.4741 | Recall 0.2065 | Precision 0.2065 | AP 0.3518 | F1 0.2547 | Time 0.58\n",
      "Epoch 0091: Loss 0.0004 | AUC 0.5034 | Recall 0.2624 | Precision 0.2624 | AP 0.3650 | F1 0.3108 | Time 0.58\n",
      "Epoch 0092: Loss 0.0004 | AUC 0.4768 | Recall 0.2044 | Precision 0.2044 | AP 0.3543 | F1 0.2529 | Time 0.61\n",
      "Epoch 0093: Loss 0.0004 | AUC 0.4801 | Recall 0.2533 | Precision 0.2533 | AP 0.3508 | F1 0.2193 | Time 0.61\n",
      "Epoch 0094: Loss 0.0004 | AUC 0.4515 | Recall 0.3325 | Precision 0.3325 | AP 0.3296 | F1 0.0548 | Time 0.61\n",
      "Epoch 0095: Loss 0.0004 | AUC 0.4685 | Recall 0.2810 | Precision 0.2810 | AP 0.3389 | F1 0.1874 | Time 0.64\n",
      "Epoch 0096: Loss 0.0004 | AUC 0.4597 | Recall 0.2572 | Precision 0.2572 | AP 0.3328 | F1 0.0804 | Time 0.66\n",
      "Epoch 0097: Loss 0.0004 | AUC 0.4483 | Recall 0.2449 | Precision 0.2449 | AP 0.3279 | F1 0.0548 | Time 0.66\n",
      "Epoch 0098: Loss 0.0004 | AUC 0.4525 | Recall 0.3323 | Precision 0.3323 | AP 0.3302 | F1 0.0548 | Time 0.67\n",
      "Epoch 0099: Loss 0.0004 | AUC 0.4516 | Recall 0.3330 | Precision 0.3330 | AP 0.3297 | F1 0.0542 | Time 0.63\n",
      "Test: Loss 0.0000 | AUC 0.4667 | Recall 0.6613 | Precision 0.6613 | AP 0.4662 | F1 0.2074 | Time 0.14\n",
      "tensor([0]) tensor([9525])\n",
      "F1 score:  tensor(0.6613)\n",
      "Precision:  tensor(0.6613)\n",
      "Recall:  tensor(0.6613)\n",
      "F1-Score:  0.0\n",
      "Confusion Matrix: \n",
      " [[5229    0]\n",
      " [4296    0]]\n",
      "ini f1:  0.0\n",
      "ini precision:  tensor(0.6613)\n",
      "ini recall:  tensor(0.6613)\n"
     ]
    }
   ],
   "source": [
    "ocgnn_model, graph_test = make_ocgnn_model(train_graph, train_node_features, label_train, test_graph, test_node_features, label_test)\n",
    "precision_score, recall_score, f1_score= predict_ocgnn(label_test, ocgnn_model, graph_test)\n",
    "print(\"ini f1: \", f1_score)\n",
    "print(\"ini precision: \", precision_score)\n",
    "print(\"ini recall: \", recall_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gae_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train.cpu()\n",
    "    pyG_train.x = train_node_features.cpu()\n",
    "    label_train = label_train.cpu()\n",
    "\n",
    "    gae_model = GAE(hid_dim=12, num_layers=12, weight_decay=3,\n",
    "                contamination=0.37, lr=0.001, epoch=100, gpu=-1,\n",
    "                verbose=3, recon_s=True, sigmoid_s=True)\n",
    "    \n",
    "    gae_compile = gae_model.fit(pyG_train, label_train)\n",
    "    return gae_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gae(gae_compile, test_graph, test_node_features, label_test):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    gae_ip_pred_res, gae_ip_score_res = gae_compile.predict(data=pyG_test, label = label_test,return_pred=True, return_score=True, prob_method='linear')\n",
    "    f1_score_pygod = eval_f1(label_test, gae_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, gae_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, gae_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 score pygod: \", f1_score_pygod)\n",
    "    \n",
    "    return f1_score_pygod, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6956\\1294941806.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_node_features = torch.tensor(train_node_features)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6956\\1294941806.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_train = torch.tensor(label_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 0.3449 | AUC 0.9597 | Recall 0.8432 | Precision 0.8432 | AP 0.9354 | F1 0.8432 | Time 5.51\n",
      "Epoch 0001: Loss 0.3343 | AUC 0.9597 | Recall 0.8432 | Precision 0.8432 | AP 0.9355 | F1 0.8432 | Time 4.22\n",
      "Epoch 0002: Loss 0.3248 | AUC 0.9598 | Recall 0.8433 | Precision 0.8433 | AP 0.9355 | F1 0.8433 | Time 4.13\n",
      "Epoch 0003: Loss 0.3162 | AUC 0.9598 | Recall 0.8433 | Precision 0.8433 | AP 0.9356 | F1 0.8433 | Time 4.51\n",
      "Epoch 0004: Loss 0.3086 | AUC 0.9598 | Recall 0.8433 | Precision 0.8433 | AP 0.9356 | F1 0.8433 | Time 4.44\n",
      "Epoch 0005: Loss 0.3017 | AUC 0.9599 | Recall 0.8432 | Precision 0.8432 | AP 0.9357 | F1 0.8432 | Time 4.26\n",
      "Epoch 0006: Loss 0.2954 | AUC 0.9599 | Recall 0.8433 | Precision 0.8433 | AP 0.9357 | F1 0.8433 | Time 4.34\n",
      "Epoch 0007: Loss 0.2899 | AUC 0.9599 | Recall 0.8433 | Precision 0.8433 | AP 0.9358 | F1 0.8433 | Time 4.61\n",
      "Epoch 0008: Loss 0.2849 | AUC 0.9600 | Recall 0.8435 | Precision 0.8435 | AP 0.9358 | F1 0.8434 | Time 4.42\n",
      "Epoch 0009: Loss 0.2805 | AUC 0.9600 | Recall 0.8437 | Precision 0.8437 | AP 0.9359 | F1 0.8437 | Time 4.31\n",
      "Epoch 0010: Loss 0.2767 | AUC 0.9600 | Recall 0.8437 | Precision 0.8437 | AP 0.9359 | F1 0.8437 | Time 4.07\n",
      "Epoch 0011: Loss 0.2732 | AUC 0.9600 | Recall 0.8441 | Precision 0.8441 | AP 0.9359 | F1 0.8441 | Time 4.06\n",
      "Epoch 0012: Loss 0.2702 | AUC 0.9601 | Recall 0.8439 | Precision 0.8439 | AP 0.9360 | F1 0.8439 | Time 4.74\n",
      "Epoch 0013: Loss 0.2676 | AUC 0.9601 | Recall 0.8439 | Precision 0.8439 | AP 0.9360 | F1 0.8439 | Time 5.90\n",
      "Epoch 0014: Loss 0.2652 | AUC 0.9601 | Recall 0.8439 | Precision 0.8439 | AP 0.9360 | F1 0.8439 | Time 5.80\n",
      "Epoch 0015: Loss 0.2632 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9361 | F1 0.8441 | Time 4.41\n",
      "Epoch 0016: Loss 0.2614 | AUC 0.9601 | Recall 0.8445 | Precision 0.8445 | AP 0.9361 | F1 0.8445 | Time 5.45\n",
      "Epoch 0017: Loss 0.2599 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 5.16\n",
      "Epoch 0018: Loss 0.2586 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.73\n",
      "Epoch 0019: Loss 0.2575 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.89\n",
      "Epoch 0020: Loss 0.2565 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.62\n",
      "Epoch 0021: Loss 0.2557 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.06\n",
      "Epoch 0022: Loss 0.2550 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.38\n",
      "Epoch 0023: Loss 0.2544 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.74\n",
      "Epoch 0024: Loss 0.2539 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9362 | F1 0.8448 | Time 4.55\n",
      "Epoch 0025: Loss 0.2534 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9362 | F1 0.8448 | Time 3.95\n",
      "Epoch 0026: Loss 0.2531 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 3.76\n",
      "Epoch 0027: Loss 0.2528 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9363 | F1 0.8446 | Time 4.36\n",
      "Epoch 0028: Loss 0.2525 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.27\n",
      "Epoch 0029: Loss 0.2522 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 3.93\n",
      "Epoch 0030: Loss 0.2520 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.01\n",
      "Epoch 0031: Loss 0.2519 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.82\n",
      "Epoch 0032: Loss 0.2517 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.48\n",
      "Epoch 0033: Loss 0.2516 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.20\n",
      "Epoch 0034: Loss 0.2514 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.66\n",
      "Epoch 0035: Loss 0.2513 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.31\n",
      "Epoch 0036: Loss 0.2513 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 3.93\n",
      "Epoch 0037: Loss 0.2512 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.26\n",
      "Epoch 0038: Loss 0.2511 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.04\n",
      "Epoch 0039: Loss 0.2510 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 3.97\n",
      "Epoch 0040: Loss 0.2510 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.22\n",
      "Epoch 0041: Loss 0.2509 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.43\n",
      "Epoch 0042: Loss 0.2509 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.64\n",
      "Epoch 0043: Loss 0.2508 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.80\n",
      "Epoch 0044: Loss 0.2508 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9363 | F1 0.8446 | Time 4.69\n",
      "Epoch 0045: Loss 0.2508 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9363 | F1 0.8446 | Time 4.95\n",
      "Epoch 0046: Loss 0.2507 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.88\n",
      "Epoch 0047: Loss 0.2507 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 5.40\n",
      "Epoch 0048: Loss 0.2507 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.51\n",
      "Epoch 0049: Loss 0.2507 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.14\n",
      "Epoch 0050: Loss 0.2506 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.05\n",
      "Epoch 0051: Loss 0.2506 | AUC 0.9602 | Recall 0.8448 | Precision 0.8448 | AP 0.9363 | F1 0.8448 | Time 4.68\n",
      "Epoch 0052: Loss 0.2506 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9363 | F1 0.8446 | Time 4.14\n",
      "Epoch 0053: Loss 0.2506 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9363 | F1 0.8446 | Time 4.04\n",
      "Epoch 0054: Loss 0.2505 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.25\n",
      "Epoch 0055: Loss 0.2505 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.90\n",
      "Epoch 0056: Loss 0.2505 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.38\n",
      "Epoch 0057: Loss 0.2505 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.32\n",
      "Epoch 0058: Loss 0.2505 | AUC 0.9602 | Recall 0.8446 | Precision 0.8446 | AP 0.9362 | F1 0.8446 | Time 4.61\n",
      "Epoch 0059: Loss 0.2504 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.38\n",
      "Epoch 0060: Loss 0.2504 | AUC 0.9602 | Recall 0.8445 | Precision 0.8445 | AP 0.9362 | F1 0.8445 | Time 4.15\n",
      "Epoch 0061: Loss 0.2504 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.11\n",
      "Epoch 0062: Loss 0.2504 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.35\n",
      "Epoch 0063: Loss 0.2504 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.32\n",
      "Epoch 0064: Loss 0.2504 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.32\n",
      "Epoch 0065: Loss 0.2503 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.85\n",
      "Epoch 0066: Loss 0.2503 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.46\n",
      "Epoch 0067: Loss 0.2503 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.07\n",
      "Epoch 0068: Loss 0.2503 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.18\n",
      "Epoch 0069: Loss 0.2503 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 3.99\n",
      "Epoch 0070: Loss 0.2502 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.48\n",
      "Epoch 0071: Loss 0.2502 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9360 | F1 0.8443 | Time 4.97\n",
      "Epoch 0072: Loss 0.2502 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9360 | F1 0.8442 | Time 4.76\n",
      "Epoch 0073: Loss 0.2502 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9360 | F1 0.8443 | Time 4.36\n",
      "Epoch 0074: Loss 0.2502 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9360 | F1 0.8443 | Time 3.92\n",
      "Epoch 0075: Loss 0.2502 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9360 | F1 0.8443 | Time 4.39\n",
      "Epoch 0076: Loss 0.2502 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 4.37\n",
      "Epoch 0077: Loss 0.2501 | AUC 0.9602 | Recall 0.8443 | Precision 0.8443 | AP 0.9361 | F1 0.8443 | Time 3.97\n",
      "Epoch 0078: Loss 0.2501 | AUC 0.9602 | Recall 0.8441 | Precision 0.8441 | AP 0.9361 | F1 0.8440 | Time 4.42\n",
      "Epoch 0079: Loss 0.2501 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9361 | F1 0.8440 | Time 4.25\n",
      "Epoch 0080: Loss 0.2501 | AUC 0.9601 | Recall 0.8439 | Precision 0.8439 | AP 0.9360 | F1 0.8439 | Time 4.35\n",
      "Epoch 0081: Loss 0.2501 | AUC 0.9601 | Recall 0.8439 | Precision 0.8439 | AP 0.9360 | F1 0.8439 | Time 4.05\n",
      "Epoch 0082: Loss 0.2501 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8440 | Time 3.87\n",
      "Epoch 0083: Loss 0.2501 | AUC 0.9601 | Recall 0.8439 | Precision 0.8439 | AP 0.9360 | F1 0.8440 | Time 4.01\n",
      "Epoch 0084: Loss 0.2501 | AUC 0.9601 | Recall 0.8439 | Precision 0.8439 | AP 0.9360 | F1 0.8440 | Time 4.02\n",
      "Epoch 0085: Loss 0.2501 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8441 | Time 4.01\n",
      "Epoch 0086: Loss 0.2501 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8441 | Time 4.04\n",
      "Epoch 0087: Loss 0.2501 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8441 | Time 4.69\n",
      "Epoch 0088: Loss 0.2501 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8441 | Time 4.14\n",
      "Epoch 0089: Loss 0.2501 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8441 | Time 3.91\n",
      "Epoch 0090: Loss 0.2500 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8441 | Time 4.00\n",
      "Epoch 0091: Loss 0.2500 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8441 | Time 3.81\n",
      "Epoch 0092: Loss 0.2500 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8441 | Time 3.83\n",
      "Epoch 0093: Loss 0.2500 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9360 | F1 0.8443 | Time 3.82\n",
      "Epoch 0094: Loss 0.2500 | AUC 0.9601 | Recall 0.8439 | Precision 0.8439 | AP 0.9360 | F1 0.8440 | Time 3.78\n",
      "Epoch 0095: Loss 0.2500 | AUC 0.9601 | Recall 0.8443 | Precision 0.8443 | AP 0.9360 | F1 0.8441 | Time 3.92\n",
      "Epoch 0096: Loss 0.2500 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9359 | F1 0.8441 | Time 4.61\n",
      "Epoch 0097: Loss 0.2500 | AUC 0.9601 | Recall 0.8439 | Precision 0.8439 | AP 0.9359 | F1 0.8440 | Time 4.14\n",
      "Epoch 0098: Loss 0.2500 | AUC 0.9601 | Recall 0.8439 | Precision 0.8439 | AP 0.9359 | F1 0.8441 | Time 4.08\n",
      "Epoch 0099: Loss 0.2500 | AUC 0.9601 | Recall 0.8441 | Precision 0.8441 | AP 0.9359 | F1 0.8442 | Time 4.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6956\\3791293239.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_node_features = torch.tensor(test_node_features)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6956\\3791293239.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_test = torch.tensor(label_test)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss 0.0000 | AUC 0.9582 | Recall 0.8724 | Precision 0.8724 | AP 0.9498 | F1 0.8725 | Time 0.86\n",
      "F1 score:  tensor(0.8724)\n",
      "Precision:  tensor(0.8724)\n",
      "Recall:  tensor(0.8724)\n",
      "F1 score pygod:  0.852900032282363\n"
     ]
    }
   ],
   "source": [
    "gae_model = make_gae_model(train_graph, train_node_features, label_train)\n",
    "f1_score_pygod, precision_score, recall_score, f1_score = predict_gae(gae_model, test_graph, test_node_features, label_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conad_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "    \n",
    "    train_node_features = torch.tensor(train_node_features)\n",
    "    label_train = torch.tensor(label_train)\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train.cpu()\n",
    "    pyG_train.x = train_node_features.cpu()\n",
    "    label_train = label_train.cpu()\n",
    "\n",
    "    conad_model = CONAD(hid_dim=10, num_layers=16, \n",
    "                        lr=0.001, weight_decay= 1, contamination=0.37,\n",
    "                        epoch=100, gpu=-1,  \n",
    "                        weight=1, dropout=0.2, verbose=3)\n",
    "    conad_compile = conad_model.fit(pyG_train, label_train)\n",
    "\n",
    "    return conad_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_conad(conda_compile, test_graph, test_node_features, label_test):\n",
    "    \n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "    \n",
    "    conad_ip_pred_res, conad_ip_score_res = conda_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear', )\n",
    "    f1_score_pygod = eval_f1(label_test, conad_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, conad_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, conad_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    unique_values, counts = torch.unique(conad_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 score pygod: \", f1_score_pygod)\n",
    "    return f1_score_pygod, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 37.5968 | AUC 0.7805 | Recall 0.6532 | Precision 0.6532 | AP 0.6571 | F1 0.6532 | Time 8.91\n",
      "Epoch 0001: Loss 37.5945 | AUC 0.7805 | Recall 0.6521 | Precision 0.6521 | AP 0.6570 | F1 0.6521 | Time 8.62\n",
      "Epoch 0002: Loss 37.5923 | AUC 0.7808 | Recall 0.6532 | Precision 0.6532 | AP 0.6574 | F1 0.6532 | Time 6.91\n",
      "Epoch 0003: Loss 37.5907 | AUC 0.7807 | Recall 0.6540 | Precision 0.6540 | AP 0.6573 | F1 0.6540 | Time 7.60\n",
      "Epoch 0004: Loss 37.5878 | AUC 0.7810 | Recall 0.6538 | Precision 0.6538 | AP 0.6574 | F1 0.6538 | Time 6.55\n",
      "Epoch 0005: Loss 37.5842 | AUC 0.7817 | Recall 0.6549 | Precision 0.6549 | AP 0.6580 | F1 0.6549 | Time 7.04\n",
      "Epoch 0006: Loss 37.5851 | AUC 0.7819 | Recall 0.6555 | Precision 0.6555 | AP 0.6583 | F1 0.6555 | Time 5.68\n",
      "Epoch 0007: Loss 37.5827 | AUC 0.7820 | Recall 0.6543 | Precision 0.6543 | AP 0.6582 | F1 0.6543 | Time 6.49\n",
      "Epoch 0008: Loss 37.5815 | AUC 0.7821 | Recall 0.6555 | Precision 0.6555 | AP 0.6583 | F1 0.6555 | Time 6.45\n",
      "Epoch 0009: Loss 37.5794 | AUC 0.7824 | Recall 0.6551 | Precision 0.6551 | AP 0.6585 | F1 0.6551 | Time 6.64\n",
      "Epoch 0010: Loss 37.5778 | AUC 0.7827 | Recall 0.6560 | Precision 0.6560 | AP 0.6587 | F1 0.6560 | Time 6.87\n",
      "Epoch 0011: Loss 37.5760 | AUC 0.7834 | Recall 0.6573 | Precision 0.6573 | AP 0.6593 | F1 0.6573 | Time 5.97\n",
      "Epoch 0012: Loss 37.5722 | AUC 0.7832 | Recall 0.6577 | Precision 0.6577 | AP 0.6592 | F1 0.6577 | Time 5.86\n",
      "Epoch 0013: Loss 37.5707 | AUC 0.7832 | Recall 0.6558 | Precision 0.6558 | AP 0.6591 | F1 0.6558 | Time 5.59\n",
      "Epoch 0014: Loss 37.5694 | AUC 0.7829 | Recall 0.6566 | Precision 0.6566 | AP 0.6589 | F1 0.6566 | Time 6.50\n",
      "Epoch 0015: Loss 37.5677 | AUC 0.7845 | Recall 0.6579 | Precision 0.6579 | AP 0.6602 | F1 0.6579 | Time 5.42\n",
      "Epoch 0016: Loss 37.5657 | AUC 0.7843 | Recall 0.6579 | Precision 0.6579 | AP 0.6599 | F1 0.6579 | Time 5.80\n",
      "Epoch 0017: Loss 37.5632 | AUC 0.7833 | Recall 0.6560 | Precision 0.6560 | AP 0.6592 | F1 0.6560 | Time 5.80\n",
      "Epoch 0018: Loss 37.5619 | AUC 0.7842 | Recall 0.6573 | Precision 0.6573 | AP 0.6599 | F1 0.6573 | Time 6.36\n",
      "Epoch 0019: Loss 37.5612 | AUC 0.7848 | Recall 0.6586 | Precision 0.6586 | AP 0.6604 | F1 0.6586 | Time 7.39\n",
      "Epoch 0020: Loss 37.5590 | AUC 0.7847 | Recall 0.6581 | Precision 0.6581 | AP 0.6602 | F1 0.6581 | Time 6.72\n",
      "Epoch 0021: Loss 37.5566 | AUC 0.7841 | Recall 0.6579 | Precision 0.6579 | AP 0.6599 | F1 0.6579 | Time 5.39\n",
      "Epoch 0022: Loss 37.5541 | AUC 0.7841 | Recall 0.6566 | Precision 0.6566 | AP 0.6598 | F1 0.6566 | Time 4.95\n",
      "Epoch 0023: Loss 37.5526 | AUC 0.7856 | Recall 0.6584 | Precision 0.6584 | AP 0.6608 | F1 0.6584 | Time 5.70\n",
      "Epoch 0024: Loss 37.5505 | AUC 0.7844 | Recall 0.6562 | Precision 0.6562 | AP 0.6598 | F1 0.6563 | Time 5.96\n",
      "Epoch 0025: Loss 37.5473 | AUC 0.7853 | Recall 0.6581 | Precision 0.6581 | AP 0.6607 | F1 0.6581 | Time 5.17\n",
      "Epoch 0026: Loss 37.5463 | AUC 0.7856 | Recall 0.6590 | Precision 0.6590 | AP 0.6609 | F1 0.6590 | Time 4.82\n",
      "Epoch 0027: Loss 37.5432 | AUC 0.7852 | Recall 0.6584 | Precision 0.6584 | AP 0.6607 | F1 0.6584 | Time 6.22\n",
      "Epoch 0028: Loss 37.5420 | AUC 0.7861 | Recall 0.6581 | Precision 0.6581 | AP 0.6612 | F1 0.6581 | Time 5.19\n",
      "Epoch 0029: Loss 37.5414 | AUC 0.7858 | Recall 0.6597 | Precision 0.6597 | AP 0.6611 | F1 0.6597 | Time 4.78\n",
      "Epoch 0030: Loss 37.5410 | AUC 0.7861 | Recall 0.6592 | Precision 0.6592 | AP 0.6612 | F1 0.6592 | Time 5.76\n",
      "Epoch 0031: Loss 37.5378 | AUC 0.7862 | Recall 0.6595 | Precision 0.6595 | AP 0.6613 | F1 0.6595 | Time 5.63\n",
      "Epoch 0032: Loss 37.5334 | AUC 0.7856 | Recall 0.6584 | Precision 0.6584 | AP 0.6608 | F1 0.6584 | Time 4.87\n",
      "Epoch 0033: Loss 37.5344 | AUC 0.7858 | Recall 0.6597 | Precision 0.6597 | AP 0.6610 | F1 0.6597 | Time 4.78\n",
      "Epoch 0034: Loss 37.5311 | AUC 0.7863 | Recall 0.6610 | Precision 0.6610 | AP 0.6615 | F1 0.6610 | Time 4.77\n",
      "Epoch 0035: Loss 37.5303 | AUC 0.7858 | Recall 0.6594 | Precision 0.6594 | AP 0.6611 | F1 0.6594 | Time 4.75\n",
      "Epoch 0036: Loss 37.5296 | AUC 0.7861 | Recall 0.6586 | Precision 0.6586 | AP 0.6612 | F1 0.6586 | Time 4.81\n",
      "Epoch 0037: Loss 37.5258 | AUC 0.7859 | Recall 0.6592 | Precision 0.6592 | AP 0.6611 | F1 0.6592 | Time 4.80\n",
      "Epoch 0038: Loss 37.5264 | AUC 0.7859 | Recall 0.6584 | Precision 0.6584 | AP 0.6610 | F1 0.6584 | Time 4.94\n",
      "Epoch 0039: Loss 37.5256 | AUC 0.7866 | Recall 0.6608 | Precision 0.6608 | AP 0.6616 | F1 0.6608 | Time 6.43\n",
      "Epoch 0040: Loss 37.5216 | AUC 0.7862 | Recall 0.6597 | Precision 0.6597 | AP 0.6614 | F1 0.6597 | Time 6.72\n",
      "Epoch 0041: Loss 37.5211 | AUC 0.7857 | Recall 0.6586 | Precision 0.6586 | AP 0.6610 | F1 0.6586 | Time 5.26\n",
      "Epoch 0042: Loss 37.5186 | AUC 0.7862 | Recall 0.6595 | Precision 0.6595 | AP 0.6615 | F1 0.6596 | Time 5.71\n",
      "Epoch 0043: Loss 37.5173 | AUC 0.7861 | Recall 0.6582 | Precision 0.6582 | AP 0.6612 | F1 0.6583 | Time 5.89\n",
      "Epoch 0044: Loss 37.5149 | AUC 0.7865 | Recall 0.6594 | Precision 0.6594 | AP 0.6615 | F1 0.6594 | Time 5.38\n",
      "Epoch 0045: Loss 37.5159 | AUC 0.7860 | Recall 0.6590 | Precision 0.6590 | AP 0.6610 | F1 0.6590 | Time 6.32\n",
      "Epoch 0046: Loss 37.5124 | AUC 0.7866 | Recall 0.6618 | Precision 0.6618 | AP 0.6617 | F1 0.6618 | Time 5.83\n",
      "Epoch 0047: Loss 37.5111 | AUC 0.7866 | Recall 0.6605 | Precision 0.6605 | AP 0.6617 | F1 0.6605 | Time 5.93\n",
      "Epoch 0048: Loss 37.5107 | AUC 0.7871 | Recall 0.6612 | Precision 0.6612 | AP 0.6621 | F1 0.6612 | Time 6.41\n",
      "Epoch 0049: Loss 37.5097 | AUC 0.7864 | Recall 0.6603 | Precision 0.6603 | AP 0.6616 | F1 0.6603 | Time 5.76\n",
      "Epoch 0050: Loss 37.5073 | AUC 0.7862 | Recall 0.6577 | Precision 0.6577 | AP 0.6611 | F1 0.6577 | Time 5.97\n",
      "Epoch 0051: Loss 37.5071 | AUC 0.7863 | Recall 0.6601 | Precision 0.6601 | AP 0.6614 | F1 0.6601 | Time 6.37\n",
      "Epoch 0052: Loss 37.5041 | AUC 0.7859 | Recall 0.6581 | Precision 0.6581 | AP 0.6610 | F1 0.6581 | Time 5.61\n",
      "Epoch 0053: Loss 37.5041 | AUC 0.7863 | Recall 0.6594 | Precision 0.6594 | AP 0.6613 | F1 0.6594 | Time 5.93\n",
      "Epoch 0054: Loss 37.5031 | AUC 0.7866 | Recall 0.6614 | Precision 0.6614 | AP 0.6617 | F1 0.6614 | Time 5.13\n",
      "Epoch 0055: Loss 37.5011 | AUC 0.7871 | Recall 0.6620 | Precision 0.6620 | AP 0.6621 | F1 0.6620 | Time 4.85\n",
      "Epoch 0056: Loss 37.4999 | AUC 0.7869 | Recall 0.6614 | Precision 0.6614 | AP 0.6619 | F1 0.6614 | Time 6.89\n",
      "Epoch 0057: Loss 37.4990 | AUC 0.7862 | Recall 0.6599 | Precision 0.6599 | AP 0.6613 | F1 0.6599 | Time 5.65\n",
      "Epoch 0058: Loss 37.4974 | AUC 0.7864 | Recall 0.6594 | Precision 0.6594 | AP 0.6615 | F1 0.6594 | Time 6.51\n",
      "Epoch 0059: Loss 37.4974 | AUC 0.7864 | Recall 0.6597 | Precision 0.6597 | AP 0.6614 | F1 0.6597 | Time 5.51\n",
      "Epoch 0060: Loss 37.4953 | AUC 0.7863 | Recall 0.6601 | Precision 0.6601 | AP 0.6614 | F1 0.6601 | Time 5.55\n",
      "Epoch 0061: Loss 37.4937 | AUC 0.7861 | Recall 0.6579 | Precision 0.6579 | AP 0.6611 | F1 0.6579 | Time 4.85\n",
      "Epoch 0062: Loss 37.4926 | AUC 0.7866 | Recall 0.6599 | Precision 0.6599 | AP 0.6617 | F1 0.6599 | Time 6.03\n",
      "Epoch 0063: Loss 37.4919 | AUC 0.7864 | Recall 0.6605 | Precision 0.6605 | AP 0.6613 | F1 0.6605 | Time 5.85\n",
      "Epoch 0064: Loss 37.4909 | AUC 0.7867 | Recall 0.6597 | Precision 0.6597 | AP 0.6617 | F1 0.6597 | Time 5.82\n",
      "Epoch 0065: Loss 37.4901 | AUC 0.7863 | Recall 0.6584 | Precision 0.6584 | AP 0.6612 | F1 0.6584 | Time 5.95\n",
      "Epoch 0066: Loss 37.4902 | AUC 0.7863 | Recall 0.6607 | Precision 0.6607 | AP 0.6614 | F1 0.6607 | Time 6.23\n",
      "Epoch 0067: Loss 37.4883 | AUC 0.7877 | Recall 0.6634 | Precision 0.6634 | AP 0.6627 | F1 0.6634 | Time 5.52\n",
      "Epoch 0068: Loss 37.4894 | AUC 0.7860 | Recall 0.6581 | Precision 0.6581 | AP 0.6610 | F1 0.6581 | Time 6.21\n",
      "Epoch 0069: Loss 37.4878 | AUC 0.7873 | Recall 0.6618 | Precision 0.6618 | AP 0.6622 | F1 0.6618 | Time 6.60\n",
      "Epoch 0070: Loss 37.4870 | AUC 0.7865 | Recall 0.6605 | Precision 0.6605 | AP 0.6616 | F1 0.6605 | Time 5.39\n",
      "Epoch 0071: Loss 37.4852 | AUC 0.7864 | Recall 0.6592 | Precision 0.6592 | AP 0.6614 | F1 0.6592 | Time 5.99\n",
      "Epoch 0072: Loss 37.4838 | AUC 0.7865 | Recall 0.6584 | Precision 0.6584 | AP 0.6614 | F1 0.6584 | Time 6.08\n",
      "Epoch 0073: Loss 37.4825 | AUC 0.7867 | Recall 0.6590 | Precision 0.6590 | AP 0.6616 | F1 0.6590 | Time 5.27\n",
      "Epoch 0074: Loss 37.4842 | AUC 0.7866 | Recall 0.6601 | Precision 0.6601 | AP 0.6616 | F1 0.6601 | Time 6.17\n",
      "Epoch 0075: Loss 37.4810 | AUC 0.7868 | Recall 0.6599 | Precision 0.6599 | AP 0.6617 | F1 0.6599 | Time 5.79\n",
      "Epoch 0076: Loss 37.4807 | AUC 0.7866 | Recall 0.6608 | Precision 0.6608 | AP 0.6617 | F1 0.6608 | Time 5.46\n",
      "Epoch 0077: Loss 37.4819 | AUC 0.7870 | Recall 0.6586 | Precision 0.6586 | AP 0.6618 | F1 0.6586 | Time 6.38\n",
      "Epoch 0078: Loss 37.4813 | AUC 0.7859 | Recall 0.6579 | Precision 0.6579 | AP 0.6609 | F1 0.6579 | Time 5.72\n",
      "Epoch 0079: Loss 37.4800 | AUC 0.7864 | Recall 0.6582 | Precision 0.6582 | AP 0.6615 | F1 0.6582 | Time 6.01\n",
      "Epoch 0080: Loss 37.4784 | AUC 0.7871 | Recall 0.6607 | Precision 0.6607 | AP 0.6620 | F1 0.6607 | Time 6.28\n",
      "Epoch 0081: Loss 37.4788 | AUC 0.7861 | Recall 0.6582 | Precision 0.6582 | AP 0.6611 | F1 0.6582 | Time 5.40\n",
      "Epoch 0082: Loss 37.4771 | AUC 0.7860 | Recall 0.6584 | Precision 0.6584 | AP 0.6610 | F1 0.6584 | Time 6.25\n",
      "Epoch 0083: Loss 37.4775 | AUC 0.7862 | Recall 0.6569 | Precision 0.6569 | AP 0.6612 | F1 0.6569 | Time 5.78\n",
      "Epoch 0084: Loss 37.4771 | AUC 0.7870 | Recall 0.6605 | Precision 0.6605 | AP 0.6619 | F1 0.6605 | Time 5.56\n",
      "Epoch 0085: Loss 37.4750 | AUC 0.7872 | Recall 0.6614 | Precision 0.6614 | AP 0.6622 | F1 0.6614 | Time 6.05\n",
      "Epoch 0086: Loss 37.4750 | AUC 0.7866 | Recall 0.6594 | Precision 0.6594 | AP 0.6616 | F1 0.6594 | Time 6.77\n",
      "Epoch 0087: Loss 37.4743 | AUC 0.7873 | Recall 0.6612 | Precision 0.6612 | AP 0.6622 | F1 0.6612 | Time 6.06\n",
      "Epoch 0088: Loss 37.4734 | AUC 0.7864 | Recall 0.6601 | Precision 0.6601 | AP 0.6615 | F1 0.6601 | Time 6.80\n",
      "Epoch 0089: Loss 37.4741 | AUC 0.7866 | Recall 0.6594 | Precision 0.6594 | AP 0.6616 | F1 0.6594 | Time 5.71\n",
      "Epoch 0090: Loss 37.4731 | AUC 0.7867 | Recall 0.6592 | Precision 0.6592 | AP 0.6616 | F1 0.6592 | Time 5.73\n",
      "Epoch 0091: Loss 37.4747 | AUC 0.7863 | Recall 0.6582 | Precision 0.6582 | AP 0.6613 | F1 0.6582 | Time 5.85\n",
      "Epoch 0092: Loss 37.4719 | AUC 0.7861 | Recall 0.6584 | Precision 0.6584 | AP 0.6613 | F1 0.6585 | Time 5.61\n",
      "Epoch 0093: Loss 37.4732 | AUC 0.7867 | Recall 0.6597 | Precision 0.6597 | AP 0.6616 | F1 0.6597 | Time 6.58\n",
      "Epoch 0094: Loss 37.4723 | AUC 0.7862 | Recall 0.6584 | Precision 0.6584 | AP 0.6612 | F1 0.6584 | Time 6.88\n",
      "Epoch 0095: Loss 37.4717 | AUC 0.7872 | Recall 0.6618 | Precision 0.6618 | AP 0.6621 | F1 0.6618 | Time 6.03\n",
      "Epoch 0096: Loss 37.4710 | AUC 0.7868 | Recall 0.6608 | Precision 0.6608 | AP 0.6618 | F1 0.6608 | Time 5.66\n",
      "Epoch 0097: Loss 37.4703 | AUC 0.7863 | Recall 0.6584 | Precision 0.6584 | AP 0.6613 | F1 0.6584 | Time 5.77\n",
      "Epoch 0098: Loss 37.4709 | AUC 0.7866 | Recall 0.6599 | Precision 0.6599 | AP 0.6616 | F1 0.6599 | Time 5.47\n",
      "Epoch 0099: Loss 37.4719 | AUC 0.7865 | Recall 0.6590 | Precision 0.6590 | AP 0.6614 | F1 0.6590 | Time 6.32\n",
      "Test: Loss 0.0098 | AUC 0.7915 | Recall 0.6688 | Precision 0.6688 | AP 0.7448 | F1 0.6731 | Time 0.66\n",
      "tensor([0, 1]) tensor([6329, 3196])\n",
      "F1 score:  tensor(0.6688)\n",
      "Precision:  tensor(0.6688)\n",
      "Recall:  tensor(0.6688)\n",
      "F1 score pygod:  0.7549386011745863\n"
     ]
    }
   ],
   "source": [
    "conad_model = make_conad_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_conad(conad_model, test_graph, label_test,  test_node_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnomalyDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_anomalydae_model(train_graph, train_node_features, \n",
    "                        label_train):\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "    label_train = label_train\n",
    "\n",
    "    anomalydae_model = AnomalyDAE(hid_dim=12, emb_dim=4, \n",
    "                        lr=0.001, contamination=0.37,\n",
    "                        epoch=100, gpu=0,\n",
    "                        weight=1, verbose=3)\n",
    "    anomalydae_compile = anomalydae_model.fit(pyG_train, label_train)\n",
    "\n",
    "    return anomalydae_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_anomalydae(label_test, anomalydae_compile, pyG_test):\n",
    "\n",
    "    test_node_features = torch.tensor(test_node_features)\n",
    "    label_test = torch.tensor(label_test)\n",
    "    \n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    anomalydae_ip_pred_res, anomalydae_ip_score_res = anomalydae_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear', )\n",
    "    \n",
    "    precision_pygod = eval_precision_at_k(label_test, anomalydae_ip_score_res)\n",
    "    recall_pygod = eval_recall_at_k(label_test, anomalydae_ip_score_res)\n",
    "    f1_score = 2*(precision_pygod*recall_pygod)/(precision_pygod+recall_pygod)\n",
    "    \n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision_pygod)\n",
    "    print(\"Recall: \", recall_pygod)\n",
    "    return f1_score, precision_pygod, recall_pygod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 6808.8037 | AUC 0.8459 | Recall 0.6921 | Precision 0.6921 | AP 0.7182 | F1 0.6921 | Time 0.97\n",
      "Epoch 0001: Loss 146.2048 | AUC 0.8468 | Recall 0.6967 | Precision 0.6967 | AP 0.7175 | F1 0.6967 | Time 0.68\n",
      "Epoch 0002: Loss 146.1078 | AUC 0.8468 | Recall 0.6967 | Precision 0.6967 | AP 0.7175 | F1 0.6967 | Time 8.99\n",
      "Epoch 0003: Loss 145.8769 | AUC 0.8468 | Recall 0.6967 | Precision 0.6967 | AP 0.7175 | F1 0.6967 | Time 8.63\n",
      "Epoch 0004: Loss 145.5796 | AUC 0.8468 | Recall 0.6967 | Precision 0.6967 | AP 0.7176 | F1 0.6967 | Time 8.81\n",
      "Epoch 0005: Loss 145.2428 | AUC 0.8469 | Recall 0.6967 | Precision 0.6967 | AP 0.7177 | F1 0.6967 | Time 8.70\n",
      "Epoch 0006: Loss 144.8798 | AUC 0.8469 | Recall 0.6967 | Precision 0.6967 | AP 0.7178 | F1 0.6967 | Time 8.75\n",
      "Epoch 0007: Loss 144.4981 | AUC 0.8467 | Recall 0.6967 | Precision 0.6967 | AP 0.7178 | F1 0.6967 | Time 8.73\n",
      "Epoch 0008: Loss 144.1026 | AUC 0.8467 | Recall 0.6965 | Precision 0.6965 | AP 0.7179 | F1 0.6965 | Time 8.77\n",
      "Epoch 0009: Loss 143.6964 | AUC 0.8466 | Recall 0.6965 | Precision 0.6965 | AP 0.7180 | F1 0.6965 | Time 8.54\n",
      "Epoch 0010: Loss 143.2817 | AUC 0.8466 | Recall 0.6963 | Precision 0.6963 | AP 0.7181 | F1 0.6963 | Time 8.64\n",
      "Epoch 0011: Loss 142.8601 | AUC 0.8466 | Recall 0.6962 | Precision 0.6962 | AP 0.7182 | F1 0.6962 | Time 8.45\n",
      "Epoch 0012: Loss 142.4328 | AUC 0.8466 | Recall 0.6962 | Precision 0.6962 | AP 0.7183 | F1 0.6962 | Time 7.97\n",
      "Epoch 0013: Loss 142.0007 | AUC 0.8466 | Recall 0.6962 | Precision 0.6962 | AP 0.7185 | F1 0.6962 | Time 8.71\n",
      "Epoch 0014: Loss 141.5647 | AUC 0.8467 | Recall 0.6962 | Precision 0.6962 | AP 0.7187 | F1 0.6962 | Time 8.66\n",
      "Epoch 0015: Loss 141.1252 | AUC 0.8466 | Recall 0.6962 | Precision 0.6962 | AP 0.7187 | F1 0.6962 | Time 8.64\n",
      "Epoch 0016: Loss 140.6828 | AUC 0.8466 | Recall 0.6962 | Precision 0.6962 | AP 0.7188 | F1 0.6962 | Time 8.74\n",
      "Epoch 0017: Loss 140.2378 | AUC 0.8465 | Recall 0.6962 | Precision 0.6962 | AP 0.7188 | F1 0.6962 | Time 8.75\n",
      "Epoch 0018: Loss 139.7905 | AUC 0.8463 | Recall 0.6962 | Precision 0.6962 | AP 0.7188 | F1 0.6962 | Time 8.69\n",
      "Epoch 0019: Loss 139.3413 | AUC 0.8463 | Recall 0.6960 | Precision 0.6960 | AP 0.7189 | F1 0.6960 | Time 8.64\n",
      "Epoch 0020: Loss 138.8904 | AUC 0.8460 | Recall 0.6960 | Precision 0.6960 | AP 0.7188 | F1 0.6960 | Time 8.81\n",
      "Epoch 0021: Loss 138.4380 | AUC 0.8459 | Recall 0.6950 | Precision 0.6950 | AP 0.7187 | F1 0.6950 | Time 8.61\n",
      "Epoch 0022: Loss 137.9842 | AUC 0.8459 | Recall 0.6949 | Precision 0.6949 | AP 0.7188 | F1 0.6949 | Time 8.63\n",
      "Epoch 0023: Loss 137.5292 | AUC 0.8458 | Recall 0.6949 | Precision 0.6949 | AP 0.7189 | F1 0.6949 | Time 8.69\n",
      "Epoch 0024: Loss 137.0732 | AUC 0.8458 | Recall 0.6947 | Precision 0.6947 | AP 0.7190 | F1 0.6947 | Time 8.69\n",
      "Epoch 0025: Loss 136.6162 | AUC 0.8456 | Recall 0.6947 | Precision 0.6947 | AP 0.7189 | F1 0.6947 | Time 8.71\n",
      "Epoch 0026: Loss 136.1584 | AUC 0.8454 | Recall 0.6945 | Precision 0.6945 | AP 0.7189 | F1 0.6945 | Time 8.55\n",
      "Epoch 0027: Loss 135.6998 | AUC 0.8453 | Recall 0.6945 | Precision 0.6945 | AP 0.7189 | F1 0.6944 | Time 8.73\n",
      "Epoch 0028: Loss 135.2407 | AUC 0.8451 | Recall 0.6945 | Precision 0.6945 | AP 0.7189 | F1 0.6945 | Time 8.67\n",
      "Epoch 0029: Loss 134.7809 | AUC 0.8450 | Recall 0.6945 | Precision 0.6945 | AP 0.7190 | F1 0.6945 | Time 8.64\n",
      "Epoch 0030: Loss 134.3206 | AUC 0.8450 | Recall 0.6945 | Precision 0.6945 | AP 0.7191 | F1 0.6945 | Time 8.75\n",
      "Epoch 0031: Loss 133.8599 | AUC 0.8451 | Recall 0.6945 | Precision 0.6945 | AP 0.7194 | F1 0.6945 | Time 8.69\n",
      "Epoch 0032: Loss 133.3988 | AUC 0.8452 | Recall 0.6945 | Precision 0.6945 | AP 0.7196 | F1 0.6945 | Time 8.78\n",
      "Epoch 0033: Loss 132.9373 | AUC 0.8452 | Recall 0.6945 | Precision 0.6945 | AP 0.7198 | F1 0.6945 | Time 8.59\n",
      "Epoch 0034: Loss 132.4755 | AUC 0.8452 | Recall 0.6941 | Precision 0.6941 | AP 0.7199 | F1 0.6941 | Time 8.67\n",
      "Epoch 0035: Loss 132.0135 | AUC 0.8452 | Recall 0.6937 | Precision 0.6937 | AP 0.7201 | F1 0.6937 | Time 8.72\n",
      "Epoch 0036: Loss 131.5513 | AUC 0.8452 | Recall 0.6936 | Precision 0.6936 | AP 0.7203 | F1 0.6936 | Time 8.65\n",
      "Epoch 0037: Loss 131.0889 | AUC 0.8452 | Recall 0.6936 | Precision 0.6936 | AP 0.7204 | F1 0.6936 | Time 8.73\n",
      "Epoch 0038: Loss 130.6263 | AUC 0.8451 | Recall 0.6932 | Precision 0.6932 | AP 0.7205 | F1 0.6932 | Time 8.70\n",
      "Epoch 0039: Loss 130.1636 | AUC 0.8451 | Recall 0.6932 | Precision 0.6932 | AP 0.7206 | F1 0.6932 | Time 8.62\n",
      "Epoch 0040: Loss 129.7008 | AUC 0.8451 | Recall 0.6932 | Precision 0.6932 | AP 0.7207 | F1 0.6932 | Time 8.63\n",
      "Epoch 0041: Loss 129.2379 | AUC 0.8451 | Recall 0.6932 | Precision 0.6932 | AP 0.7208 | F1 0.6932 | Time 8.73\n",
      "Epoch 0042: Loss 128.7750 | AUC 0.8451 | Recall 0.6930 | Precision 0.6930 | AP 0.7210 | F1 0.6930 | Time 8.61\n",
      "Epoch 0043: Loss 128.3120 | AUC 0.8451 | Recall 0.6930 | Precision 0.6930 | AP 0.7211 | F1 0.6930 | Time 8.73\n",
      "Epoch 0044: Loss 127.8490 | AUC 0.8451 | Recall 0.6928 | Precision 0.6928 | AP 0.7212 | F1 0.6927 | Time 8.72\n",
      "Epoch 0045: Loss 127.3861 | AUC 0.8449 | Recall 0.6924 | Precision 0.6924 | AP 0.7212 | F1 0.6924 | Time 8.63\n",
      "Epoch 0046: Loss 126.9231 | AUC 0.8448 | Recall 0.6919 | Precision 0.6919 | AP 0.7213 | F1 0.6919 | Time 8.59\n",
      "Epoch 0047: Loss 126.4602 | AUC 0.8447 | Recall 0.6915 | Precision 0.6915 | AP 0.7214 | F1 0.6915 | Time 8.72\n",
      "Epoch 0048: Loss 125.9974 | AUC 0.8448 | Recall 0.6915 | Precision 0.6915 | AP 0.7215 | F1 0.6915 | Time 8.65\n",
      "Epoch 0049: Loss 125.5346 | AUC 0.8447 | Recall 0.6915 | Precision 0.6915 | AP 0.7217 | F1 0.6914 | Time 8.50\n",
      "Epoch 0050: Loss 125.0719 | AUC 0.8447 | Recall 0.6913 | Precision 0.6913 | AP 0.7219 | F1 0.6913 | Time 8.82\n",
      "Epoch 0051: Loss 124.6092 | AUC 0.8446 | Recall 0.6911 | Precision 0.6911 | AP 0.7220 | F1 0.6911 | Time 8.58\n",
      "Epoch 0052: Loss 124.1467 | AUC 0.8443 | Recall 0.6911 | Precision 0.6911 | AP 0.7222 | F1 0.6911 | Time 8.67\n",
      "Epoch 0053: Loss 123.6842 | AUC 0.8441 | Recall 0.6911 | Precision 0.6911 | AP 0.7223 | F1 0.6910 | Time 8.61\n",
      "Epoch 0054: Loss 123.2219 | AUC 0.8441 | Recall 0.6909 | Precision 0.6909 | AP 0.7225 | F1 0.6909 | Time 8.66\n",
      "Epoch 0055: Loss 122.7597 | AUC 0.8440 | Recall 0.6906 | Precision 0.6906 | AP 0.7226 | F1 0.6906 | Time 8.71\n",
      "Epoch 0056: Loss 122.2977 | AUC 0.8439 | Recall 0.6898 | Precision 0.6898 | AP 0.7228 | F1 0.6898 | Time 8.63\n",
      "Epoch 0057: Loss 121.8357 | AUC 0.8437 | Recall 0.6893 | Precision 0.6893 | AP 0.7228 | F1 0.6893 | Time 8.84\n",
      "Epoch 0058: Loss 121.3739 | AUC 0.8436 | Recall 0.6885 | Precision 0.6885 | AP 0.7228 | F1 0.6885 | Time 8.49\n",
      "Epoch 0059: Loss 120.9122 | AUC 0.8432 | Recall 0.6883 | Precision 0.6883 | AP 0.7226 | F1 0.6883 | Time 8.43\n",
      "Epoch 0060: Loss 120.4507 | AUC 0.8428 | Recall 0.6882 | Precision 0.6882 | AP 0.7224 | F1 0.6882 | Time 8.72\n",
      "Epoch 0061: Loss 119.9893 | AUC 0.8424 | Recall 0.6882 | Precision 0.6882 | AP 0.7221 | F1 0.6882 | Time 8.72\n",
      "Epoch 0062: Loss 119.5281 | AUC 0.8414 | Recall 0.6882 | Precision 0.6882 | AP 0.7215 | F1 0.6882 | Time 8.63\n",
      "Epoch 0063: Loss 119.0670 | AUC 0.8409 | Recall 0.6880 | Precision 0.6880 | AP 0.7213 | F1 0.6880 | Time 8.69\n",
      "Epoch 0064: Loss 118.6061 | AUC 0.8405 | Recall 0.6878 | Precision 0.6878 | AP 0.7211 | F1 0.6878 | Time 8.69\n",
      "Epoch 0065: Loss 118.1453 | AUC 0.8404 | Recall 0.6874 | Precision 0.6874 | AP 0.7212 | F1 0.6874 | Time 8.63\n",
      "Epoch 0066: Loss 117.6848 | AUC 0.8404 | Recall 0.6869 | Precision 0.6869 | AP 0.7214 | F1 0.6869 | Time 8.59\n",
      "Epoch 0067: Loss 117.2243 | AUC 0.8404 | Recall 0.6865 | Precision 0.6865 | AP 0.7216 | F1 0.6865 | Time 8.57\n",
      "Epoch 0068: Loss 116.7640 | AUC 0.8399 | Recall 0.6863 | Precision 0.6863 | AP 0.7215 | F1 0.6863 | Time 8.64\n",
      "Epoch 0069: Loss 116.3039 | AUC 0.8397 | Recall 0.6863 | Precision 0.6863 | AP 0.7217 | F1 0.6862 | Time 8.32\n",
      "Epoch 0070: Loss 115.8440 | AUC 0.8397 | Recall 0.6863 | Precision 0.6863 | AP 0.7221 | F1 0.6863 | Time 8.72\n",
      "Epoch 0071: Loss 115.3842 | AUC 0.8396 | Recall 0.6863 | Precision 0.6863 | AP 0.7225 | F1 0.6863 | Time 8.57\n",
      "Epoch 0072: Loss 114.9246 | AUC 0.8397 | Recall 0.6863 | Precision 0.6863 | AP 0.7231 | F1 0.6862 | Time 8.68\n",
      "Epoch 0073: Loss 114.4651 | AUC 0.8393 | Recall 0.6861 | Precision 0.6861 | AP 0.7233 | F1 0.6861 | Time 8.61\n",
      "Epoch 0074: Loss 114.0058 | AUC 0.8392 | Recall 0.6859 | Precision 0.6859 | AP 0.7237 | F1 0.6859 | Time 8.72\n",
      "Epoch 0075: Loss 113.5467 | AUC 0.8389 | Recall 0.6857 | Precision 0.6857 | AP 0.7238 | F1 0.6857 | Time 8.73\n",
      "Epoch 0076: Loss 113.0877 | AUC 0.8385 | Recall 0.6857 | Precision 0.6857 | AP 0.7238 | F1 0.6856 | Time 8.57\n",
      "Epoch 0077: Loss 112.6289 | AUC 0.8381 | Recall 0.6856 | Precision 0.6856 | AP 0.7239 | F1 0.6856 | Time 8.69\n",
      "Epoch 0078: Loss 112.1702 | AUC 0.8378 | Recall 0.6854 | Precision 0.6854 | AP 0.7239 | F1 0.6853 | Time 8.69\n",
      "Epoch 0079: Loss 111.7117 | AUC 0.8378 | Recall 0.6876 | Precision 0.6876 | AP 0.7241 | F1 0.6877 | Time 8.68\n",
      "Epoch 0080: Loss 111.2533 | AUC 0.8377 | Recall 0.6895 | Precision 0.6895 | AP 0.7243 | F1 0.6893 | Time 8.58\n",
      "Epoch 0081: Loss 110.7950 | AUC 0.8378 | Recall 0.6936 | Precision 0.6936 | AP 0.7246 | F1 0.6936 | Time 8.74\n",
      "Epoch 0082: Loss 110.3369 | AUC 0.8373 | Recall 0.7038 | Precision 0.7038 | AP 0.7246 | F1 0.7053 | Time 8.66\n",
      "Epoch 0083: Loss 109.8790 | AUC 0.8369 | Recall 0.7103 | Precision 0.7103 | AP 0.7246 | F1 0.7102 | Time 8.56\n",
      "Epoch 0084: Loss 109.4212 | AUC 0.8362 | Recall 0.7255 | Precision 0.7255 | AP 0.7243 | F1 0.7267 | Time 8.70\n",
      "Epoch 0085: Loss 108.9635 | AUC 0.8358 | Recall 0.7382 | Precision 0.7382 | AP 0.7243 | F1 0.7380 | Time 8.70\n",
      "Epoch 0086: Loss 108.5059 | AUC 0.8355 | Recall 0.7447 | Precision 0.7447 | AP 0.7242 | F1 0.7447 | Time 8.58\n",
      "Epoch 0087: Loss 108.0485 | AUC 0.8347 | Recall 0.7569 | Precision 0.7569 | AP 0.7239 | F1 0.7568 | Time 8.76\n",
      "Epoch 0088: Loss 107.5912 | AUC 0.8343 | Recall 0.7645 | Precision 0.7645 | AP 0.7240 | F1 0.7647 | Time 8.47\n",
      "Epoch 0089: Loss 107.1340 | AUC 0.8342 | Recall 0.7673 | Precision 0.7673 | AP 0.7240 | F1 0.7673 | Time 8.60\n",
      "Epoch 0090: Loss 106.6769 | AUC 0.8334 | Recall 0.7671 | Precision 0.7671 | AP 0.7230 | F1 0.7671 | Time 8.68\n",
      "Epoch 0091: Loss 106.2199 | AUC 0.8327 | Recall 0.7668 | Precision 0.7668 | AP 0.7218 | F1 0.7668 | Time 8.70\n",
      "Epoch 0092: Loss 105.7631 | AUC 0.8325 | Recall 0.7670 | Precision 0.7670 | AP 0.7208 | F1 0.7670 | Time 8.72\n",
      "Epoch 0093: Loss 105.3063 | AUC 0.8316 | Recall 0.7666 | Precision 0.7666 | AP 0.7191 | F1 0.7665 | Time 8.62\n",
      "Epoch 0094: Loss 104.8496 | AUC 0.8311 | Recall 0.7666 | Precision 0.7666 | AP 0.7175 | F1 0.7666 | Time 8.69\n",
      "Epoch 0095: Loss 104.3930 | AUC 0.8294 | Recall 0.7658 | Precision 0.7658 | AP 0.7152 | F1 0.7658 | Time 8.67\n",
      "Epoch 0096: Loss 103.9365 | AUC 0.8261 | Recall 0.7651 | Precision 0.7651 | AP 0.7082 | F1 0.7651 | Time 8.75\n",
      "Epoch 0097: Loss 103.4801 | AUC 0.8230 | Recall 0.7644 | Precision 0.7644 | AP 0.6997 | F1 0.7646 | Time 8.49\n",
      "Epoch 0098: Loss 103.0237 | AUC 0.8220 | Recall 0.7588 | Precision 0.7588 | AP 0.6976 | F1 0.7608 | Time 8.68\n",
      "Epoch 0099: Loss 102.5674 | AUC 0.8196 | Recall 0.7510 | Precision 0.7510 | AP 0.6940 | F1 0.7560 | Time 8.64\n",
      "Test: Loss 1.7728 | AUC 0.8477 | Recall 0.7945 | Precision 0.7945 | AP 0.7929 | F1 0.7945 | Time 8.18\n",
      "tensor([1]) tensor([9525])\n",
      "F1 score:  tensor(0.7945)\n",
      "Precision:  tensor(0.7945)\n",
      "Recall:  tensor(0.7945)\n",
      "F1 score pygod:  0.6216626872151074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "anomalydae_model = make_anomalydae_model(train_graph, train_node_features, label_train)\n",
    "precision_score, recall_score, f1_score = predict_anomalydae(label_test, anomalydae_model, graph_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_guide_model(train_graph, train_node_features, \n",
    "                        label_train, test_graph, test_node_features,\n",
    "                        label_test):\n",
    "\n",
    "    pyG_train = from_networkx(train_graph)\n",
    "    pyG_train = pyG_train\n",
    "    pyG_train.x = train_node_features\n",
    "    label_train = label_train\n",
    "\n",
    "    pyG_test = from_networkx(test_graph)\n",
    "    pyG_test = pyG_test\n",
    "    pyG_test.x = test_node_features\n",
    "\n",
    "    guide_model = GUIDE(hid_a=64, hid_s=4, num_layers=4,  \n",
    "                             weight_decay=1,alpha=0.5, contamination=0.1, lr=0.001, epoch=100, gpu=0, \n",
    "                             graphlet_size=16, selected_motif=False,\n",
    "                             verbose=3)\n",
    "    guide_compile = guide_model.fit(pyG_train, label_train)\n",
    "\n",
    "    return guide_compile, pyG_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_guide(label_test, guide_compile, pyG_test):\n",
    "    guide_ip_pred_res, guide_ip_score_res = guide_compile.predict(data=pyG_test, label = label_test, return_pred=True, return_score=True, prob_method='linear', )\n",
    "    f1_score_pygod = eval_f1(label_test, guide_ip_pred_res)\n",
    "    precision = eval_precision_at_k(label_test, guide_ip_score_res)\n",
    "    recall = eval_recall_at_k(label_test, guide_ip_score_res)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    unique_values, counts = torch.unique(guide_ip_pred_res, return_counts=True)\n",
    "    print(unique_values, counts)\n",
    "    print(\"F1 score: \", f1_score)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 score pygod: \", f1_score_pygod)\n",
    "    return f1_score_pygod, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "guide_model, graph_test = make_guide_model(train_graph, train_node_features, label_train,\n",
    "                                            test_graph, test_node_features, label_test)\n",
    "precision_score, recall_score, f1_score = predict_guide(label_test, guide_model, graph_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
